{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "## IMD1107 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](htttps://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "- Transformers have revolutionized NLP due to their ability to handle long-range dependencies and parallelize computations, overcoming limitations of RNNs and LSTMs.\n",
    "\n",
    "- The attention mechanism is central to transformers, allowing models to focus on the most relevant parts of the input sequence when making predictions.\n",
    "\n",
    "- Transformers consist of an encoder and decoder, each with multiple layers including self-attention, feed-forward networks, and positional encoding.\n",
    "\n",
    "- The quadratic complexity of transformers poses challenges for processing longer texts, impacting training time and memory consumption.\n",
    "\n",
    "- Transformers have been successfully applied beyond NLP in domains like computer vision, music generation, speech recognition, and video processing.\n",
    "\n",
    "- Common transformer architectures for NLP include BERT, GPT, RoBERTa, T5, and XLNet, each with unique strengths.\n",
    "\n",
    "- Transformers can be used as feature extractors, using their ability to capture rich syntactical and contextual information from text data.\n",
    "\n",
    "- Key steps for using transformers involve starting with a pretrained model, optional domain-specific fine-tuning, task-specific training, and potentially using the model as a feature extractor.\n",
    "\n",
    "- The 512-token limit in transformers, due to quadratic complexity, can impact accuracy when important information is lost during truncation of longer texts.\n",
    "\n",
    "- A simple workaround to handle the 512-token limit is to focus on the most relevant information, such as using the last 512 tokens in legal documents where the decision is often at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this class, you will be able to:\n",
    "\n",
    "1.  Explain the key advantages of Transformer models over Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) in handling Natural Language Processing tasks, particularly in terms of long-range dependencies and parallelization.\n",
    "\n",
    "2.  Describe the fundamental principles of the attention mechanism, specifically self-attention, and articulate its role in enabling Transformers to focus on relevant parts of input sequences for effective context understanding and information processing.\n",
    "\n",
    "3.  Outline the architecture of a Transformer model, differentiating between the encoder and decoder components, and identify the function of key layers such as self-attention layers, feed-forward networks, and positional encoding within the overall architecture.\n",
    "\n",
    "4.  Apply pre-trained Transformer models from the Hugging Face Transformers library to perform practical Natural Language Processing tasks, such as masked language modeling and text classification, through fine-tuning techniques on domain-specific datasets.\n",
    "\n",
    "5.  Discuss the computational implications of the quadratic complexity inherent in Transformer models, especially when processing long text sequences, and recognize common strategies, like truncation and focusing on relevant text segments, used to mitigate these challenges in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers in Natural Language Processing\n",
    "\n",
    "Transformers have fundamentally transformed NLP since their introduction in 2017. They excel at handling long-range dependencies and support tasks ranging from question answering and text summarization to machine translation. Unlike sequential models (RNNs, LSTMs) that struggle with vanishing gradients and limited parallelism, transformers process all tokens simultaneously. This simultaneous processing enables them to capture both local and global context with a single, powerful mechanism: **self-attention**.\n",
    "\n",
    "### Transition from Sequential Models\n",
    "\n",
    "Before the advent of transformers, recurrent networks such as RNNs and LSTMs dominated the landscape. These models processed data one time step at a time, which imposed innate limitations:\n",
    "- **Limited Long-Range Dependency Capture:** Distant words or tokens were less likely to influence the current prediction.\n",
    "- **Sequential Computation:** The inability to perform computations in parallel hindered efficiency.\n",
    "\n",
    "Transformers overcame these challenges by processing the entire sequence in parallel, allowing the model to capture relationships between tokens regardless of their distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "Fundamental to the Transformer is **attention**. Intuitively, attention lets the model decide which parts of the input are most relevant when updating a token's representation. Consider translating an ambiguous word like \"judge\" in English: the model must consider nearby words (such as gender cues in Portuguese \"juiz\" vs. \"juíza\") when determining its translation.\n",
    "\n",
    "The standard attention computation is given by:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigg(\\frac{QK^T}{\\sqrt{d_k}}\\Bigg)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Q (Query):** A transformed representation of the current token.\n",
    "- **K (Key):** Encodes features of tokens to determine their “importance”.\n",
    "- **V (Value):** The signal that is passing the context along.\n",
    "- $ d_k $ is the dimensionality of the key vectors.\n",
    "\n",
    "The process for computing attention is as follows:\n",
    "1. **Transformation:** Each token is projected into three vectors (query, key, and value) via learned linear transformations.\n",
    "2. **Score Calculation:** Compute the similarity between the query and all keys.\n",
    "3. **Weighted Sum:** Normalize these similarities via softmax and then compute a weighted sum of the value vectors.\n",
    "\n",
    "> **Note:** This mechanism enables parallel processing and dynamically focuses the model’s attention on the most influential parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Architecture of a Transformer\n",
    "\n",
    "A Transformer is divided into two main blocks: the **encoder** and the **decoder**. Each block is formed by stacking identical layers.\n",
    "\n",
    "\n",
    "An illustration from the original transformer paper is shown below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformers_basic.png\" alt=\"Basic Transformer Architecture\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n",
    "- **Input Processing:** Converts raw tokens into embeddings.\n",
    "- **Self-Attention:** Each token interacts with every other token to learn contextual relationships.\n",
    "- **Feed-Forward Processing:** Applies position-wise feed-forward networks to transform the representations.\n",
    "\n",
    "**Usage:**  \n",
    "Models that require a deep understanding of the input without needing to generate new sequences primarily use **only an encoder**. An example is **BERT (Bidirectional Encoder Representations from Transformers)**, which is designed for understanding language tasks such as classification, question answering, and named entity recognition. Here, the focus is on creating high-quality contextual representations of the input text.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "- **Context Incorporation:** Leverages encoder outputs through cross-attention (in the full Transformer) or operates independently for auto-regressive tasks.\n",
    "- **Sequential Generation:** Generates one token at a time while attending to previously generated tokens.\n",
    "\n",
    "**Usage:**  \n",
    "Models tasked with generating text, such as language models, frequently use **only a decoder**. For example, **GPT (Generative Pre-trained Transformer)** models use a stack of decoder layers optimized for predicting the next token in a sequence. These decoders employ causal (masked) self-attention to ensure that the prediction for a token depends only on preceding tokens.\n",
    "\n",
    "### Why Some Models Use Only One Component\n",
    "\n",
    "- **Encoder-Only Models:**  \n",
    "  - **Task Focus:** These models are designed for understanding and interpreting input data.\n",
    "  - **Application Areas:** They work well for tasks like sentiment analysis, classification, or extracting semantic features from text.\n",
    "  - **Architecture Simplification:** Removing the decoder simplifies the architecture by focusing solely on creating rich, bidirectional contextual embeddings.\n",
    "\n",
    "- **Decoder-Only Models:**  \n",
    "  - **Task Focus:** These models are geared towards generating text.\n",
    "  - **Sequential Generation:** They generate output one token at a time and require masking in the self-attention layers to prevent the model from “seeing” future tokens during training.\n",
    "  - **Application Areas:** Ideal for tasks such as story completion, code generation, or any scenario where text generation is the end goal.\n",
    "\n",
    "- **Encoder-Decoder Models (Full Transformers):**  \n",
    "  - **Task Focus:** These models excel in tasks that require a transformation from one sequence to another.\n",
    "  - **Application Areas:** They are commonly used in machine translation, summarization, and other sequence-to-sequence problems where an input sequence is transformed into a different output sequence.\n",
    "  - **Architecture Benefit:** The encoder processes and understands the input context, while the decoder generates the corresponding output using both the encoder's context and the sequential output it builds.\n",
    "\n",
    "So, the choice between using an encoder, a decoder, or both hinges on the nature of the task at hand: understanding versus generating text. Tasks focused on language understanding benefit from encoder-only architectures, while tasks focused on language generation are better served by decoder-only architectures. When both understanding and generation are required (as in translation), a full encoder-decoder architecture is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components of Transformer Layers\n",
    "\n",
    "Transformers rely on a series of key components:\n",
    "\n",
    "### 1. Self-Attention Layer\n",
    "- **Parallel Processing:** Every token in the sequence attends to every other token at the same time.\n",
    "- **Contextual Awareness:** Self-attention updates each token’s representation based on the full context.\n",
    "\n",
    "**Steps:**\n",
    "1. Linearly transform tokens into Query, Key, Value vectors.\n",
    "2. Compute similarity scores via dot products.\n",
    "3. Generate a weighted output via a softmax-normalized weighted sum over values.\n",
    "\n",
    "\n",
    "### 2. Position-Wise Feed-Forward Neural Networks (FFNN)\n",
    "- Applies a two-layer MLP on each token’s representation with a non-linear activation (typically ReLU).\n",
    "- Uses shared parameters across tokens, further refining each token’s feature set.\n",
    "\n",
    "\n",
    "### 3. Positional Encoding\n",
    "Since self-attention does not consider order, positional encodings provide the token order by incorporating sine and cosine functions of varying frequencies:\n",
    "\n",
    "$$\n",
    "\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), \\quad\n",
    "\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "> **Important:** Positional encoding is crucial for downstream tasks that depend on the sequential order of tokens, such as language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Quadratic Complexity in Transformers\n",
    "\n",
    "Transformers have revolutionized various fields of AI, particularly natural language processing. However, it's crucial to understand their computational characteristics, especially when dealing with long sequences. A key aspect is the **quadratic time and space complexity**, denoted as $ O(n^2) $, where $ n $ is the length of the input sequence. This complexity stems from the **self-attention mechanism**, a fundamental component of Transformer architecture.\n",
    "\n",
    "fundamental to the Transformer model is the self-attention layer. In this layer, every token in the input sequence interacts with and attends to every other token to compute context-aware representations.  For each token, the attention mechanism calculates a score reflecting its relationship with all other tokens in the sequence.  This process involves pairwise comparisons between all tokens.\n",
    "\n",
    "Let's consider an input sequence of length $ n $. For the first token, it needs to attend to $ n $ tokens (including itself). The second token also needs to attend to $ n $ tokens, and so on, for all $ n $ tokens in the sequence.  Therefore, the total number of attention computations scales proportionally to $ n \\times n = n^2 $.  This quadratic relationship is why Transformers are said to have $ O(n^2) $ complexity.\n",
    "\n",
    "This quadratic complexity impacts both computational time and memory usage.  Not only does the number of operations increase quadratically with sequence length, but the intermediate attention weights, which represent the relationships between all pairs of tokens, also need to be stored. For longer input sequences, this leads to significant memory demands.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/transformer_quadratic.webp\" alt=\"\" style=\"width: 70%; height: 70%\"/>\n",
    "</p>\n",
    "\n",
    "As illustrated, for a sequence of length 9, we observe $ 9^2 = 81 $ attention computations.  Critically, if we double the sequence length to 18, the computations quadruple to $ 18^2 = 324 $. This rapid growth presents challenges when processing long documents or sequences.\n",
    "\n",
    "\n",
    "### Ramifications of Quadratic Scaling\n",
    "\n",
    "- **Memory Footprint:** The self-attention mechanism necessitates storing or computing attention scores for every pair of tokens.  For a sequence length $ n $ and hidden dimension $ d $, storing the attention matrix alone requires $ O(n^2 \\times d) $ memory.  As sequence length increases, the memory demands become a bottleneck, especially when training or deploying models with limited GPU memory.\n",
    "\n",
    "- **Inference Time:** The quadratic increase in operations directly translates to longer processing times.  For tasks involving long documents, such as document summarization, question answering over extensive texts, or code analysis, the inference time can become prohibitively slow.  This limits the applicability of standard Transformers in scenarios requiring efficient processing of lengthy sequences.\n",
    "\n",
    "\n",
    "### Strategies to Alleviate Quadratic Overhead\n",
    "\n",
    "To address the limitations imposed by quadratic complexity, various techniques have been developed to enhance Transformer architectures. Many of these methods are incorporated in models like ModernBERT, aiming to improve computational and memory efficiency and extend the manageable sequence length.\n",
    "\n",
    "#### 1. Alternating Global and Local Attention Patterns\n",
    "\n",
    "- **Concept:** The key idea is to strategically reduce the scope of attention computation in certain layers.  Instead of applying computationally expensive global self-attention in every layer, we can alternate between layers with global attention and layers with more efficient local attention.\n",
    "\n",
    "- **Implementation:** A common approach involves using global attention layers sparingly, for example, every third layer. In the intervening layers, we employ local attention mechanisms, such as **sliding-window attention**. In sliding-window attention, each token only attends to a fixed-size window of tokens around it.  For instance, a token might only attend to the $ w $ tokens immediately preceding and succeeding it, where $ w $ is the window size (e.g., 128 tokens).\n",
    "\n",
    "- **Benefits:**\n",
    "    - **Reduced Computational Cost:** Local attention significantly reduces the computational burden.  For a window size $ w $, local attention reduces the complexity from $ O(n^2) $ in global attention to approximately $ O(n \\times w) $.  This is because for each of the $ n $ tokens, we only perform attention computations within a window of size $ w $. If $ w << n $, this represents a substantial saving.\n",
    "    - **Preservation of Long-Range Context:** By incorporating occasional global attention layers, the model retains the ability to capture long-range dependencies within the sequence.  These global layers act as a bridge, allowing information to propagate across the entire sequence, even with local attention in other layers.\n",
    "\n",
    "#### 2. Unpadding Methods\n",
    "\n",
    "- **Concept:** Padding is a common practice in batch processing of sequences. To process sequences of varying lengths in batches, shorter sequences are padded with special \"padding\" tokens to match the length of the longest sequence in the batch. However, these padding tokens are semantically meaningless and contribute unnecessary computations in the attention mechanism. Unpadding techniques aim to eliminate these redundant computations.\n",
    "\n",
    "- **Implementation:**  Instead of processing the padded sequences directly, unpadding methods first identify and remove the padding tokens from each sequence within a batch. The remaining meaningful tokens are then concatenated into a single, continuous sequence. This concatenated sequence is processed using an efficient attention mechanism that is aware of the original sequence boundaries.  Memory-efficient attention kernels are often employed to handle these unpadded sequences effectively.\n",
    "\n",
    "- **Benefits:**\n",
    "    - **Enhanced Efficiency:** By removing computations associated with padding tokens, unpadding reduces computational overhead during both training and inference.  The model focuses its resources on processing actual content.\n",
    "    - **Increased Throughput:** Processing only meaningful tokens leads to more efficient resource utilization.  This results in higher throughput, meaning more tokens can be processed per unit of time.\n",
    "\n",
    "#### 3. Flash Attention Kernels\n",
    "\n",
    "- **Concept:** Flash Attention is not an architectural change but rather a set of optimized implementations of the attention mechanism. These optimized kernels are designed to maximize memory and compute efficiency, especially when dealing with long sequences, by reordering computations.\n",
    "\n",
    "- **Implementation:** Flash Attention re-arranges the standard attention computation to better align with the characteristics of modern GPU hardware.  Traditional attention implementations often involve multiple memory reads and writes, which can be a bottleneck, especially with long sequences. Flash Attention uses techniques like tiling and kernel blending to reduce memory bandwidth requirements. For instance, Flash Attention 3 is optimized for global attention layers, while Flash Attention 2 is tailored for local attention layers, providing specialized optimizations for different attention patterns.\n",
    "\n",
    "- **Benefits:**\n",
    "    - **Reduced Memory Bandwidth Demands:** By minimizing data movement between GPU memory and compute units, Flash Attention significantly lowers memory bandwidth usage. This is crucial for processing long sequences, where memory bandwidth can become a limiting factor.\n",
    "    - **Accelerated Computation:** Optimized kernels in Flash Attention lead to faster attention computations without compromising accuracy.  This results in overall speed improvements in model training and inference, particularly for long sequences.\n",
    "\n",
    "#### 4. Positional Embeddings: Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "- **Concept:** Positional embeddings are crucial for Transformers to understand the order of tokens in a sequence, as self-attention is permutation-invariant. Rotary Positional Embeddings (RoPE) offer an alternative to traditional absolute positional embeddings. Unlike absolute positional embeddings, which add a fixed vector to the token embeddings based on their position, RoPE directly incorporates positional information into the attention computation itself by modifying the query and key vectors based on their positions through rotation matrices.\n",
    "\n",
    "- **Implementation:** RoPE encodes positional information by applying rotations in the embedding space as a function of token position.  Specifically, when calculating attention scores between tokens at positions $ i $ and $ j $, RoPE rotates the query vector of the $ i $-th token and the key vector of the $ j $-th token using rotation matrices dependent on their respective positions.  A parameter called “RoPE theta” controls the frequency of rotation and allows for context length extension. By adjusting “RoPE theta,” the model can maintain performance even when extrapolating to context lengths beyond what it was originally trained on (e.g., extending from 1024 tokens to 8192 tokens).\n",
    "\n",
    "- **Benefits:**\n",
    "    - **Scalability to Longer Sequences:** RoPE's approach to positional encoding is more amenable to handling longer sequences compared to absolute positional embeddings. It can be extended to longer context lengths without a proportional increase in complexity or performance degradation.\n",
    "    - **Flexibility and Combination:** RoPE can be seamlessly integrated with other efficiency techniques like unpadding and alternating attention mechanisms, providing a versatile solution for long-context Transformers.\n",
    "\n",
    "#### 5. Refinements in Activation and Normalization\n",
    "\n",
    "- **Activation Functions:** Standard Transformers often use GeLU (Gaussian Error Linear Unit) as the activation function.  More recent advancements explore alternatives like GeGLU (Gated Exponential Linear Unit). GeGLU introduces a gating mechanism, which can enhance model expressiveness without a significant increase in computational cost. This gated mechanism allows the model to selectively control the flow of information through the network, potentially improving representation learning.\n",
    "\n",
    "- **Normalization:**  Normalization techniques like LayerNorm are critical for stabilizing training in deep networks.  Pre-normalization, where LayerNorm is applied before the attention and feed-forward layers (instead of after), has been shown to improve training stability, especially in architectures that combine global and local attention. Pre-normalization helps to ensure that gradients are well-behaved, assisting more effective training, particularly when employing complex attention patterns.\n",
    "\n",
    "#### 6. Hardware-Aware Model Optimizations\n",
    "\n",
    "- **Tensor Tiling:** Modern GPUs and other hardware accelerators perform optimally when matrix dimensions are aligned with their internal architecture. Tensor tiling involves ensuring that the dimensions of weight matrices in the model are divisible by hardware-specific numbers, such as multiples of 64. This alignment can significantly improve computational efficiency on GPUs by improving memory access patterns and parallel processing.\n",
    "\n",
    "- **Weight Initialization Strategies:**  When scaling models to handle longer contexts or larger vocabularies, efficient weight initialization becomes crucial. Center tiling is a weight initialization method that enables the efficient expansion of parameter matrices from a smaller, pre-trained model into a larger model. This technique allows parameter matrices from a smaller model to be expanded and reused in a larger model while maintaining or even improving performance. It enables transfer learning and reduces the need to train large models from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Transformers Beyond NLP\n",
    "\n",
    "Originally developed for Natural Language Processing (NLP), Transformers possess an architecture uniquely suited to identify sophisticated patterns and dependencies within input data. This capability has led to significant exploration and application of Transformers in diverse fields beyond text processing, achieving remarkable results. Let's explore several domains where Transformers have made substantial contributions.\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "In computer vision, Transformers offer a distinct advantage in capturing **long-range dependencies** between different parts of an image. Unlike Convolutional Neural Networks (CNNs) which inherently have a limited receptive field, Transformers, through their attention mechanism, can relate any two regions in the image directly. This is crucial because understanding a visual scene often requires considering relationships between objects and regions that are far apart in the image.\n",
    "\n",
    "Consider an image as a grid of pixels. Traditional CNNs process images by applying convolutional filters, which are effective at capturing local patterns. However, to understand the context of a pixel in relation to distant pixels, multiple convolutional layers are needed, potentially making it computationally expensive and less efficient at capturing truly long-range dependencies. Transformers address this by using **self-attention**.\n",
    "\n",
    "#### Image Transformer\n",
    "\n",
    "The Image Transformer adapts the original Transformer model for image processing at a fine-grained level. It conceptualizes each pixel in an image as a sequential token, analogous to words in a sentence. This allows the model to analyze images pixel by pixel, capturing subtle details and sophisticated relationships throughout the image.\n",
    "\n",
    "For an image of size $H \\times W$ with $C$ channels, we can consider each pixel as a feature vector. Let $P_{i,j} \\in \\mathbb{R}^C$ be the pixel at row $i$ and column $j$. The Image Transformer processes the sequence of these pixel vectors, effectively linearizing the 2D image into a 1D sequence.  The self-attention mechanism can then compute relationships between any pair of pixels $(P_{i,j}, P_{k,l})$ regardless of their spatial distance in the image.\n",
    "\n",
    "#### Vision Transformer (ViT)\n",
    "\n",
    "The Vision Transformer (ViT) takes a different approach to image processing, aiming for computational efficiency while retaining the benefits of the Transformer architecture. ViT processes images by dividing them into **patches**.  An input image is split into fixed-size patches, and each patch is then linearly embedded to form a token. These patch tokens are fed into a standard Transformer encoder.\n",
    "\n",
    "For an image of size $H \\times W$, ViT divides it into $N = \\frac{HW}{P^2}$ patches of size $P \\times P$. Each patch is flattened and linearly projected into a $D$-dimensional embedding space. Let $x \\in \\mathbb{R}^{H \\times W \\times C}$ be the input image. It is reshaped into a sequence of flattened patches $x_p \\in \\mathbb{R}^{N \\times (P^2C)}$.  A linear projection is applied to each patch $E \\in \\mathbb{R}^{(P^2C) \\times D}$ to get patch embeddings $z_0 = [x_{p}^1E; x_{p}^2E; ...; x_{p}^NE] + E_{pos}$, where $E_{pos} \\in \\mathbb{R}^{N \\times D}$ are positional embeddings. These patch embeddings are then processed by the Transformer encoder.\n",
    "\n",
    "By treating image patches as tokens, ViT significantly reduces the sequence length compared to processing individual pixels, making it computationally more tractable, especially for high-resolution images. ViT has shown that this patch-based approach can achieve competitive performance with latest CNNs on image classification tasks, while utilizing the global context understanding of Transformers.\n",
    "\n",
    "### Music Generation\n",
    "\n",
    "Transformers have also proven remarkably effective in music generation. The self-attention mechanism's capacity to maintain a **long context** is highly advantageous for music composition. Musical pieces are inherently structured with long-term dependencies; a musical note's relevance and unity often depend on notes played much earlier in the piece. Transformers are naturally suited to model these temporal dependencies.\n",
    "\n",
    "Consider the generation of a melody. The choice of a note is not only influenced by the immediately preceding notes but also by the overall musical phrase, key, and style established earlier in the composition. Recurrent Neural Networks (RNNs), while historically used in music generation, can struggle with very long-range dependencies due to issues like vanishing gradients. Transformers, with their attention mechanism, can directly access information from anywhere in the generated musical sequence, regardless of distance.\n",
    "\n",
    "#### MuseNet\n",
    "\n",
    "MuseNet is a compelling example of Transformer application in music generation. It employs Transformers to compose musical pieces up to several minutes long, orchestrating across as many as ten different instruments.  MuseNet's capability extends to style transfer, allowing it to generate music that blends various musical styles, from classical composers like Mozart to genres like country music and pop artists such as The Beatles. This demonstrates the Transformer's ability to learn and combine complex musical structures and styles.\n",
    "\n",
    "The generation process in MuseNet involves training a Transformer on a large dataset of musical scores. The model learns to predict the next musical token (note, duration, instrument, etc.) based on the preceding sequence of tokens. The long context window of the Transformer allows it to maintain musical coherence over extended compositions and to incorporate stylistic elements learned from diverse musical traditions.\n",
    "\n",
    "### Speech Recognition\n",
    "\n",
    "Transformers have demonstrated exceptional efficacy in speech recognition. Their self-attention mechanism is particularly well-suited for modeling the **temporal forces of speech**. Speech is a sequential signal where the meaning of a phoneme or word can depend on the context provided by preceding and succeeding sounds, often across considerable time spans.\n",
    "\n",
    "Traditional Automatic Speech Recognition (ASR) systems often relied on complex architectures combining acoustic models, language models, and alignment algorithms like Hidden Markov Models (HMMs) and Connectionist Temporal Classification (CTC).  HMMs are used to model the temporal structure of speech, and CTC is used to handle the alignment between the acoustic signal and the transcribed text.\n",
    "\n",
    "#### Speech-Transformer\n",
    "\n",
    "The Speech-Transformer represents a simplification in ASR system design by reducing the reliance on these complex, hand-engineered components. It effectively operates without requiring explicit HMMs or CTC for alignment. Despite this architectural simplification, Speech-Transformers achieve advanced performance in speech recognition tasks.\n",
    "\n",
    "The Speech-Transformer processes the audio signal, often converted into spectrograms or filter bank features. These acoustic features are then treated as input sequences, similar to words in NLP. The Transformer's self-attention mechanism directly learns to map these acoustic sequences to text transcriptions. The elimination of CTC and HMMs makes the system conceptually simpler and potentially easier to train and deploy, while maintaining or even improving accuracy due to the powerful sequence modeling capabilities of Transformers.\n",
    "\n",
    "### Video Processing\n",
    "\n",
    "Similar to computer vision, Transformers have found significant utility in video processing.  In video, each frame can be considered a spatial token, and the sequence of frames over time provides the temporal dimension.  Understanding video content necessitates capturing both spatial patterns within each frame and temporal patterns across frames.\n",
    "\n",
    "#### Video Transformer\n",
    "\n",
    "The Video Transformer leverages the strengths of Transformers to extract elaborate spatial and temporal patterns from video sequences. By analyzing the relationships between frames and their sequential order, the Video Transformer effectively understands and processes video data.\n",
    "\n",
    "Consider video understanding tasks such as action recognition or video captioning.  To identify actions or describe video content, the model must not only recognize objects and scenes within individual frames (spatial understanding) but also track how these elements change and interact over time (temporal understanding).  Video Transformers can model these spatio-temporal interactions effectively by attending to both spatial features within frames and temporal relationships across frames.\n",
    "\n",
    "Different approaches exist for applying Transformers to video. Some methods treat each frame as a token and process the sequence of frames directly. Others may first extract features from each frame using CNNs and then apply Transformers to the sequence of frame-level features to model temporal dependencies. Regardless of the specific approach, the central principle is to utilize the Transformer's attention mechanism to integrate spatial and temporal information for video understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Transformers excel wherever data exhibits structure and complex interdependencies. Their ability to model detailed relationships within input data renders them remarkably versatile and applicable across a wide array of domains. Although originating in NLP, ongoing research continues to broaden their potential, extending the limits of their capabilities in diverse applications.\n",
    ">\n",
    "> As demonstrated, Transformers have successfully expanded beyond text processing, making substantial contributions to fields like computer vision, music generation, speech recognition, and video processing. Their unique capacity to capture long-range dependencies and learn from structured data has opened up new avenues for innovation and progress across these disciplines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Transformer Architectures for NLP\n",
    "\n",
    "Transformers have greatly impacted Natural Language Processing by modeling dependencies in textual data with self-attention mechanisms. Key transformer models are designed with different characteristics to address numerous tasks.\n",
    "\n",
    "### 1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- **Architecture and Directionality:**  \n",
    "  BERT is built using multiple stacked transformer encoder layers. Its unique feature is bidirectional processing, which considers both preceding and following words when determining the context of a word. This approach enhances the understanding of semantic meaning.\n",
    "\n",
    "- **Pre-training Tasks:**  \n",
    "  BERT is initially trained on large unlabeled datasets using two unsupervised tasks:\n",
    "  \n",
    "  - **Masked Language Modeling (MLM):**  \n",
    "    A percentage of input tokens is randomly replaced with a mask token. The model learns to predict the original token based on its surrounding context. One can think of the prediction probability as:  \n",
    "    $$\n",
    "    P(x_i \\mid x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n)\n",
    "    $$\n",
    "  \n",
    "  - **Next Sentence Prediction (NSP):**  \n",
    "    The model is given two sentences and must decide whether the second sentence logically follows the first in the original text.\n",
    "\n",
    "- **Usage in Downstream Tasks:**  \n",
    "  Once pre-training is complete, the model’s learned representations are fine-tuned for specific applications such as text classification, question answering, or named entity recognition with minimal architectural changes.\n",
    "\n",
    "### 2. GPT (Generative Pretrained Transformer)\n",
    "\n",
    "- **Architecture and Directionality:**  \n",
    "  GPT is made up of transformer decoder layers arranged in a stack. It uses a unidirectional (left-to-right) approach, meaning the prediction for each token only depends on previous tokens. This makes GPT especially good at generating coherent text.\n",
    "\n",
    "- **Pre-training Objective:**  \n",
    "  GPT is pre-trained using a language modeling objective. Essentially, it is trained to predict the next token in a sequence:\n",
    "  $$\n",
    "  P(x_{i+1} \\mid x_1, \\dots, x_i)\n",
    "  $$\n",
    "  \n",
    "- **Text Generation:**  \n",
    "  Due to its unidirectional setup, GPT excels in applications like story generation, dialogue systems, and text continuation where context is provided by a prompt.\n",
    "\n",
    "### 3. RoBERTa (Robustly Optimized BERT Approach)\n",
    "\n",
    "- **Enhancements over BERT:**  \n",
    "  RoBERTa retains the basic architecture of BERT (stacked encoder layers) but includes key changes:\n",
    "  \n",
    "  - Uses **dynamic masking** rather than a fixed mask setup, meaning different tokens are masked in each training cycle.\n",
    "  - Trains with larger batch sizes and on more extensive datasets.\n",
    "\n",
    "- **Performance Considerations:**  \n",
    "  Improved training procedures allow RoBERTa to often achieve better performance on benchmarks by learning more sturdy representations.\n",
    "\n",
    "### 4. T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "- **Unified Framework:**  \n",
    "  T5 reconceptualizes all NLP tasks as converting input text to output text. Whether the task is translation, summarization, or classification, both input and output are treated as text sequences.\n",
    "  \n",
    "- **Training Objective:**  \n",
    "  The model is pre-trained using a denoising cost function, where corrupted text is restored to its original form. In mathematical form, one can describe the objective as minimizing:\n",
    "  $$\n",
    "  \\min_{\\theta} \\, E\\left[d\\left(g(f(x; \\theta)), x\\right)\\right]\n",
    "  $$\n",
    "  where $ f $ and $ g $ represent the encoder and decoder functions respectively, and $ d(\\cdot) $ is a suitable difference measure.\n",
    "  \n",
    "- **Advantages:**  \n",
    "  This unified approach avoids the need for task-specific architectures and simplifies the learning process across various applications.\n",
    "\n",
    "### 5. XLNet\n",
    "\n",
    "- **Combining Strengths from BERT and GPT:**  \n",
    "  XLNet addresses some of the limits of BERT while incorporating the sequential prediction method of GPT. \n",
    "\n",
    "- **Permutation-Based Objective:**  \n",
    "  Instead of masking fixed tokens, XLNet randomizes the order of the entire sequence and trains the model to predict tokens based on a permuted context. This method is known as Permutation Language Modeling (PLM).\n",
    "\n",
    "- **Two-Stream Self-Attention:**  \n",
    "  XLNet employs a method where both content-based and query-based streams of self-attention are used. This helps the model capture long-range dependencies with greater precision.\n",
    "\n",
    "- **Comparison to BERT:**  \n",
    "  XLNet often shows improved performance in tasks where capturing long-range dependencies is essential.\n",
    "\n",
    "### Model Sizes: Base versus Large\n",
    "\n",
    "- **Base Models:**  \n",
    "  These models offer a balance between performance and computational efficiency. For example, **BERT-base** consists of 12 layers with 768-dimensional hidden states, totaling roughly 110 million parameters.\n",
    "\n",
    "- **Large Models:**  \n",
    "  Larger versions have more layers and/or wider hidden states. **BERT-large** uses 24 layers with 1024-dimensional hidden states, which results in approximately 340 million parameters.\n",
    "\n",
    "- **Trade-Offs:**  \n",
    "  While larger models generally offer improved performance due to their ability to encode more complex patterns, they demand significantly more computation and memory. Overfitting is also a risk when training on small datasets, making base models sometimes a more practical choice.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformers as Feature Extractors\n",
    "\n",
    "Transformers can provide dense vector representations of text, capturing both syntactic and contextual features. These vectors are powerful features for many downstream tasks.\n",
    "\n",
    "### What Are Features?\n",
    "\n",
    "- **Definition:**  \n",
    "  Features are numerical representations that capture key properties of the input data. In NLP, these might include:\n",
    "  - Word frequency\n",
    "  - Sentence structure\n",
    "  - Contextual semantics\n",
    "\n",
    "- **Role in Machine Learning:**  \n",
    "  These features serve as inputs to further processing steps, allowing models to perform tasks like classification or regression.\n",
    "\n",
    "### How Transformers Extract Features\n",
    "\n",
    "- **Process:**  \n",
    "  1. An input text is tokenized.\n",
    "  2. Tokens are converted into vectors (embeddings).\n",
    "  3. The transformer processes these embeddings through layers of self-attention, computing contextualized representations.\n",
    "  \n",
    "  A simplified representation calculation in a transformer layer is:\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$\n",
    "  where $ Q $, $ K $, and $ V $ represent queries, keys, and values, and $ d_k $ is the dimension of the key vectors.\n",
    "\n",
    "- **Benefits:**  \n",
    "  - **Rich Representations:** Captures local and global context of the text.\n",
    "  - **Flexibility:** Works on both labeled and unlabeled data.\n",
    "  \n",
    "- **Challenges:**  \n",
    "  - **Computational Demands:** Requires significant processing power.\n",
    "  - **Memory Footprint:** The large number of parameters demands greater memory resources.\n",
    "\n",
    "> **Note:** While there is a high computational cost, the quality of features extracted by transformers justifies the use in many high-level NLP applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Steps for Using Transformers\n",
    "\n",
    "To use transformers on a specific task, we need to follow these steps:\n",
    "\n",
    "### Step 1: Start with a Pretrained Model\n",
    "\n",
    "The first step is to select a pretrained model from the [Hugging Face Transformers library](https://huggingface.co/transformers/pretrained_models.html). These pretrained models have been trained on large amounts of text data and have learned general language representations. Using a pretrained model provides a strong foundation for your specific task.\n",
    "\n",
    "*Note:* Training a model from scratch is an advanced topic and rarely necessary. In most cases, you can warm-start your model from a pretrained model. If you're interested in learning more about training your own model from scratch, refer to [this resource](https://huggingface.co/blog/how-to-train).\n",
    "\n",
    "### Step 2: Fine-tune the Model on Domain-Specific Text (Optional)\n",
    "\n",
    "Fine-tuning involves adapting a pretrained model to a new domain by training it on domain-specific text. This step is optional but can enhance the model's performance on your specific task. By exposing the model to text that is similar to your target domain, it can learn domain-specific language patterns and representations.\n",
    "\n",
    "### Step 3: Train the Model for Your Task\n",
    "\n",
    "Once you have a fine-tuned model (or a pretrained model if you skipped step 2), you can train it for your specific task. This typically involves adding a classification or regression head on top of the model and training it using your task-specific data.\n",
    "\n",
    "Alternatively, you can use the model as a feature extractor for your task, which is a more advanced approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classifying Court Decision Labels\n",
    "\n",
    "Let's explore these steps using a subset of the [BrCAD-5](https://www.kaggle.com/datasets/eliasjacob/brcad5) dataset, which contains over 765,000 legal case information from Brazilian Federal Courts. Our goal is to train a model to predict the label for a court decision based on its text.\n",
    "\n",
    "1. **Select a Pretrained Model**: We'll choose a suitable pretrained model from the Hugging Face Transformers library that aligns with our task requirements, such as language support and model architecture.\n",
    "\n",
    "2. **Fine-tune the Model (Optional)**: If we have a sufficient amount of domain-specific text (legal case information in this case), we can fine-tune the pretrained model on this data to capture domain-specific language patterns.\n",
    "\n",
    "3. **Train the Model for Label Prediction**: We'll add a classification head on top of the model and train it using the labeled court decision data from BrCAD-5. The model will learn to predict the appropriate label based on the text of the court decision.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Remember, the key is to start with a strong pretrained model and adapt it to your specific task through fine-tuning and task-specific training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at neuralmind/bert-large-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Define the model checkpoints for the base and large versions of the BERT model\n",
    "model_checkpoint_base = \"neuralmind/bert-base-portuguese-cased\"\n",
    "model_checkpoint_large = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# Load the tokenizer for the base BERT model\n",
    "# The tokenizer is responsible for converting text into tokens that the model can understand\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_checkpoint_base)\n",
    "\n",
    "# Load the masked language model (MLM) for the base BERT model\n",
    "# The MLM is used for tasks like predicting masked words in a sentence\n",
    "model_mlm_base = AutoModelForMaskedLM.from_pretrained(model_checkpoint_base)\n",
    "\n",
    "# Load the tokenizer for the large BERT model\n",
    "# This tokenizer works similarly to the base tokenizer but is tailored for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(model_checkpoint_large)\n",
    "\n",
    "# Load the masked language model (MLM) for the large BERT model\n",
    "# This MLM is used for tasks like predicting masked words in a sentence, similar to the base model but with more parameters\n",
    "model_mlm_large = AutoModelForMaskedLM.from_pretrained(model_checkpoint_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_base.is_fast  # A fast tokenizer from HF Transformers uses Rust under the hood for faster tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 108,954,466 trainable parameters\n",
      "The model has 334,428,258 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    # Sum the number of elements (numel) for each parameter in the model\n",
    "    # Only include parameters that require gradients (i.e., are trainable)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # Print the number of trainable parameters in a human-readable format with commas\n",
    "    print(f\"The model has {n_parameters:,} trainable parameters\")\n",
    "\n",
    "\n",
    "# Count and print the number of trainable parameters for the base BERT model\n",
    "count_parameters(model_mlm_base)\n",
    "\n",
    "# Count and print the number of trainable parameters for the large BERT model\n",
    "count_parameters(model_mlm_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the model architecture summary of a BERT (Bidirectional Encoder Representations from Transformers) model specifically designed for masked language modeling (MLM) tasks. Let's break it down:\n",
    "\n",
    "### 1. Overall Architecture: `BertForMaskedLM`\n",
    "\n",
    "- **Goal:**  \n",
    "  The model combines a pretrained BERT backbone with an additional output head designed for MLM tasks. This head produces vocabulary logits for each token, especially focusing on the tokens that were masked in the input.\n",
    "\n",
    "\n",
    "### 2. Base Model: `BertModel`\n",
    "\n",
    "`BertModel` forms the structural backbone and consists of two major components: token embeddings and stacked encoder layers. \n",
    "\n",
    "#### A. BertEmbeddings\n",
    "\n",
    "This module converts input tokens into dense vectors by learning several types of embeddings:\n",
    "\n",
    "- **Word Embeddings:**  \n",
    "  Each token is mapped to a vector using a learned embedding matrix with shape:  \n",
    "  $$\n",
    "  (29794,\\,768)\n",
    "  $$\n",
    "  Here, 29,794 represents the vocabulary size and 768 is the hidden size.\n",
    "\n",
    "- **Position Embeddings:**  \n",
    "  The model uses position embeddings to encode the order of tokens. This is achieved with a learned embedding matrix of shape:  \n",
    "  $$\n",
    "  (512,\\,768)\n",
    "  $$\n",
    "\n",
    "- **Token Type Embeddings:**  \n",
    "  When processing pair sequences (such as question-answer pairs), token type embeddings help the model distinguish between different segments. The corresponding matrix has the shape:  \n",
    "  $$\n",
    "  (2,\\,768)\n",
    "  $$\n",
    "\n",
    "- **Combination Process:**  \n",
    "  All three embeddings are summed element-wise for each token:\n",
    "  $$\n",
    "  h^{(i)}_0 = e_{\\text{word}}^{(i)} + e_{\\text{pos}}^{(i)} + e_{\\text{type}}^{(i)}\n",
    "  $$\n",
    "  This sum is then normalized using layer normalization (with an epsilon value of $1 \\times 10^{-12}$) and regularized using dropout with a probability of 0.1.\n",
    "\n",
    "#### B. BertEncoder\n",
    "\n",
    "The encoder consists of a stack of 12 identical layers (numbered 0 to 11 in the base model). Each layer (or **BertLayer**) has the following subcomponents:\n",
    "\n",
    "- **BertAttention:**  \n",
    "  This implements multi-head self-attention and comprises two main parts:\n",
    "  \n",
    "  - **Self-Attention Mechanism:**  \n",
    "    For every token, the model computes query (Q), key (K), and value (V) vectors using linear transformations. Each transformation maps an input vector of dimension 768 to an output of the same size. Dropout is applied to the attention weights to reduce overfitting.\n",
    "\n",
    "  - **Output Processing:**  \n",
    "    After computing the attention scores, the resulting vectors are merged and passed through a linear layer to project them back to the 768-dimensional space. This is followed by layer normalization and dropout.\n",
    "\n",
    "- **BertIntermediate:**  \n",
    "  This component applies a dense layer that increases the dimensionality from 768 to 3072. The non-linear activation function used is GELU. The transformation can be expressed as:\n",
    "  $$\n",
    "  z_i = \\text{GELU}(h_i \\cdot W_1 + b_1)\n",
    "  $$\n",
    "  where $h_i$ is the output from the attention sublayer.\n",
    "\n",
    "- **BertOutput:**  \n",
    "  The intermediate representation is then projected back to the hidden size (768) using another linear transformation:\n",
    "  $$\n",
    "  h^{\\text{new}}_i = \\text{GELU}(z_i \\cdot W_2 + b_2)\n",
    "  $$\n",
    "  This step also includes layer normalization and dropout, and it adds a residual connection from the original input, ensuring training stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Masked Language Modeling Head: `BertOnlyMLMHead`\n",
    "\n",
    "The MLM head is designed to transform the encoder's output into predictions over the vocabulary for each masked token.\n",
    "\n",
    "- **BertLMPredictionHead:**  \n",
    "  This component orchestrates the final steps in transforming the encoder output into prediction logits.\n",
    "\n",
    "- **Prediction Head Transformation:**  \n",
    "  The output of the final encoder layer undergoes an additional dense transformation followed by a GELU activation and layer normalization:\n",
    "  $$\n",
    "  t_i = \\text{GELU}(h_L^{(i)} \\cdot W_3 + b_3)\n",
    "  $$\n",
    "  Here, $h_L^{(i)}$ is the representation of the $i$-th token from the final encoder layer.\n",
    "\n",
    "- **Decoder:**  \n",
    "  A final linear layer converts the transformed representation $t_i$ into logits corresponding to the vocabulary terms:\n",
    "  $$\n",
    "  \\text{logits}_i = t_i \\cdot W_{E}^T + b_4\n",
    "  $$\n",
    "  The decoder uses weights that directly map from the hidden size (768) to the vocabulary size (29794).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlm_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main differences between the two BERT models are in the model size and architecture:\n",
    ">\n",
    "> 1. Embedding dimensions:\n",
    ">       - In the first model, the word embeddings, position embeddings, and token type embeddings have a dimension of 768.\n",
    ">       - In the second model, these embeddings have a dimension of 1024, indicating a larger embedding size.\n",
    ">\n",
    "> 2. Number of encoder layers:\n",
    ">       - The first model has 12 encoder layers (`(0-11): 12 x BertLayer`).\n",
    ">       - The second model has 24 encoder layers (`(0-23): 24 x BertLayer`)\n",
    ">\n",
    "> 3. Intermediate layer dimensions:\n",
    ">       - In the first model, the intermediate layer (`BertIntermediate`) has an output dimension of 3072.\n",
    ">       - In the second model, the intermediate layer has an output dimension of 4096, which is larger than the first model.\n",
    ">\n",
    "> 4. Hidden state dimensions:\n",
    ">       - The first model uses hidden states with a dimension of 768 throughout the architecture, including the self-attention layers, intermediate layers, and output layers.\n",
    ">       - The second model uses hidden states with a dimension of 1024 throughout the architecture.\n",
    ">\n",
    "> The rest of the architecture, including the self-attention mechanism, layer normalization, dropout, and the MLM head, remains the same between the two models.\n",
    ">\n",
    "> The large model has higher-dimensional embeddings, more encoder layers, and larger intermediate layer dimensions. This suggests that the large model has a higher capacity and can potentially capture more complex patterns and representations from the input data. However, the larger model size also means increased computational requirements and longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and creating a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58529, 1), (6504, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the unlabeled dataset from a Parquet file\n",
    "# Only the 'text' column is read from the file\n",
    "df_unlabeled = pd.read_parquet(\"data/legal/unlabeled_texts.parquet\", columns=[\"text\"])\n",
    "\n",
    "# Split the unlabeled dataset into training and validation sets\n",
    "# 10% of the data is used for validation, and the split is reproducible with a fixed random state\n",
    "df_unlabeled_train, df_unlabeled_valid = train_test_split(\n",
    "    df_unlabeled, test_size=0.10, random_state=271828\n",
    ")\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "# This shows the number of rows and columns in each set\n",
    "df_unlabeled_train.shape, df_unlabeled_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the pandas DataFrame containing the unlabeled training data into a Hugging Face Dataset\n",
    "# This allows for easier manipulation and integration with Hugging Face's tools and models\n",
    "dataset_unlabeled_train = datasets.Dataset.from_pandas(df_unlabeled_train)\n",
    "\n",
    "# Convert the pandas DataFrame containing the unlabeled validation data into a Hugging Face Dataset\n",
    "# This allows for easier manipulation and integration with Hugging Face's tools and models\n",
    "dataset_unlabeled_valid = datasets.Dataset.from_pandas(df_unlabeled_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 58529\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 6504\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the path to save the outputs of the base BERT masked language model\n",
    "path_to_save_lm_base = Path(\"./outputs/transformers_basics/bert_masked_lm_base\")\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the path to save the outputs of the large BERT masked language model\n",
    "path_to_save_lm_large = Path(\"./outputs/transformers_basics/bert_masked_lm_large\")\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm_large.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the Language Model on the domain text\n",
    "\n",
    "Remember our transfer learning class. During this stage, the general-domain language model adapts itself to the idiosyncrasies of the domain-specific text. This is done by training the model on the domain-specific text. This step is optional, but it can improve the performance of the model on your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeaa4fb870547338b863921e86c8872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b812311da1462e9c7f0bb632e10450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf79fea69ba4fc8af7ef4f19093fdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01167fc99d674691a6b18f6dff4ad689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text in the given examples using the tokenizer object.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the tokenized input text.\n",
    "    \"\"\"\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"], truncation=False, padding=False\n",
    "    )  # Tokenize the input text\n",
    "    if tokenizer.is_fast:\n",
    "        # If the tokenizer is a fast tokenizer, add word IDs to the result\n",
    "        result[\"word_ids\"] = [\n",
    "            result.word_ids(i) for i in range(len(result[\"input_ids\"]))\n",
    "        ]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Create partial functions for tokenizing using the base and large tokenizers\n",
    "# This allows us to pass the tokenizer as a fixed argument to the tokenize_function\n",
    "tokenize_function_base = partial(tokenize_function, tokenizer=tokenizer_base)\n",
    "tokenize_function_large = partial(tokenize_function, tokenizer=tokenizer_large)\n",
    "\n",
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The map function applies the tokenize_function_base to each example in the dataset\n",
    "# The batched=True argument processes the examples in batches for efficiency\n",
    "# The remove_columns argument removes the specified columns from the dataset after tokenization\n",
    "dataset_train_tokenized_mlm_base = dataset_unlabeled_train.map(\n",
    "    tokenize_function_base, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_valid_tokenized_mlm_base = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_base, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# Tokenize the training dataset using the large tokenizer\n",
    "dataset_train_tokenized_mlm_large = dataset_unlabeled_train.map(\n",
    "    tokenize_function_large, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the large tokenizer\n",
    "dataset_valid_tokenized_mlm_large = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_large, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b240a474ee244b69b77c71177e6033f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c38bdee77942ccb31c91ea0a2f889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053902e0a84a4407a8ecef19f000724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de9916194024145935b74ba826c6519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples for base model: 271519\n",
      "Number of validation examples for base model: 29823\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    This function groups together a set of texts as contiguous text of fixed length (chunk_size).\n",
    "    It's useful for training masked language models.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the examples to group. Each key corresponds to a feature,\n",
    "                and each value is a list of lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the grouped examples. Each key corresponds to a feature,\n",
    "      and each value is a list of lists of tokens.\n",
    "    \"\"\"\n",
    "    # Concatenate all texts for each feature\n",
    "    concatenated_examples = {k: np.concatenate(examples[k]) for k in examples.keys()}\n",
    "\n",
    "    # Compute the total length of the concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Adjust the total length to be a multiple of chunk_size, dropping the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "\n",
    "    # Split the concatenated texts into chunks of size chunk_size using NumPy\n",
    "    result = {\n",
    "        k: np.split(t[:total_length], total_length // chunk_size)\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create a new 'labels' column that is a copy of the 'input_ids' column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Define the chunk size for grouping texts\n",
    "chunk_size = 512\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset for the base BERT model\n",
    "dataset_train_tokenized_mlm_base = dataset_train_tokenized_mlm_base.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset for the base BERT model\n",
    "dataset_valid_tokenized_mlm_base = dataset_valid_tokenized_mlm_base.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset for the large BERT model\n",
    "dataset_train_tokenized_mlm_large = dataset_train_tokenized_mlm_large.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset for the large BERT model\n",
    "dataset_valid_tokenized_mlm_large = dataset_valid_tokenized_mlm_large.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of training examples for base model: {len(dataset_train_tokenized_mlm_base)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of validation examples for base model: {len(dataset_valid_tokenized_mlm_base)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM) using the base BERT tokenizer\n",
    "# The data collator will dynamically mask tokens in the input text with a probability of 0.15\n",
    "data_collator_mlm_base = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_base, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM) using the large BERT tokenizer\n",
    "# The data collator will dynamically mask tokens in the input text with a probability of 0.15\n",
    "data_collator_mlm_large = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_large, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation using the base BERT model\n",
    "batch_size_base = 20\n",
    "\n",
    "# Extract the model name from the model checkpoint path for the base BERT model\n",
    "model_name_base = model_checkpoint_base.split(\"/\")[-1]\n",
    "\n",
    "# Set up the training arguments for fine-tuning the base BERT model on a masked language modeling task\n",
    "training_args_mlm_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base\n",
    "    / f\"{model_name_base}-finetuned-mlm\",  # Directory to save the model checkpoints\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size_base,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size_base,  # Batch size for evaluation\n",
    "    bf16=True,  # Use bfloat16 precision (change to \"fp16\" if using a free GPU)\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_steps=1,  # Log the training loss after every 1 epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to use for selecting the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=3,  # Number of gradient accumulation steps\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the base BERT model\n",
    "# The Trainer class provides an easy-to-use API for training and evaluating models\n",
    "trainer_mlm_base = Trainer(\n",
    "    model=model_mlm_base,  # The model to be trained (base BERT masked language model)\n",
    "    args=training_args_mlm_base,  # Training arguments defined earlier\n",
    "    train_dataset=dataset_train_tokenized_mlm_base,  # Tokenized training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_mlm_base,  # Tokenized validation dataset\n",
    "    data_collator=data_collator_mlm_base,  # Data collator for dynamically masking tokens\n",
    "    processing_class=tokenizer_base,  # Tokenizer for processing the input text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meliasjacob\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nas-elias/drive/UFRN/Disciplinas/2024-1/Dell Deep Learning and Gen-AI/wandb/run-20240726_063716-qwihhcur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eliasjacob/huggingface/runs/qwihhcur' target=\"_blank\">outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm</a></strong> to <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eliasjacob/huggingface/runs/qwihhcur' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface/runs/qwihhcur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6786' max='6786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6786/6786 4:50:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.579700</td>\n",
       "      <td>0.473443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.408905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6786, training_loss=0.5313287478850508, metrics={'train_runtime': 17450.1629, 'train_samples_per_second': 46.679, 'train_steps_per_second': 0.389, 'total_flos': 2.143305955958661e+17, 'train_loss': 0.5313287478850508, 'epoch': 2.9991160872127285})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This took around 5 hours to train on 2 x NVIDIA RTX 3090 GPUs\n",
    "trainer_mlm_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='746' max='746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [746/746 04:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.38791826367378235,\n",
       " 'eval_runtime': 293.0877,\n",
       " 'eval_samples_per_second': 101.755,\n",
       " 'eval_steps_per_second': 2.545,\n",
       " 'epoch': 2.9991160872127285}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer_mlm_base.save_model(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")\n",
    "tokenizer_base.save_pretrained(\n",
    "    path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "trainer_mlm_base.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the trainer, model, and tokenizer for the base BERT model to None\n",
    "# This helps free up memory by removing references to these objects\n",
    "trainer_mlm_base = None\n",
    "model_mlm_base = None\n",
    "tokenizer_base = None\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Clear the CUDA memory cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation\n",
    "batch_size_large = 14\n",
    "\n",
    "# Extract the model name from the model checkpoint path\n",
    "# This will be used to name the output directory for the trained model\n",
    "model_name_large = model_checkpoint_large.split(\"/\")[-1]\n",
    "\n",
    "# Define the training arguments for the large masked language model (MLM)\n",
    "training_args_mlm_large = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_large\n",
    "    / f\"{model_name_large}-finetuned-mlm\",  # Output directory for the trained model\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it already exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size_large,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size_large,  # Batch size for evaluation\n",
    "    bf16=True,  # Use bf16 precision. Change to \"fp16\" if using a free GPU\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints and delete the older ones\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_steps=1,  # Log the training loss after every 1 step\n",
    "    eval_steps=1,  # Evaluate the model after every 1 step\n",
    "    save_steps=1,  # Save the model after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use the evaluation loss to determine the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating the model parameters\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the large masked language model (MLM)\n",
    "trainer_mlm_large = Trainer(\n",
    "    model=model_mlm_large,  # The pre-trained large BERT model for masked language modeling\n",
    "    args=training_args_mlm_large,  # The training arguments defined earlier for the large model\n",
    "    train_dataset=dataset_train_tokenized_mlm_large,  # The tokenized training dataset for the large model\n",
    "    eval_dataset=dataset_valid_tokenized_mlm_large,  # The tokenized validation dataset for the large model\n",
    "    data_collator=data_collator_mlm_large,  # The data collator for dynamic masking during training\n",
    "    processing_class=tokenizer_large,  # The tokenizer used to process the input text for the large model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meliasjacob\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nas-elias/drive/UFRN/Disciplinas/2024-1/Dell Deep Learning and Gen-AI/wandb/run-20240727_062917-mpbgvodr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eliasjacob/huggingface/runs/mpbgvodr' target=\"_blank\">outputs/transformers_basics/bert_masked_lm_large/bert-large-portuguese-cased-finetuned-mlm</a></strong> to <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eliasjacob/huggingface/runs/mpbgvodr' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface/runs/mpbgvodr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7272' max='7272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7272/7272 13:25:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.381371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>0.309468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7272, training_loss=0.4201359052510217, metrics={'train_runtime': 48363.8901, 'train_samples_per_second': 16.842, 'train_steps_per_second': 0.15, 'total_flos': 7.590524853366497e+17, 'train_loss': 0.4201359052510217, 'epoch': 2.999381315735203})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the large masked language model (MLM)\n",
    "# This process involves multiple epochs of training on the training dataset\n",
    "# Note: This training process took almost 14 hours on 2 x NVIDIA RTX 3090 GPUs\n",
    "trainer_mlm_large.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1066' max='1066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1066/1066 11:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30810099840164185,\n",
       " 'eval_runtime': 709.2879,\n",
       " 'eval_samples_per_second': 42.046,\n",
       " 'eval_steps_per_second': 1.503,\n",
       " 'epoch': 2.999381315735203}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained large masked language model (MLM) to the specified directory\n",
    "trainer_mlm_large.save_model(\n",
    "    path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Save the tokenizer used for the large MLM to the same directory\n",
    "tokenizer_large.save_pretrained(\n",
    "    path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Evaluate the trained large MLM on the validation dataset\n",
    "# This will return a dictionary containing the evaluation metrics\n",
    "trainer_mlm_large.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/transformers_basics/bert_masked_lm_large/bert-large-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing a Language Model\n",
    "\n",
    "To ensure that a language model is effective and reliable, we need to assess its performance. This is usually done by evaluating how well the model can predict a word in a sentence. The primary metric used for this purpose is known as 'Perplexity'.\n",
    "\n",
    "### Understanding Perplexity\n",
    "\n",
    "Perplexity is a quantitative measure of how well a probability model predicts a sample. concerning language models, it gauges how surprised or 'thrown-off' the model is upon encountering new data. Essentially, it is a measure of \"surprise\".\n",
    "\n",
    "A lower perplexity indicates that the model was less surprised by the new data, signifying that it was better trained and has a good understanding of the language patterns in the provided data. Therefore, a lower perplexity value is indicative of better training.\n",
    "\n",
    "### Calculating Perplexity\n",
    "\n",
    "Perplexity is defined as the exponentiation of the entropy. Entropy is a measure of the uncertainty associated with a random variable. Since the loss function of the language model is the cross-entropy loss, we can use the loss value to calculate the perplexity. The formula for perplexity is:\n",
    "\n",
    "$$Perplexity = e^{loss}$$\n",
    "\n",
    "Where:\n",
    "- $e$ is the base of the natural logarithm (Euler's number, approximately 2.71828)\n",
    "- $loss$ is the cross-entropy loss\n",
    "\n",
    "### Choice of Logarithm Base\n",
    "\n",
    "The choice of base for the logarithm in calculating perplexity or entropy often depends on the context or the historical convention of the field.\n",
    "\n",
    "- In information theory, the base of the logarithm is typically 2, resulting in units of bits (binary digits). This is because information was originally conceptualized concerning binary decisions (yes/no, true/false, 0/1), and thus, using a base-2 logarithm is natural: a message space of $2^n$ messages each carry $n$ bits of information.\n",
    "\n",
    "- The rationale behind using $e$ as the base is somewhat unclear. In numerous domains of machine learning, $e$ possesses unique attributes, however, these properties do not hold relevance here. Euler's number ($e$) exhibits several intriguing properties, especially in machine learning, where a majority of the basic mathematical principles and techniques (like calculus and optimization methods) often function more efficiently or are simpler with natural logarithms.\n",
    "\n",
    "> It's important to note that the base of the logarithm doesn't change the fundamental interpretation of entropy or perplexity - it's merely a scaling factor. However, base-2 logarithms will give you a measure in bits, while natural logarithms will give you a measure in nats (natural units of information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the trainer for the large masked language model (MLM) to None to free up memory\n",
    "trainer_mlm_large = None\n",
    "\n",
    "# Set the large masked language model (MLM) to None to free up memory\n",
    "model_mlm_large = None\n",
    "\n",
    "# Set the tokenizer for the large MLM to None to free up memory\n",
    "tokenizer_large = None\n",
    "\n",
    "# Collect garbage to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the base model is 1.4739093074325733\n",
      "The perplexity for the large model is 1.3608384245024145\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(f\"The perplexity for the base model is {math.exp(0.38791826367378235)}\")\n",
    "print(f\"The perplexity for the large model is {math.exp(0.30810099840164185)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Define the path to save the base masked language model (MLM)\n",
    "# This path points to the directory where the base MLM model will be saved\n",
    "path_to_save_lm_base = Path(\"./outputs/transformers_basics/bert_masked_lm_base\")\n",
    "\n",
    "# Define the path to save the large masked language model (MLM)\n",
    "# This path points to the directory where the large MLM model will be saved\n",
    "path_to_save_lm_large = Path(\"./outputs/transformers_basics/bert_masked_lm_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned base masked language model (MLM) from the specified directory\n",
    "# This model is a BERT base model fine-tuned on a Portuguese dataset\n",
    "model_base = AutoModelForMaskedLM.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base MLM from the same directory\n",
    "# The tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned large masked language model (MLM) from the specified directory\n",
    "# This model is a BERT large model fine-tuned on a Portuguese dataset\n",
    "model_large = AutoModelForMaskedLM.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large MLM from the same directory\n",
    "# The tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for the base masked language model (MLM)\n",
    "# The pipeline is used to fill in the masked tokens in the input text\n",
    "# 'fill-mask' specifies the task type for the pipeline\n",
    "# model_base is the fine-tuned base MLM model\n",
    "# tokenizer_base is the tokenizer for the base MLM model\n",
    "# top_k=5 specifies that the top 5 predictions for the masked token will be returned\n",
    "pipe_base = pipeline(\"fill-mask\", model=model_base, tokenizer=tokenizer_base, top_k=5)\n",
    "\n",
    "# Create a pipeline for the large masked language model (MLM)\n",
    "pipe_large = pipeline(\n",
    "    \"fill-mask\", model=model_large, tokenizer=tokenizer_large, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9281540513038635,\n",
       "  'token': 131,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de :'},\n",
       " {'score': 0.012005731463432312,\n",
       "  'token': 21982,\n",
       "  'token_str': 'homicídio',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de homicídio'},\n",
       " {'score': 0.0050378949381411076,\n",
       "  'token': 18144,\n",
       "  'token_str': 'roubo',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de roubo'},\n",
       " {'score': 0.0032502533867955208,\n",
       "  'token': 1112,\n",
       "  'token_str': '“',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de “'},\n",
       " {'score': 0.0027919497806578875,\n",
       "  'token': 184,\n",
       "  'token_str': 're',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de re'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base(\"O artigo 121 do Código Penal prevê o crime de [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9837086796760559,\n",
       "  'token': 131,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de :'},\n",
       " {'score': 0.006448815111070871,\n",
       "  'token': 21982,\n",
       "  'token_str': 'homicídio',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de homicídio'},\n",
       " {'score': 0.0011397271882742643,\n",
       "  'token': 119,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de.'},\n",
       " {'score': 0.0007863400387577713,\n",
       "  'token': 1386,\n",
       "  'token_str': 'morte',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de morte'},\n",
       " {'score': 0.0007723842863924801,\n",
       "  'token': 9566,\n",
       "  'token_str': 'corrupção',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de corrupção'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_large(\"O artigo 121 do Código Penal prevê o crime de [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3354406952857971,\n",
       "  'token': 17225,\n",
       "  'token_str': 'julgado',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em julgado para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.2519117295742035,\n",
       "  'token': 5370,\n",
       "  'token_str': 'aberto',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em aberto para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.2214008867740631,\n",
       "  'token': 21244,\n",
       "  'token_str': 'dobro',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em dobro para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.06615797430276871,\n",
       "  'token': 3418,\n",
       "  'token_str': 'curso',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em curso para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.03829769790172577,\n",
       "  'token': 4712,\n",
       "  'token_str': 'branco',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em branco para interposição de recurso pela Fazenda Pública'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base(\n",
    "    \"O Código de Processo Civil prevê prazo em [MASK] para interposição de recurso pela Fazenda Pública\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5983186960220337,\n",
       "  'token': 2241,\n",
       "  'token_str': 'lei',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em lei para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.31673961877822876,\n",
       "  'token': 21244,\n",
       "  'token_str': 'dobro',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em dobro para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.015298635698854923,\n",
       "  'token': 2502,\n",
       "  'token_str': 'Lei',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em Lei para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.01301574520766735,\n",
       "  'token': 20554,\n",
       "  'token_str': 'razoável',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em razoável para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.009656175971031189,\n",
       "  'token': 5370,\n",
       "  'token_str': 'aberto',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em aberto para interposição de recurso pela Fazenda Pública'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_large(\n",
    "    \"O Código de Processo Civil prevê prazo em [MASK] para interposição de recurso pela Fazenda Pública\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our Document Classifier Using Our Fine-Tuned Language Model\n",
    "\n",
    "### Understanding the Language Model Output Structure\n",
    "\n",
    "Before diving into the details of document classification, it's essential to grasp the structure of the output from the language model. The output is a vector with dimensions of `max_tokens` x `embedding_dimension`. Taking BERT-base as an example, the embedding dimension is 768. This means that for each token in the input text, there is a corresponding vector of size 768.\n",
    "\n",
    "In practical scenarios, utilizing the entire array of vectors as input for our classifier may not be feasible due to the vast amount of information involved. Instead, we focus on using the vector corresponding to the `[CLS]` token.\n",
    "\n",
    "### The Significance of the `[CLS]` Token\n",
    "\n",
    "The `[CLS]` token is a special token that precedes the input text and represents the entirety of the input concerning BERT models. This token's vector size is 768, which is significantly more manageable compared to the entire vector array. `[CLS]` stands for `CL`a`S`sification and is specifically designed for classification tasks.\n",
    "\n",
    "Here's an example to illustrate the usage of the `[CLS]` token:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "outputs = tokenizer('Eu gosto muito de farofa')\n",
    "tokenizer.decode(outputs['input_ids'])\n",
    "```\n",
    "\n",
    "Resulting output: `'[CLS] Eu gosto muito de farofa [SEP]'`\n",
    "\n",
    "In the above output, you'll notice that the `[CLS]` token is added to the start of the input text, while the `[SEP]` token is appended to the end. However, for classification purposes, we only need to focus on the `[CLS]` token and can ignore the `[SEP]` token. The role of the `[SEP]` token in BERT is to enable the separation of two sentences, but since our input text contains only one sentence, its usage is unnecessary here.\n",
    "\n",
    "### Carrying Out Classification Using the `[CLS]` Token\n",
    "\n",
    "Now that we know how to extract the vector for the `[CLS]` token, we can use it as input for our classifier. The classifier's output will be a vector of size `num_labels`, where `num_labels` refers to the number of labels present in our dataset. For example, if we have 4 labels, the classifier would output a vector of size 4.\n",
    "\n",
    "This output vector will be crucial in calculating the model's loss and updating its weights during the training process. By comparing the predicted label probabilities with the actual labels, we can measure the model's performance and make necessary adjustments to improve its accuracy.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "To summarize, the process of document classification using a fine-tuned language model involves the following steps:\n",
    "\n",
    "1. Tokenize the input text and add the `[CLS]` token at the beginning.\n",
    "2. Pass the tokenized input through the language model to obtain the output vector.\n",
    "3. Extract the vector corresponding to the `[CLS]` token.\n",
    "4. Use the `[CLS]` token vector as input for the classifier.\n",
    "5. Obtain the classifier's output vector, which represents the predicted label probabilities.\n",
    "6. Calculate the loss by comparing the predicted labels with the actual labels.\n",
    "7. Update the model's weights based on the calculated loss to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52026, 2), (13007, 2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the training dataset from a Parquet file\n",
    "# Only the 'text' and 'label' columns are read from the file\n",
    "df_train = pd.read_parquet(\"data/legal/train.parquet\", columns=[\"text\", \"label\"])\n",
    "\n",
    "# Load the validation dataset from a Parquet file\n",
    "# Only the 'text' and 'label' columns are read from the file\n",
    "df_valid = pd.read_parquet(\"data/legal/valid.parquet\", columns=[\"text\", \"label\"])\n",
    "\n",
    "# Define the path to save the base masked language model (MLM)\n",
    "# This path points to the directory where the base MLM model will be saved\n",
    "path_to_save_lm_base = Path(\"./outputs/transformers_basics/bert_masked_lm_base\")\n",
    "\n",
    "# Define the path to save the large masked language model (MLM)\n",
    "# This path points to the directory where the large MLM model will be saved\n",
    "path_to_save_lm_large = Path(\"./outputs/transformers_basics/bert_masked_lm_large\")\n",
    "\n",
    "# Display the shapes of the training and validation datasets\n",
    "# This shows the number of rows and columns in each dataset\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'IMPROCEDENTE': 0,\n",
       "  'PROCEDENTE': 1,\n",
       "  'PARCIALMENTE PROCEDENTE': 2,\n",
       "  'EXTINTO SEM MÉRITO': 3},\n",
       " {0: 'IMPROCEDENTE',\n",
       "  1: 'PROCEDENTE',\n",
       "  2: 'PARCIALMENTE PROCEDENTE',\n",
       "  3: 'EXTINTO SEM MÉRITO'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map each unique label in the training dataset to a unique ID\n",
    "# df_train.label.unique() returns an array of unique labels in the training dataset\n",
    "# The dictionary comprehension iterates over the unique labels and assigns an ID to each label\n",
    "label2id = {df_train.label.unique()[i]: i for i in range(len(df_train.label.unique()))}\n",
    "\n",
    "# Create a dictionary to map each unique ID back to its corresponding label\n",
    "# This is the reverse mapping of the label2id dictionary\n",
    "# The dictionary comprehension iterates over the items in label2id and swaps the keys and values\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Display the label-to-ID and ID-to-label mappings\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>SENTENÇA Vistos etc. Dispensado o relatório, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17972</th>\n",
       "      <td>SENTENÇA Relatório dispensado. No caso, não há...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34527</th>\n",
       "      <td>SENTENÇA Vistos etc. Trata-se de pedido de res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58381</th>\n",
       "      <td>TERMO DE AUDIÊNCIA DE INSTRUÇÃO Ação Especial ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56474</th>\n",
       "      <td>SENTENÇA Trata-se de ação em que a parte autor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1387   SENTENÇA Vistos etc. Dispensado o relatório, a...      0\n",
       "17972  SENTENÇA Relatório dispensado. No caso, não há...      0\n",
       "34527  SENTENÇA Vistos etc. Trata-se de pedido de res...      1\n",
       "58381  TERMO DE AUDIÊNCIA DE INSTRUÇÃO Ação Especial ...      1\n",
       "56474  SENTENÇA Trata-se de ação em que a parte autor...      2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the labels in the training dataset to their corresponding IDs\n",
    "# This replaces the label names with their respective IDs using the label2id dictionary\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "\n",
    "# Map the labels in the validation dataset to their corresponding IDs\n",
    "# This replaces the label names with their respective IDs using the label2id dictionary\n",
    "df_valid[\"label\"] = df_valid[\"label\"].map(label2id)\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "# This shows the updated training dataset with labels replaced by their corresponding IDs\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the training DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for training and evaluation\n",
    "dataset_labeled_train = datasets.Dataset.from_pandas(df_train)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for validation and evaluation\n",
    "dataset_labeled_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to preprocess the input examples using a specified tokenizer\n",
    "# The function tokenizes the input text, truncates it to a maximum length of 512 tokens,\n",
    "# and pads the sequences to ensure they are of equal length\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Create a partial function for preprocessing using the base tokenizer\n",
    "preprocess_function_base = partial(preprocess_function, tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a partial function for preprocessing using the large tokenizer\n",
    "preprocess_function_large = partial(preprocess_function, tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5925c9a8644e7296aabf9398da73fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9354aaa1fd47589fee714c9ce37655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84446a6120aa47efacc5eb869d7ec795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4757dc5b7a741e39110d7f3e6c5fd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The preprocess_function_base tokenizes the text, truncates it to 512 tokens, and pads the sequences\n",
    "# The batched=True argument processes the dataset in batches for efficiency\n",
    "dataset_labeled_train_tokenized_base = dataset_labeled_train.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_labeled_valid_tokenized_base = dataset_labeled_valid.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the training dataset using the large tokenizer\n",
    "dataset_labeled_train_tokenized_large = dataset_labeled_train.map(\n",
    "    preprocess_function_large, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the large tokenizer\n",
    "dataset_labeled_valid_tokenized_large = dataset_labeled_valid.map(\n",
    "    preprocess_function_large, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for the base tokenizer\n",
    "# The data collator dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "data_collator_base = DataCollatorWithPadding(tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a data collator for the large tokenizer\n",
    "data_collator_large = DataCollatorWithPadding(tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate module from the Hugging Face library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate module\n",
    "# This metric will be used to evaluate the performance of the model\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "# This function will be used to evaluate the performance of the model during training and validation\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Convert the model's output logits to predicted class labels\n",
    "    # np.argmax(predictions, axis=1) selects the index of the maximum logit for each prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute the accuracy metric using the predicted and true labels\n",
    "    # accuracy.compute() calculates the accuracy of the predictions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels in the training dataset\n",
    "# This will be used to configure the classification model\n",
    "n_labels = df_train.label.nunique()\n",
    "\n",
    "# Load the configuration for the base masked language model (MLM) and modify it for sequence classification\n",
    "# The configuration is loaded from the specified directory and the number of labels is set to n_labels\n",
    "config_base = AutoConfig.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    num_labels=n_labels,\n",
    ")\n",
    "\n",
    "# Load the base masked language model (MLM) and modify it for sequence classification\n",
    "# The model is loaded from the specified directory and the configuration is set to config_base\n",
    "classifier_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    config=config_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2710' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2710/2710 1:03:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>0.608936</td>\n",
       "      <td>0.744599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.584451</td>\n",
       "      <td>0.759899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.558055</td>\n",
       "      <td>0.775967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.565196</td>\n",
       "      <td>0.777043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.580136</td>\n",
       "      <td>0.779657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=0.5146869243291031, metrics={'train_runtime': 3837.1369, 'train_samples_per_second': 67.793, 'train_steps_per_second': 0.706, 'total_flos': 6.844430787637248e+16, 'train_loss': 0.5146869243291031, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the base classifier\n",
    "# These arguments configure various aspects of the training process\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base\n",
    "    / \"base_classifier_legal\",  # Directory to save the model and other outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=48,  # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation (adjust based on GPU memory)\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients before updating\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use 16-bit floating point precision for training (adjust based on GPU support)\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for the base classifier\n",
    "# The Trainer handles the training and evaluation of the model\n",
    "trainer_base = Trainer(\n",
    "    model=classifier_base,  # The model to be trained\n",
    "    args=training_args_base,  # Training arguments\n",
    "    train_dataset=dataset_labeled_train_tokenized_base,  # Training dataset\n",
    "    eval_dataset=dataset_labeled_valid_tokenized_base,  # Evaluation dataset\n",
    "    processing_class=tokenizer_base,  # Tokenizer for preprocessing the input text\n",
    "    data_collator=data_collator_base,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5580551624298096,\n",
       " 'eval_accuracy': 0.7759667871146306,\n",
       " 'eval_runtime': 60.6451,\n",
       " 'eval_samples_per_second': 214.477,\n",
       " 'eval_steps_per_second': 1.682,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Can you guess why the accuracy is so low?`\n",
    "\n",
    "\n",
    "## Understanding Low Accuracy: The Limitation of 512 Tokens\n",
    "\n",
    "When working with transformer models, it's essential to be aware of a key limitation: most models can only process a maximum of **512 tokens**. This restriction has a significant impact on the accuracy of predictions, especially when dealing with longer texts.\n",
    "\n",
    "### The Self-Attention Mechanism and Quadratic Complexity\n",
    "\n",
    "The 512-token limit is a result of the *quadratic complexity* of the **self-attention mechanism**, which is a fundamental component of transformer models. Self-attention allows the model to weigh the importance of each token in relation to others, enabling it to capture context and dependencies within the input text.\n",
    "\n",
    "However, the computational cost of self-attention grows quadratically with the number of tokens. As the input length increases, the memory and computational requirements become prohibitively expensive. To mitigate this issue, most transformer models impose a maximum token limit of 512.\n",
    "\n",
    "### The Impact of Truncation on Accuracy\n",
    "\n",
    "When an input text exceeds 512 tokens, the model automatically truncates it by removing tokens until it fits within the limit. This truncation process can have a detrimental effect on the model's accuracy.\n",
    "\n",
    "Important information, such as key context or relevant details, may be lost during truncation. The model is forced to make predictions based on an incomplete representation of the original text, leading to lower accuracy scores.\n",
    "\n",
    "### Strategies for Handling Longer Texts\n",
    "\n",
    "While the 512-token limit can be challenging, there are several approaches to mitigate its impact:\n",
    "\n",
    "1. **Sliding Window Approach**:\n",
    "    - Divide the long text into smaller, overlapping chunks (windows).\n",
    "    - Process each window individually and aggregate the results.\n",
    "    - This approach can help capture local context, but it may struggle with long-range dependencies.\n",
    "\n",
    "2. **Alternative Neural Network Architectures**:\n",
    "    - Consider using other architectures, such as Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs).\n",
    "    - These architectures can handle longer sequences without the same token limit constraints.\n",
    "    - However, they may not capture long-range dependencies as effectively as transformers.\n",
    "\n",
    "3. **Transformer Variants for Longer Sequences**:\n",
    "    - Explore transformer-based models specifically designed for handling longer texts, such as Longformer and BigBird.\n",
    "    - These models introduce modifications to the self-attention mechanism to reduce computational complexity.\n",
    "    - Keep in mind that these models are relatively new and may have limitations or trade-offs compared to standard transformers.\n",
    "\n",
    "\n",
    "To make informed decisions about handling longer texts, you need to understand the characteristics of your dataset. Analyze the average number of tokens per text and the distribution of text lengths.\n",
    "\n",
    "If a significant portion of your texts exceeds the 512-token limit, consider applying one of the strategies mentioned above. Experiment with different approaches and evaluate their impact on accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52026.000000\n",
       "mean      2373.339407\n",
       "std       1822.717847\n",
       "min        151.000000\n",
       "25%       1133.000000\n",
       "50%       1799.000000\n",
       "75%       3031.000000\n",
       "max      11434.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Initialize an empty list to store the sizes of tokenized input sequences\n",
    "sizes = []\n",
    "\n",
    "# Iterate over each text in the training dataset\n",
    "for txt in df_train.text:\n",
    "    # Tokenize the text without truncation and get the length of the tokenized input sequence\n",
    "    # Append the length of the tokenized input sequence to the sizes list\n",
    "    sizes.append(len(tokenizer_base(txt, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "# Convert the sizes list to a Pandas Series and display descriptive statistics\n",
    "# This provides an overview of the distribution of tokenized input sequence lengths\n",
    "pd.Series(sizes).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`As we can see above, the average number of tokens in our dataset is 2,373. This is significantly higher than the 512 token limit. Therefore, we need to employ a workaround to handle this limitation. We won't cover more complex approaches in this class, but we can use a simple and effective workaround - understanding our data! Let's see how we can do this.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SENTENÇA Tipo A RELATÓRIO Trata-se de ação declaratória de inexistência de débito e indenizatória por danos morais, com pedido de repetição de indébito, ajuizada por Lúcia Matias de Souza em face do Instituto Nacional do Seguro Social – INSS e do Banco Bradesco S/A, em razão da existência de contrato de empréstimo consignado celebrado perante a aludida instituição financeira que, segundo diz a autora, não foi por ela contratado. É o que importa relatar. Passo a decidir. FUNDAMENTAÇÃO Das preliminares arguidas Quanto à preliminar de ilegitimidade passiva alegada pelo INSS (anexo 11), entendo que a Autarquia ré detém legitimidade para figurar no pólo passivo da ação, tendo em vista que é responsável pelo gerenciamento e pagamento dos descontos realizados nos benefícios previdenciários em decorrência de empréstimo consignado. Assim, a partir do momento em que opera o desconto nos valores tem interesse e legitimidade para figurar no pólo passivo da presente demanda. Ademais, só o INSS tem poder para fazer cessar os descontos efetuados, sendo a cessação um dos objetos dessa demanda. No que diz respeito à suposta falta de interesse de agir, é preciso dizer que o caso sob exame não cuida de concessão de beneficio previdenciário, razão pela qual se faz desnecessário prévio requerimento administrativo. Desse modo, afastadas as preliminares suscitadas, passo ao exame do mérito. Mérito A questão posta em Juízo dispensa maiores esclarecimentos, eis que os fatos objetos de controvérsia encontram-se suficientemente provados. Os empréstimos consignados, disponibilizados aos aposentados e pensionistas do INSS, são feitos com descontos na folha de pagamento, conforme autorizado pela Lei 10.820/2003. O valor das parcelas é descontado diretamente do benefício previdenciário. Nas relações de consumo, que são estabelecidas entre instituições financeiras e beneficiários pactuantes dos contratos de empréstimos em consignação (Súmula n.o 297 do STJ), não se pode atribuir ao consumidor, considerado parte hipossuficiente na relação jurídica, a responsabilidade pelos encargos financeiros decorrentes de ocorrência de suspeita de fraude na operacionalização dos empréstimos consignados. Ademais, nessas relações, entre beneficiários e as instituições financeiras, consoante disposto no art. 6o, VIII, da Lei n.o 8.078/90, deve ser facilitada a defesa do consumidor, sobretudo considerando que as instituições financeiras detêm os documentos que comprovam a celebração dos contratos, restando dificultosa qualquer comprovação de fraude pelo beneficiário. Os doutrinadores afirmam, atualmente, que os fatos negativos são possíveis de serem provados e o devem ser pela parte que os alega, diferente do que acontecia outrora, quando vigorava a aplicação da máxima negativa non sunt probanda, ou seja, que os fatos negativos não precisavam ser provados, sendo impossível a produção de sua prova. Hoje, somente os fatos absolutamente negativos é que são insuscetíveis de prova, não pela sua negatividade, mas por sua indefinição. Tem-se como impossível se provar que nunca se esteve em determinado local. Então, nessas hipóteses, tem-se que a parte que alega o fato positivo é que tem o ônus probatório, aplicando-se a teoria da distribuição dinâmica do ônus da prova. Em relação aos fatos relativamente negativos, estes sim podem ser provados. Se, a título de exemplo, alguém afirma que não compareceu ao trabalho em um dia determinado, é possível provar indiretamente a sua ausência ao trabalho, comprovando que foi a um consultório médico. A chamada 'certidão negativa', expedida pelas autoridades fiscais, é um meio de prova de que 'não há débitos fiscais pendentes'. (DIDIER JR., Fredie. Curso de Direito Processual Civil. Vol. 2. Salvador: Editora Jus Podivm, 2007). Todavia, conforme leciona Luiz Guilherme Marinoni, “quando se inverte o ônus, é preciso supor que aquele que vai assumi-lo terá a possibilidade de cumpri-lo, pena de a inversão do ônus da prova significa a imposição de uma perda, e não apenas a transferência de um ônus. Nessa perspectiva, a inversão do ônus da prova somente deve ocorrer quando o réu tem a possibilidade de demonstrar a não existência do fato constitutivo” (Processo de Conhecimento. São Paulo: RT, 2007, p.269/270). No caso em análise, o contrato impugnado é o Contrato no 808431996, no valor de R$565,91 (quinhentos e sessenta e cinco reais e noventa e um centavos).O extrato do NB1061609828 (anexo 07) confirma os descontos no beneficio da parte autora. Pois bem. Tanto o INSS quanto o Banco réu não carrearam os contratos em tela nem qualquer documento que pudesse comprovar a celebração dos negócios jurídicos realizados pela parte autora. Ante a não comprovação da legitimidade dos contratos em tela, entendo que o Banco réu, como beneficiária do presente contrato, deve ser condenada a repetição de indébito. Quanto ao valor da indenização devida, tenho que a reparação pecuniária visa a proporcionar uma espécie de compensação que atenue a ofensa causada, atentando-se, que ao beneficiário não é dado tirar proveito do sinistro, posto que não se destina a indenização ao seu enriquecimento. Portanto, o valor deve ser apenas suficiente ao reparo, sob pena de estar o Judiciário autorizando o enriquecimento sem causa da vítima. O valor da indenização por danos morais deve ser suficiente para, a um só tempo, desestimular reiteração da conduta lesiva pelo réu e abrandar, na medida do possível, o constrangimento e a humilhação causados ao autor lesado. No caso em apreço, entendo que o pagamento de R$ 5.000,00 (cinco mil reais), é suficiente para atender à pretensão formulada. Com efeito, a indenização nos valores indicados demonstra-se adequada, na medida em que, deve-se ter em mente o seu caráter pedagógico, a fim de desestimular os réus a proceder, com desídia, em suas atividades, mormente quando envolvido interesses de pessoas que se encontrem em posição de maior vulnerabilidade. Portanto, tenho que resta configurada a responsabilidade do Banco Bradesco de indenizar, em dobro, o valor indevidamente descontado do benefício da parte promovente, nos termos do art. 42, parágrafo único do CDC, bem como indenizar os danos morais suportados pela parte promovente. Registre-se que, nas demandas referentes a empréstimos consignados celebrados fraudulentamente, a responsabilidade do INSS cinge-se tão somente a cessação dos descontos no benefício previdenciário do prejudicado, sendo a instituição financeira responsável pela reparação dos danos materiais e morais eventualmente suportados. DISPOSITIVO Isso posto, julgo PROCEDENTE o pedido para determinar que o INSS cesse os descontos das parcelas do Contratono 808431996. Condeno, também, a título de danos materiais, o Banco Bradesco a devolver os valores descontados com relação aos citados contratos de empréstimo, em dobro, nos termos do art. 42, parágrafo único, do CDC, devendo tais valores serem acrescidos de juros de mora de 1% ao mês desde o evento danoso (súmula 54 – STJ) e correção monetária com base no IPCA-E desde o efetivo prejuízo (súmula 43 – STJ). Condeno, ainda, o bancoréua pagar, a título de indenização por danos morais, a quantia de R$ 5.000,00 (cinco mil reais), valor este que deve ser atualizado exclusivamente pela taxa SELIC desde a publicação desta sentença. Declaro a inexistência do contrato no808431996. Declaro extinto o processo com resolução do mérito, nos termos do art. 487, I, do Código de Processo Civil. Custas e honorários advocatícios indevidos em primeiro grau de jurisdição (art. 55 da Lei no 9.099/95, c/c art. 1o da Lei no 10.259/01). Registre-se. Intimem-se as partes (Lei no 10.259/01, art. 8o). Campina Grande-PB, data supra. JUIZ FEDERAL\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10, random_state=271828)[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Can you notice that the really relevant information for our classification task is not in the beginning of the text, but in the end?`\n",
    "\n",
    "> (....)\n",
    ">\n",
    "> DISPOSITIVO Isso posto, `julgo PROCEDENTE` o pedido para determinar que o INSS cesse os descontos das parcelas do Contratono 808431996. Condeno, também, a título de danos materiais, o Banco Bradesco a devolver os valores descontados com relação aos citados contratos de empréstimo, em dobro, nos termos do art. 42, parágrafo único, do CDC, devendo tais valores serem acrescidos de juros de mora de 1% ao mês desde o evento danoso (súmula 54 – STJ) e correção monetária com base no IPCA-E desde o efetivo prejuízo (súmula 43 – STJ). Condeno, ainda, o bancoréua pagar, a título de indenização por danos morais, a quantia de R$ 5.000,00 (cinco mil reais), valor este que deve ser atualizado exclusivamente pela taxa SELIC desde a publicação desta sentença. Declaro a inexistência do contrato no808431996. Declaro extinto o processo com resolução do mérito, nos termos do art. 487, I, do Código de Processo Civil. Custas e honorários advocatícios indevidos em primeiro grau de jurisdição (art. 55 da Lei no 9.099/95, c/c art. 1o da Lei no 10.259/01). Registre-se. Intimem-se as partes (Lei no 10.259/01, art. 8o). Campina Grande-PB, data supra. JUIZ FEDERAL\n",
    ">\n",
    "\n",
    "This is very common in this kind of documents. The judge starts with a thorough description of the case and then goes to the decision. So, we can use the last 512 tokens of the text to train our model. We just need to change the truncation_side parameter to 'left' in the tokenizer.\n",
    "\n",
    "Let's see how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_base.truncation_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Eu gosto muito [SEP]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input text using the base tokenizer\n",
    "# The padding=True argument ensures that the sequence is padded to the maximum length\n",
    "# The truncation=True argument ensures that the sequence is truncated to the maximum length if it exceeds it\n",
    "# The max_length=5 argument sets the maximum length of the tokenized sequence to 5 tokens\n",
    "out_len5 = tokenizer_base(\n",
    "    \"Eu gosto muito de farofa com banana\", padding=True, truncation=True, max_length=5\n",
    ")  # This is to simulate the truncation\n",
    "\n",
    "# Decode the tokenized input IDs back to a string\n",
    "# This converts the token IDs back to the corresponding text\n",
    "# The decoded text will be truncated to the first 5 tokens\n",
    "tokenizer_base.decode(out_len5[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the truncation side for the base tokenizer to 'left'\n",
    "# This means that if the input text needs to be truncated, tokens will be removed from the beginning (left side) of the sequence\n",
    "# This setting is useful when the most important information is at the end of the sequence\n",
    "tokenizer_base.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] com banana [SEP]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input text using the base tokenizer\n",
    "# The padding=True argument ensures that the sequence is padded to the maximum length\n",
    "# The truncation=True argument ensures that the sequence is truncated to the maximum length if it exceeds it\n",
    "# The max_length=5 argument sets the maximum length of the tokenized sequence to 5 tokens\n",
    "out_len5 = tokenizer_base(\n",
    "    \"Eu gosto muito de farofa com banana\", padding=True, truncation=True, max_length=5\n",
    ")\n",
    "\n",
    "# Decode the tokenized input IDs back to a string\n",
    "# This converts the token IDs back to the corresponding text\n",
    "# The decoded text will be truncated to the first 5 tokens\n",
    "tokenizer_base.decode(out_len5[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the training DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for training and evaluation\n",
    "dataset_labeled_train = datasets.Dataset.from_pandas(df_train)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for validation and evaluation\n",
    "dataset_labeled_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "# Define a function to preprocess the input examples using a specified tokenizer\n",
    "# The function tokenizes the input text, truncates it to a maximum length of 512 tokens,\n",
    "# and pads the sequences to ensure they are of equal length\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Create a partial function for preprocessing using the base tokenizer\n",
    "# This partial function allows us to call preprocess_function with only the examples argument,\n",
    "# as the tokenizer argument is already set to tokenizer_base\n",
    "preprocess_function_base = partial(preprocess_function, tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a partial function for preprocessing using the large tokenizer\n",
    "preprocess_function_large = partial(preprocess_function, tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb6f4c92f9944a69fef4b63c528b4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59850c67d6cf46c4877ea48a4d9e6d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The preprocess_function_base tokenizes the text, truncates it to 512 tokens, and pads the sequences\n",
    "# The batched=True argument processes the dataset in batches for efficiency\n",
    "dataset_labeled_train_tokenized_base = dataset_labeled_train.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_labeled_valid_tokenized_base = dataset_labeled_valid.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for the base tokenizer\n",
    "# The data collator dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "data_collator_base = DataCollatorWithPadding(tokenizer=tokenizer_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate module from the Hugging Face library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate module\n",
    "# This metric will be used to evaluate the performance of the model\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "# This function will be used to evaluate the performance of the model during training and validation\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Convert the model's output logits to predicted class labels\n",
    "    # np.argmax(predictions, axis=1) selects the index of the maximum logit for each prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute the accuracy metric using the predicted and true labels\n",
    "    # accuracy.compute() calculates the accuracy of the predictions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels in the training dataset\n",
    "# This will be used to configure the classification model\n",
    "n_labels = df_train.label.nunique()\n",
    "\n",
    "# Load the configuration for the base masked language model (MLM) and modify it for sequence classification\n",
    "# The configuration is loaded from the specified directory and the number of labels is set to n_labels\n",
    "config_base = AutoConfig.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    num_labels=n_labels,\n",
    ")\n",
    "\n",
    "# Load the base masked language model (MLM) and modify it for sequence classification\n",
    "classifier_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    config=config_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2710' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2710/2710 1:04:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.125238</td>\n",
       "      <td>0.955178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.121206</td>\n",
       "      <td>0.957408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.121330</td>\n",
       "      <td>0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.961252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.121680</td>\n",
       "      <td>0.961790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=0.11554007523614102, metrics={'train_runtime': 3846.022, 'train_samples_per_second': 67.636, 'train_steps_per_second': 0.705, 'total_flos': 6.844430787637248e+16, 'train_loss': 0.11554007523614102, 'epoch': 5.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the base classifier\n",
    "# These arguments configure various aspects of the training process\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base\n",
    "    / \"base_classifier_legal\",  # Directory to save the model and other outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=48,  # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation (adjust based on GPU memory)\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients before updating\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use 16-bit floating point precision for training (adjust based on GPU support)\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for the base classifier\n",
    "# The Trainer handles the training and evaluation of the model\n",
    "trainer_base = Trainer(\n",
    "    model=classifier_base,  # The model to be trained\n",
    "    args=training_args_base,  # Training arguments\n",
    "    train_dataset=dataset_labeled_train_tokenized_base,  # Training dataset\n",
    "    eval_dataset=dataset_labeled_valid_tokenized_base,  # Evaluation dataset\n",
    "    processing_class=tokenizer_base,  # Tokenizer for preprocessing the input text\n",
    "    data_collator=data_collator_base,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11843354254961014,\n",
       " 'eval_accuracy': 0.9612516337356808,\n",
       " 'eval_runtime': 60.1118,\n",
       " 'eval_samples_per_second': 216.38,\n",
       " 'eval_steps_per_second': 1.697,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achieved a significant improvement in our model's accuracy, which soared from 77.9% to an impressive 96.1%. This upswing is indeed fantastic news!\n",
    "\n",
    "Let's gain a better understanding of this improvement by examining it in terms of the error rate. The error rate is simply calculated as (1 - accuracy). With this formula, our initial error rate was 22.5%, and our improved error rate dropped dramatically to 3.9%.\n",
    "\n",
    "To put this into perspective, we've effectively reduced the error rate by nearly six-fold! In other words, our model is now making far fewer mistakes than before, indicating an exponential enhancement in its overall performance.\n",
    "\n",
    "By using the last 512 tokens in the text data, we were able to direct the focus of our model towards the most relevant information. This approach is a simple yet effective workaround to overcome the 512 token limitation in transformers.\n",
    "\n",
    "This method may seem simple, but it's proven to be an effectively strategic approach to overcome such limitations and handle large amounts of data proficiently. `Remember, sometimes simplicity is the key to master complex challenges!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External resources\n",
    "\n",
    "- [Large Language Models for the curious beginner](https://www.youtube.com/watch?v=LPZh9BOjkQs)\n",
    "- [How I Finally Understood Self-Attention (With PyTorch)](https://www.youtube.com/watch?v=FepOyFtYQ6I)\n",
    "- [Visualizing transformers and attention | Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)\n",
    "- [Transformers (how LLMs work) explained visually](https://www.youtube.com/watch?v=wjZofJX0v4M)\n",
    "- [Attention in transformers, step-by-step](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "- [How might LLMs store facts](https://www.youtube.com/watch?v=9-Jl0dxWQs8&t=354s)\n",
    "- [2024 in Post-Transformer Architectures: State Space Models, RWKV [Latent Space LIVE! @ NeurIPS 2024]](https://www.youtube.com/watch?v=LPe6iC73lrc)\n",
    "- [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference - ModernBERT Paper](https://arxiv.org/abs/2412.13663)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Takeaways\n",
    "- Transformers have become a fundamental tool in NLP, enabling more effective handling of long-range dependencies and parallelization compared to traditional sequential models.\n",
    "\n",
    "- The attention mechanism allows transformers to capture complex relationships and focus on the most relevant information, leading to improved performance on various NLP tasks.\n",
    "\n",
    "- While the quadratic complexity of transformers poses challenges for longer texts, ongoing research aims to develop more efficient variants and attention mechanisms to overcome these limitations.\n",
    "\n",
    "- The successful application of transformers beyond NLP highlights their versatility in capturing patterns and dependencies in structured data across different domains.\n",
    "\n",
    "- Understanding the architecture and components of transformers, such as the encoder-decoder structure, self-attention, and positional encoding, is crucial for effectively employing their capabilities.\n",
    "\n",
    "- Familiarity with common transformer architectures like BERT, GPT, RoBERTa, T5, and XLNet allows practitioners to choose the most suitable model for their specific NLP task.\n",
    "\n",
    "- Transformers can be powerful feature extractors, providing rich representations that capture syntactical and contextual information for downstream tasks.\n",
    "\n",
    "- Following best practices, such as starting with pretrained models, fine-tuning on domain-specific data, and task-specific training, can help achieve optimal results when using transformers.\n",
    "\n",
    "- Awareness of the 512-token limit and developing strategies to handle longer texts, such as focusing on the most relevant information or using sliding window approaches, is essential for maintaining accuracy in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is the key advantage of transformers compared to traditional sequential models like RNNs and LSTMs?\n",
    "\n",
    "2. What is the role of the attention mechanism in transformers?\n",
    "\n",
    "3. What are the main components of a typical transformer architecture?\n",
    "\n",
    "4. What is the impact of quadratic complexity on the performance of transformers for longer texts?\n",
    "\n",
    "5. How have transformers been applied beyond natural language processing (NLP)?\n",
    "\n",
    "6. What are some common transformer architectures used for NLP tasks?\n",
    "\n",
    "7. How can transformers be used as feature extractors?\n",
    "\n",
    "8. What are the key steps for using transformers on a specific task?\n",
    "\n",
    "9. What is the limitation of the 512-token limit in transformers, and how does it impact accuracy?\n",
    "\n",
    "10. What is a simple workaround to handle the 512-token limit and improve accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "<!--\n",
    "1. Transformers can handle long-range dependencies effectively and parallelize computations, avoiding the vanishing gradient problem that plagues RNNs and LSTMs.\n",
    "\n",
    "2. The attention mechanism allows the model to focus on the most relevant parts of the input sequence when predicting a specific output, enabling it to capture complex relationships and dependencies between words.\n",
    "\n",
    "3. A typical transformer consists of an encoder and a decoder, each composed of multiple identical layers. The key components include self-attention layers, feed-forward neural networks, and positional encoding.\n",
    "\n",
    "4. The quadratic complexity of transformers results in slower training times and high memory consumption when dealing with longer sequences, hindering their practicability in scenarios involving extensive texts.\n",
    "\n",
    "5. Transformers have been successfully applied in various domains, including computer vision (e.g., Image Transformer, Vision Transformer), music generation (e.g., MuseNet), speech recognition (e.g., Speech-Transformer), and video processing (e.g.,\n",
    "Video Transformer).\n",
    "\n",
    "6. Common transformer architectures for NLP include BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pretrained Transformer), RoBERTa (Robustly Optimized BERT Approach), T5 (Text-to-Text Transfer Transformer), and\n",
    "XLNet.\n",
    "\n",
    "7. Transformers can be used as feature extractors by utilizing their ability to capture rich syntactical and contextual information from text data. The extracted features, typically represented by the vector corresponding to the [CLS] token, can\n",
    "be used as input for downstream tasks like classification or regression.\n",
    "\n",
    "8. The key steps for using transformers include starting with a pretrained model, optionally fine-tuning the model on domain-specific text, training the model for the specific task using task-specific data, and using the model as a feature\n",
    "extractor if needed.\n",
    "\n",
    "9. Most transformer models can only process a maximum of 512 tokens due to the quadratic complexity of the self-attention mechanism. When an input text exceeds this limit, it is truncated, potentially losing important information and leading to\n",
    "lower accuracy in predictions.\n",
    "\n",
    "10. A simple workaround is to focus on the most relevant information in the text data. For example, in legal documents where the decision is often at the end, using the last 512 tokens of the text can significantly improve accuracy by directing the model's attention to the most important part of the document. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imd1107-nlp-1oxVnwDa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
