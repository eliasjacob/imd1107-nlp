{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "## IMD1107 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](htttps://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Keypoints\n",
    "- Transformers have revolutionized NLP due to their ability to handle long-range dependencies and parallelize computations, overcoming limitations of RNNs and LSTMs.\n",
    "\n",
    "- The attention mechanism is central to transformers, allowing models to focus on the most relevant parts of the input sequence when making predictions.\n",
    "\n",
    "- Transformers consist of an encoder and decoder, each with multiple layers including self-attention, feed-forward networks, and positional encoding.\n",
    "\n",
    "- The quadratic complexity of transformers poses challenges for processing longer texts, impacting training time and memory consumption.\n",
    "\n",
    "- Transformers have been successfully applied beyond NLP in domains like computer vision, music generation, speech recognition, and video processing.\n",
    "\n",
    "- Common transformer architectures for NLP include BERT, GPT, RoBERTa, T5, and XLNet, each with unique strengths.\n",
    "\n",
    "- Transformers can be used as feature extractors, leveraging their ability to capture rich syntactical and contextual information from text data.\n",
    "\n",
    "- Key steps for using transformers involve starting with a pretrained model, optional domain-specific fine-tuning, task-specific training, and potentially using the model as a feature extractor.\n",
    "\n",
    "- The 512-token limit in transformers, due to quadratic complexity, can impact accuracy when important information is lost during truncation of longer texts.\n",
    "\n",
    "- A simple workaround to handle the 512-token limit is to focus on the most relevant information, such as using the last 512 tokens in legal documents where the decision is often at the end.\n",
    "\n",
    "## Takeaways\n",
    "- Transformers have become a fundamental tool in NLP, enabling more effective handling of long-range dependencies and parallelization compared to traditional sequential models.\n",
    "\n",
    "- The attention mechanism allows transformers to capture complex relationships and focus on the most relevant information, leading to improved performance on various NLP tasks.\n",
    "\n",
    "- While the quadratic complexity of transformers poses challenges for longer texts, ongoing research aims to develop more efficient variants and attention mechanisms to overcome these limitations.\n",
    "\n",
    "- The successful application of transformers beyond NLP highlights their versatility in capturing patterns and dependencies in structured data across different domains.\n",
    "\n",
    "- Understanding the architecture and components of transformers, such as the encoder-decoder structure, self-attention, and positional encoding, is crucial for effectively leveraging their capabilities.\n",
    "\n",
    "- Familiarity with common transformer architectures like BERT, GPT, RoBERTa, T5, and XLNet allows practitioners to choose the most suitable model for their specific NLP task.\n",
    "\n",
    "- Transformers can be powerful feature extractors, providing rich representations that capture syntactical and contextual information for downstream tasks.\n",
    "\n",
    "- Following best practices, such as starting with pretrained models, fine-tuning on domain-specific data, and task-specific training, can help achieve optimal results when using transformers.\n",
    "\n",
    "- Awareness of the 512-token limit and developing strategies to handle longer texts, such as focusing on the most relevant information or using sliding window approaches, is essential for maintaining accuracy in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers have revolutionized the field of Natural Language Processing (NLP) since their introduction by Vaswani et al. in 2017. These architectures have become the foundation for tackling a wide array of NLP tasks, including question answering, text summarization, and machine translation. The key to their success lies in their ability to handle long-range dependencies effectively, avoiding the vanishing gradient problem that plagues traditional models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs).\n",
    "\n",
    "## The Shift from Sequential Models\n",
    "\n",
    "Prior to the advent of transformers, sequential model architectures such as RNNs and LSTMs dominated the NLP landscape. These models processed data sequentially, one piece at a time, which limited their ability to capture long-range dependencies and parallelize computations. Transformers introduced a pattern shift by considering all the tokens in a sequence simultaneously, enabling them to capture complex relationships and dependencies between words more effectively.\n",
    "\n",
    "## The Power of Attention Mechanism\n",
    "\n",
    "At the heart of transformers lies the attention mechanism, a process that determines the influence of various inputs on the output. In essence, attention allows the model to focus on the most relevant parts of the input sequence when predicting a specific output. This is particularly useful in tasks like machine translation, where the correct translation of a word often depends on its context and relationships with other words in the sentence.\n",
    "\n",
    "For example, when translating the word \"it\" from English to French, the attention mechanism would assign higher weights to the words that provide gender context, ensuring the correct gender agreement in the translated output. By capturing these dependencies, transformers can generate more accurate and contextually relevant predictions.\n",
    "\n",
    "## A Closer Look\n",
    "\n",
    "A typical transformer model consists of an encoder and a decoder, each composed of multiple identical layers. The encoder processes the input sequence, generating a vector representation for each token, while the decoder takes these vectors as inputs and predicts the output sequence one token at a time.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/transformers_basic.png\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The key components that make up the transformer architecture are:\n",
    "\n",
    "### 1. Self-Attention Layer\n",
    "\n",
    "Self-attention, also known as intra-attention, is a mechanism that relates each word in the input sequence to every other word. It calculates the similarity between words and assigns higher weights to words that are more closely related. This allows the model to capture the contextual relationships between words, regardless of their position in the sequence.\n",
    "\n",
    "The self-attention process involves three main steps:\n",
    "\n",
    "1. **Transformation:** Each input vector (word) is transformed into three different vectors: a Query vector, a Key vector, and a Value vector.\n",
    "2. **Attention Scores:** The model calculates attention scores by measuring the compatibility between the Query and Key vectors. These scores determine the importance of each word in relation to the others.\n",
    "3. **Output Generation:** The attention scores are used to weight the Value vectors, which are then summed up to produce the output for a single time step. This process allows each output to consider the entire input sequence, enabling the model to capture long-range dependencies effectively.\n",
    "\n",
    "By processing each word and its context simultaneously, self-attention makes transformers highly parallelizable, resulting in faster training times compared to RNNs and LSTMs.\n",
    "\n",
    "### 2. Feed Forward Neural Networks (FFNN)\n",
    "\n",
    "Following the self-attention layer, transformers employ a position-wise feed-forward network. This network consists of two linear transformations with a ReLU activation function in between. It operates on each position separately but shares the same parameters across all positions. While this layer does not change the dimensionality of the input, it introduces additional non-linearity and flexibility to the model, allowing it to learn more complex representations.\n",
    "\n",
    "### 3. Positional Encoding\n",
    "\n",
    "Since the self-attention layer does not inherently consider the order of words in a sequence, transformers need a mechanism to incorporate positional information. This is achieved through positional encoding, which injects information about a token's position directly into its vector representation.\n",
    "\n",
    "One common approach is to use sine and cosine functions to generate positional encodings. These functions are applied to different elements of the token vectors, effectively encoding the position as angles in a high-dimensional space. The periodic nature of these functions allows the model to deduce to longer sequences, making it robust to variations in sequence length.\n",
    "\n",
    "> The combination of self-attention, feed-forward networks, and positional encoding enables transformers to capture complex relationships between words, adapt to different contexts, and maintain the order of the sequence. These components work together to create a powerful and flexible architecture that outperforms traditional sequential models in various NLP tasks.\n",
    ">\n",
    "> Transformers have transformed the NLP landscape, offering a more effective and efficient approach to handling long-range dependencies and capturing contextual relationships between words. By leveraging the attention mechanism, feed-forward networks, and positional encoding, transformers have become the go-to architecture for a wide range of NLP tasks, from question answering to machine translation.\n",
    ">\n",
    "> As you continue to explore the world of transformers, keep in mind the key concepts discussed in this overview. Understanding the inner workings of self-attention, the role of feed-forward networks, and the importance of positional encoding will provide you with a solid foundation for working with these powerful architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Complexity in Transformers\n",
    "\n",
    "Transformers have revolutionized the field of natural language processing (NLP) and achieved remarkable success in various tasks. However, despite their numerous advantages, Transformers face a significant limitation when dealing with longer texts due to their computational complexity.\n",
    "\n",
    "### Understanding the Quadratic Complexity\n",
    "\n",
    "The time and space complexity of Transformers is `O(n^2)`, where `n` represents the length of the input sequence. This quadratic complexity arises from the attention mechanism, which is a central component of Transformers.\n",
    "\n",
    "In the self-attention layer, each token in the input sequence needs to calculate its relationship with every other token. As a result, for an input sequence of length `n`, the model performs `n^2` computations for each layer of the Transformer. This means that as the input text grows longer, the number of computations required increases quadratically.\n",
    "\n",
    "The quadratic complexity not only affects the computational time but also the memory usage. The model needs to store the computed relationships between all pairs of tokens, leading to high memory consumption for longer sequences.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/transformer_quadratic.webp\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "The image above illustrates the computational complexity for a sequence of length 9. In this case, the model performs 81 (or `9^2`) computations. Notably, as the sequence length doubles, the number of computations quadruples.\n",
    "\n",
    "### Impact on Real-World Applications\n",
    "\n",
    "The quadratic complexity of Transformers poses challenges when applying them to tasks involving long documents or large-scale language modeling. Some real-world applications affected by this limitation include:\n",
    "\n",
    "1. **Question Answering**: In question answering tasks, the input text is a question, and the model needs to provide the correct answer. The input text can be quite lengthy, requiring the model to consider the entire context.\n",
    "\n",
    "2. **Machine Translation**: Machine translation involves translating a sentence from one language to another. The input sentence can be long, and the model needs to take into account the complete context to generate an accurate translation.\n",
    "\n",
    "3. **Summarization**: Summarization tasks involve generating a concise summary of a long document. The model needs to process the entire input text to capture the key information and produce a coherent summary.\n",
    "\n",
    "4. **Language Modeling**: Language modeling aims to predict the next word or sequence of words given a context. When dealing with long documents, the model needs to consider the entire context to make accurate predictions.\n",
    "\n",
    "5. **Text Classification**: Text classification involves assigning predefined categories or labels to input texts. Long documents pose challenges for Transformers in capturing the relevant information for accurate classification.\n",
    "\n",
    "The quadratic complexity of Transformers results in slower training times and high memory consumption when dealing with longer sequences. This limitation hinders the practicability of Transformers in scenarios involving extensive texts.\n",
    "\n",
    "### Addressing the Quadratic Complexity\n",
    "\n",
    "To alleviate the quadratic complexity problem and make Transformers more efficient for longer sequences, several approaches have been proposed:\n",
    "\n",
    "1. **Sparse Attention Mechanisms**: Instead of attending to all tokens in the input sequence, sparse attention mechanisms focus on a subset of relevant tokens. By selectively attending to fewer tokens, these mechanisms reduce the computational complexity.\n",
    "\n",
    "2. **Long Range Arena (LRA) Benchmark**: The LRA benchmark was introduced to evaluate the efficiency of different sparse attention mechanisms. It provides a standardized framework for comparing and assessing the performance of Transformers on longer sequences.\n",
    "\n",
    "3. **Memory-Efficient Transformers**: Researchers have developed techniques to reduce the memory usage of Transformers. For example, locality-sensitive hashing has been employed to efficiently store and retrieve the computed relationships between tokens.\n",
    "\n",
    "Ongoing research efforts aim to further improve the efficiency and scalability of Transformers, enabling their application to a wider range of text lengths and real-world scenarios.\n",
    "\n",
    "> It's important to note that while the quadratic complexity of Transformers presents challenges, their effectiveness in capturing contextual information and achieving state-of-the-art performance in various NLP tasks cannot be overlooked. The development of more efficient Transformer variants and attention mechanisms is an active area of research, with the goal of overcoming the limitations posed by longer sequences.\n",
    ">\n",
    "> As advancements continue to be made, Transformers are expected to become more capable of handling longer texts efficiently, expanding their applicability and impact in the field of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Transformers Beyond NLP\n",
    "\n",
    "Transformers, originally designed for Natural Language Processing (NLP) tasks, have a unique architecture that allows them to identify complex patterns and dependencies in input data. This powerful capability has led to their application in various domains beyond text processing, yielding promising results. Let's explore some of these areas where Transformers have made significant contributions.\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "In the field of computer vision, Transformers offer a significant advantage in recognizing long-range dependencies between pixels. By treating each pixel as a sequence, similar to how they handle words in a sentence, Transformers can effectively process and analyze images at a granular level.\n",
    "\n",
    "#### Image Transformer\n",
    "\n",
    "The Image Transformer is an advanced version of the original transformer model specifically designed for image processing. It treats each pixel in an image as a sequence token, enabling it to capture fine-grained details and relationships within the image.\n",
    "\n",
    "#### Vision Transformer (ViT)\n",
    "\n",
    "The Vision Transformer (ViT) takes a unique approach to image processing by using a single transformer encoder to process patches of an image. Instead of relying on traditional convolutional layers, ViT treats these image patches as tokens within a sentence, allowing it to learn and extract meaningful features from the visual data.\n",
    "\n",
    "### Music Generation\n",
    "\n",
    "Transformers have also found fascinating applications in the realm of music generation. The self-attention mechanism of Transformers provides a long context, which is particularly beneficial for music composition since musical notes often have dependencies on previous notes.\n",
    "\n",
    "#### MuseNet\n",
    "\n",
    "MuseNet is a notable example of how Transformers can be used for music generation. It utilizes Transformers to generate musical compositions up to four minutes in length, incorporating ten different instruments. Moreover, MuseNet has the ability to combine various musical styles, ranging from country music and classical Mozart to the Beatles, showcasing its versatility and creative potential.\n",
    "\n",
    "### Speech Recognition\n",
    "\n",
    "Transformers have shown remarkable effectiveness in the field of speech recognition. Their self-attention mechanism excels at modeling the temporal mechanics of speech, making them highly suitable for Automatic Speech Recognition (ASR) systems.\n",
    "\n",
    "#### Speech-Transformer\n",
    "\n",
    "The Speech-Transformer simplifies ASR systems by eliminating the need for complex components such as Hidden Markov Models or Connectionist Temporal Classification (CTC). Despite this simplification, the Speech-Transformer maintains its effectiveness in accurately recognizing and transcribing speech.\n",
    "\n",
    "### Video Processing\n",
    "\n",
    "Just as in computer vision, Transformers have found applications in video processing. Each frame of a video can be treated as a token, with sequence information indicating its position within the video.\n",
    "\n",
    "#### Video Transformer\n",
    "\n",
    "The Video Transformer leverages the power of Transformers to extract complex spatial and temporal patterns from video sequences. By analyzing the relationships between frames and their temporal order, the Video Transformer offers an effective way to understand and process video data.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "> Transformers can offer significant benefits anywhere there is structured data with complex dependencies.\n",
    ">\n",
    "> The ability of Transformers to model detailed relationships between inputs makes them highly versatile and applicable across diverse domains. While their origins lie in NLP, researchers continue to explore and expand their potential, pushing the boundaries of what's possible with this powerful architecture.\n",
    ">\n",
    "> As we have seen, Transformers have successfully ventured beyond the realm of text processing, making significant contributions in areas such as computer vision, music generation, speech recognition, and video processing. Their unique ability to capture long-range dependencies and learn from structured data has opened up new avenues for innovation and advancement in these fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Transformer Architectures for NLP\n",
    "\n",
    "Transformers have transformed (sorry, pun intended) Natural Language Processing (NLP) with their ability to effectively capture dependencies in sequence data. This has led to the development of several powerful transformer architectures tailored for various NLP tasks. In this section, we will explore five commonly used transformer architectures and discuss their unique characteristics and strengths.\n",
    "\n",
    "## 1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT, developed by Google, is a pre-trained transformer model that has significantly advanced the field of NLP. One of the key innovations of BERT is its bidirectional approach to language understanding. Unlike traditional models that only consider the words that come before a given word, BERT takes into account the full context by looking at both the preceding and following words. This bidirectional context enables BERT to capture a more nuanced understanding of language semantics, leading to improved performance on a wide range of NLP tasks.\n",
    "\n",
    "BERT's architecture consists of multiple transformer encoder layers stacked on top of each other. During the pre-training phase, BERT is trained on large amounts of unlabeled text data using two novel unsupervised learning tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). MLM involves randomly masking a percentage of input tokens and training the model to predict the original tokens based on the surrounding context. NSP, on the other hand, trains the model to determine whether two given sentences follow each other in the original text. These pre-training tasks allow BERT to learn rich linguistic representations that can be fine-tuned for specific downstream tasks with minimal modifications.\n",
    "\n",
    "## 2. GPT (Generative Pretrained Transformer)\n",
    "\n",
    "GPT, introduced by OpenAI, is another influential transformer architecture in NLP. Unlike BERT, GPT adopts a unidirectional approach, where only the previous words in a sentence are used for prediction. This unidirectional nature makes GPT particularly well-suited for tasks that require generating human-like text, such as language modeling, text completion, and conversational AI.\n",
    "\n",
    "The GPT architecture consists of multiple transformer decoder layers stacked together. During pre-training, GPT is trained on a large corpus of text data using a language modeling objective, where the model learns to predict the next word given the previous words in a sequence. This allows GPT to capture the syntactic structures and patterns present in natural language.\n",
    "\n",
    "One of the strengths of GPT is its ability to generate coherent and fluent text that closely resembles human writing. By conditioning the model on a prompt or a few examples, GPT can generate contextually relevant and grammatically correct continuations. This has led to impressive results in tasks like story generation, dialogue systems, and content creation.\n",
    "\n",
    "## 3. RoBERTa (Robustly Optimized BERT Approach)\n",
    "\n",
    "RoBERTa, developed by Facebook, is a variant of BERT that aims to improve upon the original architecture. While sharing the same fundamental structure as BERT, RoBERTa introduces several key modifications to enhance performance and robustness.\n",
    "\n",
    "One notable change in RoBERTa is the use of dynamic masking during the pre-training phase. Unlike BERT, which uses a fixed set of masked tokens for each training instance, RoBERTa generates a new set of masked tokens for each input sequence. This dynamic masking strategy exposes the model to a more diverse set of masked positions, leading to better generalization.\n",
    "\n",
    "Additionally, RoBERTa employs larger batch sizes and trains on significantly more data compared to BERT. The increased training data and batch sizes help the model learn more robust representations and improve its performance on downstream tasks.\n",
    "\n",
    "RoBERTa has demonstrated state-of-the-art results on various NLP benchmarks, often outperforming the original BERT model. Its success highlights the importance of careful optimization and training strategies in achieving optimal performance with transformer architectures.\n",
    "\n",
    "## 4. T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "T5, introduced by Google, takes a unique approach to NLP by framing every task as a text-to-text problem. Instead of designing task-specific architectures, T5 proposes a unified framework where all tasks are treated as sequence-to-sequence problems, with the input and output being text strings.\n",
    "\n",
    "For example, in a translation task, the input would be the source language text, and the output would be the target language text. Similarly, for a summarization task, the input would be the original text, and the output would be the summary. This consistent problem formulation allows T5 to be applied to a wide range of NLP tasks without requiring task-specific modifications.\n",
    "\n",
    "T5 is pre-trained on a massive corpus of web pages using a denoising objective, where the model learns to reconstruct the original text from corrupted input sequences. This pre-training enables T5 to learn rich linguistic representations that can be fine-tuned for various downstream tasks.\n",
    "\n",
    "One of the advantages of T5's text-to-text approach is its flexibility and simplicity. By treating all tasks as sequence-to-sequence problems, T5 can exploit the same architecture and training procedure across different tasks, reducing the need for task-specific engineering. This has led to impressive performance on a wide range of NLP benchmarks, making T5 a versatile and powerful tool in the NLP toolkit.\n",
    "\n",
    "## 5. XLNet\n",
    "\n",
    "XLNet, jointly developed by Google Brain and Carnegie Mellon University, combines the strengths of both BERT and GPT architectures. It aims to address some of the limitations of BERT while incorporating the auto-regressive nature of GPT.\n",
    "\n",
    "One of the key differences between XLNet and BERT is the way they handle masking during pre-training. While BERT uses a fixed set of masked tokens, XLNet employs a novel permutation-based training objective called \"Permutation Language Modeling\" (PLM). In PLM, the input sequence is randomly permuted, and the model is trained to predict the target token based on the permuted context. This allows XLNet to capture bidirectional context while preserving the auto-regressive property of language modeling.\n",
    "\n",
    "XLNet also introduces the concept of \"two-stream self-attention,\" where the model uses both content-based and query-based attention mechanisms. This enables XLNet to better capture long-range dependencies and model more complex relationships between tokens.\n",
    "\n",
    "Compared to BERT, XLNet has demonstrated improved performance on various NLP tasks, particularly in scenarios where long-range dependencies and auto-regressive modeling are crucial. Its ability to combine the strengths of both BERT and GPT has made XLNet a popular choice for many NLP applications.\n",
    "\n",
    "## Model Sizes: Base vs. Large\n",
    "\n",
    "When working with transformer architectures, you may encounter terms like \"base\" and \"large\" to describe the size of the model. These terms refer to the number of parameters and the depth of the architecture.\n",
    "\n",
    "- **Base Models**: Base models are the standard size for a given transformer architecture. They typically have a moderate number of parameters and are designed to balance performance and computational efficiency. For example, BERT-base consists of 12 transformer encoder layers with 768 hidden units each, resulting in approximately 110 million parameters.\n",
    "\n",
    "- **Large Models**: Large models are expanded versions of the base architecture, with increased depth and/or width. They have a significantly higher number of parameters compared to their base counterparts. For instance, BERT-large has 24 transformer encoder layers with 1024 hidden units each, amounting to around 340 million parameters.\n",
    "\n",
    "The choice between base and large models depends on several factors, such as the complexity of the task, the available computational resources, and the size of the training data. Large models generally achieve better performance due to their increased capacity to capture complex patterns and relationships in the data. However, they also require more computational resources and longer training times.\n",
    "\n",
    "It's important to note that large models are more prone to overfitting, especially when trained on smaller datasets. They have a higher capacity to memorize noise and irrelevant patterns, which can hinder generalization to unseen data. Therefore, when working with limited training data, base models may be a more suitable choice to mitigate overfitting risks.\n",
    "\n",
    "In practice, the decision between base and large models often involves a trade-off between performance and computational efficiency. It's common to start with a base model and scale up to a large model if the task demands higher performance and sufficient resources are available.\n",
    "\n",
    ">\n",
    "> As the field of NLP continues to evolve, we can expect further advancements and refinements in transformer architectures, pushing the boundaries of natural language understanding and generation. By leveraging these powerful tools and techniques, researchers and practitioners can unlock new possibilities in a wide range of NLP applications, from sentiment analysis and machine translation to question answering and content generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformers as Feature Extractors\n",
    "\n",
    "Transformers have revolutionized the field of Natural Language Processing (NLP) by providing a powerful tool for extracting rich syntactical and contextual information from text data. These extracted features can be leveraged for a wide range of downstream tasks, such as classification, regression, and more. In this section, we will explore the concept of features and how transformers can be effectively utilized as feature extractors.\n",
    "\n",
    "### Understanding Features and Feature Extraction\n",
    "\n",
    "Features are the distinctive properties or characteristics that are extracted from a dataset to capture its essential information. In the context of NLP, features can represent various aspects of text data, such as word frequencies, sentence structure, or semantic relationships. These features serve as the foundation for building accurate and insightful models for various tasks.\n",
    "\n",
    "Feature extractors are sophisticated algorithms designed to automatically derive meaningful features from raw data, regardless of its modality (e.g., text, images, or audio). They are capable of identifying and capturing relevant patterns, structures, and relationships within the data, enabling more effective analysis and prediction.\n",
    "\n",
    "### Transformers as Feature Extractors\n",
    "\n",
    "Transformers are a class of neural networks that have proven to be exceptionally well-suited for extracting features from textual data. At their core, transformers operate by taking a sequence of words as input and generating a numeric vector representation that encapsulates the semantic meaning of the entire text.\n",
    "\n",
    "When using transformers as feature extractors, the process involves feeding a word sequence into the transformer model and obtaining a dense numeric vector that captures the essential semantic information of the input text. This vector representation can then be used as input to other machine learning algorithms or neural networks, enabling them to perform various prediction and analysis tasks effectively.\n",
    "\n",
    "### Benefits and Limitations of Transformers as Feature Extractors\n",
    "\n",
    "One of the key advantages of using transformers as feature extractors is their ability to handle textual data effectively, making them particularly useful for tasks such as text classification or regression. Transformers also excel at unsupervised learning, meaning they can extract meaningful features from unlabeled text data, reducing the reliance on labeled datasets.\n",
    "\n",
    "Moreover, transformers have the capacity to capture complex semantic relationships and contextual information within the text, enabling them to generate rich and informative feature representations. This ability to capture elaborate patterns and dependencies makes transformers a powerful tool for various NLP applications.\n",
    "\n",
    "However, it is important to note that transformers also have some limitations. They are computationally intensive and require substantial amounts of data for training. Additionally, transformers have a significant memory footprint due to the need to store the weights of the neural network. This can make them challenging to deploy on resource-constrained devices such as mobile phones or embedded systems with limited memory.\n",
    "\n",
    "<br>\n",
    "\n",
    "> While transformers have their limitations in terms of computational complexity and memory requirements, their benefits in terms of feature extraction and unsupervised learning make them an indispensable tool in the NLP toolkit. As research in this area continues to advance, we can expect to see further improvements and innovations in the use of transformers as feature extractors, pushing the boundaries of what is possible in natural language understanding and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Steps for Using Transformers\n",
    "\n",
    "To use transformers on a specific task, we need to follow these steps:\n",
    "\n",
    "### Step 1: Start with a Pretrained Model\n",
    "\n",
    "The first step is to select a pretrained model from the [Hugging Face Transformers library](https://huggingface.co/transformers/pretrained_models.html). These pretrained models have been trained on large amounts of text data and have learned general language representations. Using a pretrained model provides a strong foundation for your specific task.\n",
    "\n",
    "*Note:* Training a model from scratch is an advanced topic and rarely necessary. In most cases, you can warm-start your model from a pretrained model. If you're interested in learning more about training your own model from scratch, refer to [this resource](https://huggingface.co/blog/how-to-train).\n",
    "\n",
    "### Step 2: Fine-tune the Model on Domain-Specific Text (Optional)\n",
    "\n",
    "Fine-tuning involves adapting a pretrained model to a new domain by training it on domain-specific text. This step is optional but can enhance the model's performance on your specific task. By exposing the model to text that is similar to your target domain, it can learn domain-specific language patterns and representations.\n",
    "\n",
    "### Step 3: Train the Model for Your Task\n",
    "\n",
    "Once you have a fine-tuned model (or a pretrained model if you skipped step 2), you can train it for your specific task. This typically involves adding a classification or regression head on top of the model and training it using your task-specific data.\n",
    "\n",
    "Alternatively, you can use the model as a feature extractor for your task, which is a more advanced approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classifying Court Decision Labels\n",
    "\n",
    "Let's explore these steps using a subset of the [BrCAD-5](https://www.kaggle.com/datasets/eliasjacob/brcad5) dataset, which contains over 765,000 legal case information from Brazilian Federal Courts. Our goal is to train a model to predict the label for a court decision based on its text.\n",
    "\n",
    "1. **Select a Pretrained Model**: We'll choose a suitable pretrained model from the Hugging Face Transformers library that aligns with our task requirements, such as language support and model architecture.\n",
    "\n",
    "2. **Fine-tune the Model (Optional)**: If we have a sufficient amount of domain-specific text (legal case information in this case), we can fine-tune the pretrained model on this data to capture domain-specific language patterns.\n",
    "\n",
    "3. **Train the Model for Label Prediction**: We'll add a classification head on top of the model and train it using the labeled court decision data from BrCAD-5. The model will learn to predict the appropriate label based on the text of the court decision.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Remember, the key is to start with a strong pretrained model and adapt it to your specific task through fine-tuning and task-specific training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at neuralmind/bert-large-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Define the model checkpoints for the base and large versions of the BERT model\n",
    "model_checkpoint_base = \"neuralmind/bert-base-portuguese-cased\"\n",
    "model_checkpoint_large = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# Load the tokenizer for the base BERT model\n",
    "# The tokenizer is responsible for converting text into tokens that the model can understand\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_checkpoint_base)\n",
    "\n",
    "# Load the masked language model (MLM) for the base BERT model\n",
    "# The MLM is used for tasks like predicting masked words in a sentence\n",
    "model_mlm_base = AutoModelForMaskedLM.from_pretrained(model_checkpoint_base)\n",
    "\n",
    "# Load the tokenizer for the large BERT model\n",
    "# This tokenizer works similarly to the base tokenizer but is tailored for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(model_checkpoint_large)\n",
    "\n",
    "# Load the masked language model (MLM) for the large BERT model\n",
    "# This MLM is used for tasks like predicting masked words in a sentence, similar to the base model but with more parameters\n",
    "model_mlm_large = AutoModelForMaskedLM.from_pretrained(model_checkpoint_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_base.is_fast # A fast tokenizer from HF Transformers uses Rust under the hood for faster tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 108,954,466 trainable parameters\n",
      "The model has 334,428,258 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    # Sum the number of elements (numel) for each parameter in the model\n",
    "    # Only include parameters that require gradients (i.e., are trainable)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # Print the number of trainable parameters in a human-readable format with commas\n",
    "    print(f\"The model has {n_parameters:,} trainable parameters\")\n",
    "\n",
    "# Count and print the number of trainable parameters for the base BERT model\n",
    "count_parameters(model_mlm_base)\n",
    "\n",
    "# Count and print the number of trainable parameters for the large BERT model\n",
    "count_parameters(model_mlm_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above, you can see the model architecture summary of a BERT (Bidirectional Encoder Representations from Transformers) model specifically designed for masked language modeling (MLM) tasks. Let's break it down:\n",
    ">\n",
    "> 1. `BertForMaskedLM`: This is the main class that represents the BERT model for masked language modeling.\n",
    ">\n",
    "> 2. `BertModel`: This is the fundamental BERT model that consists of the following components:\n",
    "> - `BertEmbeddings`: This module handles the input embeddings, including word embeddings, position embeddings, and token type embeddings. It also applies layer normalization and dropout.\n",
    "> - `BertEncoder`: This is the main encoder component of BERT, which consists of a stack of `BertLayer` modules.\n",
    "> - `BertLayer`: Each layer in the encoder consists of a self-attention mechanism (`BertAttention`), an intermediate feed-forward network (`BertIntermediate`), and an output projection (`BertOutput`).\n",
    "> - `BertAttention`: This module performs self-attention on the input representations using query, key, and value linear transformations, followed by dropout.\n",
    "> - `BertIntermediate`: This is a feed-forward network with a GELU activation function.\n",
    "> - `BertOutput`: This module applies a dense linear transformation, layer normalization, and dropout to the output of the intermediate layer.\n",
    ">\n",
    "> 3. `BertOnlyMLMHead`: This module is specific to the masked language modeling task and consists of the following components:\n",
    "> - `BertLMPredictionHead`: This module performs the final prediction for the masked tokens.\n",
    "> - `BertPredictionHeadTransform`: This module applies a dense linear transformation, GELU activation, and layer normalization to the output of the BERT encoder.\n",
    "> - `decoder`: This is a linear layer that maps the transformed representations to the vocabulary size for predicting the masked tokens.\n",
    ">\n",
    "> The model architecture summary provides details about the dimensions of the embeddings, the number of layers in the encoder, and the sizes of the intermediate and output layers.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlm_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main differences between the two BERT models are in the model size and architecture:\n",
    ">\n",
    "> 1. Embedding dimensions:\n",
    "> - In the first model, the word embeddings, position embeddings, and token type embeddings have a dimension of 768.\n",
    "> - In the second model, these embeddings have a dimension of 1024, indicating a larger embedding size.\n",
    ">\n",
    "> 2. Number of encoder layers:\n",
    "> - The first model has 12 encoder layers (`(0-11): 12 x BertLayer`).\n",
    "> - The second model has 24 encoder layers (`(0-23): 24 x BertLayer`)\n",
    ">\n",
    "> 3. Intermediate layer dimensions:\n",
    "> - In the first model, the intermediate layer (`BertIntermediate`) has an output dimension of 3072.\n",
    "> - In the second model, the intermediate layer has an output dimension of 4096, which is larger than the first model.\n",
    ">\n",
    "> 4. Hidden state dimensions:\n",
    "> - The first model uses hidden states with a dimension of 768 throughout the architecture, including the self-attention layers, intermediate layers, and output layers.\n",
    "> - The second model uses hidden states with a dimension of 1024 throughout the architecture.\n",
    ">\n",
    "> The rest of the architecture, including the self-attention mechanism, layer normalization, dropout, and the MLM head, remains the same between the two models.\n",
    ">\n",
    "> The large model has higher-dimensional embeddings, more encoder layers, and larger intermediate layer dimensions. This suggests that the large model has a higher capacity and can potentially capture more complex patterns and representations from the input data. However, the larger model size also means increased computational requirements and longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and creating a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58529, 1), (6504, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the unlabeled dataset from a Parquet file\n",
    "# Only the 'text' column is read from the file\n",
    "df_unlabeled = pd.read_parquet('data/legal/unlabeled_texts.parquet', columns=['text'])\n",
    "\n",
    "# Split the unlabeled dataset into training and validation sets\n",
    "# 10% of the data is used for validation, and the split is reproducible with a fixed random state\n",
    "df_unlabeled_train, df_unlabeled_valid = train_test_split(df_unlabeled, test_size=0.10, random_state=271828)\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "# This shows the number of rows and columns in each set\n",
    "df_unlabeled_train.shape, df_unlabeled_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the pandas DataFrame containing the unlabeled training data into a Hugging Face Dataset\n",
    "# This allows for easier manipulation and integration with Hugging Face's tools and models\n",
    "dataset_unlabeled_train = datasets.Dataset.from_pandas(df_unlabeled_train)\n",
    "\n",
    "# Convert the pandas DataFrame containing the unlabeled validation data into a Hugging Face Dataset\n",
    "# This allows for easier manipulation and integration with Hugging Face's tools and models\n",
    "dataset_unlabeled_valid = datasets.Dataset.from_pandas(df_unlabeled_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 58529\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 6504\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the path to save the outputs of the base BERT masked language model\n",
    "path_to_save_lm_base = Path('./outputs/transformers_basics/bert_masked_lm_base')\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the path to save the outputs of the large BERT masked language model\n",
    "path_to_save_lm_large = Path('./outputs/transformers_basics/bert_masked_lm_large')\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm_large.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune the Language Model on the domain text\n",
    "\n",
    "Remember our transfer learning class. During this stage, the general-domain language model adapts itself to the idiosyncrasies of the domain-specific text. This is done by training the model on the domain-specific text. This step is optional, but it can improve the performance of the model on your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80910caaa004779bfa01b4209e0c244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1d7f95157f4c8ba661e61cf4cb3044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82bf619d7784c4c99b7e50517d89fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29848078179248289fb514496ae57b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text in the given examples using the tokenizer object.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the tokenized input text.\n",
    "    \"\"\"\n",
    "    result = tokenizer(examples[\"text\"])  # Tokenize the input text\n",
    "    if tokenizer.is_fast:\n",
    "        # If the tokenizer is a fast tokenizer, add word IDs to the result\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# Create partial functions for tokenizing using the base and large tokenizers\n",
    "# This allows us to pass the tokenizer as a fixed argument to the tokenize_function\n",
    "tokenize_function_base = partial(tokenize_function, tokenizer=tokenizer_base)\n",
    "tokenize_function_large = partial(tokenize_function, tokenizer=tokenizer_large)\n",
    "\n",
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The map function applies the tokenize_function_base to each example in the dataset\n",
    "# The batched=True argument processes the examples in batches for efficiency\n",
    "# The remove_columns argument removes the specified columns from the dataset after tokenization\n",
    "dataset_train_tokenized_mlm_base = dataset_unlabeled_train.map(\n",
    "    tokenize_function_base, batched=True, remove_columns=[\"text\", '__index_level_0__']\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_valid_tokenized_mlm_base = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_base, batched=True, remove_columns=[\"text\", '__index_level_0__']\n",
    ")\n",
    "\n",
    "# Tokenize the training dataset using the large tokenizer\n",
    "dataset_train_tokenized_mlm_large = dataset_unlabeled_train.map(\n",
    "    tokenize_function_large, batched=True, remove_columns=[\"text\", '__index_level_0__']\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the large tokenizer\n",
    "dataset_valid_tokenized_mlm_large = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_large, batched=True, remove_columns=[\"text\", '__index_level_0__']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1cd084c40e4665897fcb4801c5dedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcc77666aa642bda0b7f8117944b28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b066d33fa4836a3046c6ff0e13e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3f6d75108e4167822dc7d2761baffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    This function groups together a set of texts as contiguous text of fixed length (chunk_size). \n",
    "    It's useful for training masked language models.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the examples to group. Each key corresponds to a feature, \n",
    "                and each value is a list of lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the grouped examples. Each key corresponds to a feature, \n",
    "      and each value is a list of lists of tokens.\n",
    "    \"\"\"\n",
    "    # Concatenate all texts for each feature\n",
    "    concatenated_examples = {k: np.concatenate(examples[k]) for k in examples.keys()}\n",
    "    \n",
    "    # Compute the total length of the concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Adjust the total length to be a multiple of chunk_size, dropping the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    \n",
    "    # Split the concatenated texts into chunks of size chunk_size using NumPy\n",
    "    result = {\n",
    "        k: np.split(t[:total_length], total_length // chunk_size)\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Create a new 'labels' column that is a copy of the 'input_ids' column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define the chunk size for grouping texts\n",
    "chunk_size = 512\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset for the base BERT model\n",
    "dataset_train_tokenized_mlm_base = dataset_train_tokenized_mlm_base.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset for the base BERT model\n",
    "dataset_valid_tokenized_mlm_base = dataset_valid_tokenized_mlm_base.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset for the large BERT model\n",
    "dataset_train_tokenized_mlm_large = dataset_train_tokenized_mlm_large.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset for the large BERT model\n",
    "dataset_valid_tokenized_mlm_large = dataset_valid_tokenized_mlm_large.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM) using the base BERT tokenizer\n",
    "# The data collator will dynamically mask tokens in the input text with a probability of 0.15\n",
    "data_collator_mlm_base = DataCollatorForLanguageModeling(tokenizer=tokenizer_base, mlm_probability=0.15)\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM) using the large BERT tokenizer\n",
    "# The data collator will dynamically mask tokens in the input text with a probability of 0.15\n",
    "data_collator_mlm_large = DataCollatorForLanguageModeling(tokenizer=tokenizer_large, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation using the base BERT model\n",
    "batch_size_base = 20\n",
    "\n",
    "# Extract the model name from the model checkpoint path for the base BERT model\n",
    "model_name_base = model_checkpoint_base.split(\"/\")[-1]\n",
    "\n",
    "# Set up the training arguments for fine-tuning the base BERT model on a masked language modeling task\n",
    "training_args_mlm_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\",  # Directory to save the model checkpoints\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size_base,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size_base,  # Batch size for evaluation\n",
    "    bf16=True,  # Use bfloat16 precision (change to \"fp16\" if using a free GPU)\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_steps=1, # Log the training loss after every 1 epoch\n",
    "    eval_steps=1, # Evaluate the model after every 1 epoch\n",
    "    save_steps=1, # Save the model after every 1 epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to use for selecting the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=3,  # Number of gradient accumulation steps\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the base BERT model\n",
    "# The Trainer class provides an easy-to-use API for training and evaluating models\n",
    "trainer_mlm_base = Trainer(\n",
    "    model=model_mlm_base,  # The model to be trained (base BERT masked language model)\n",
    "    args=training_args_mlm_base,  # Training arguments defined earlier\n",
    "    train_dataset=dataset_train_tokenized_mlm_base,  # Tokenized training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_mlm_base,  # Tokenized validation dataset\n",
    "    data_collator=data_collator_mlm_base,  # Data collator for dynamically masking tokens\n",
    "    tokenizer=tokenizer_base,  # Tokenizer for processing the input text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meliasjacob\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nas-elias/drive/UFRN/Disciplinas/2024-1/Dell Deep Learning and Gen-AI/wandb/run-20240726_063716-qwihhcur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eliasjacob/huggingface/runs/qwihhcur' target=\"_blank\">outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm</a></strong> to <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eliasjacob/huggingface/runs/qwihhcur' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface/runs/qwihhcur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6786' max='6786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6786/6786 4:50:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.579700</td>\n",
       "      <td>0.473443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.408905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6786, training_loss=0.5313287478850508, metrics={'train_runtime': 17450.1629, 'train_samples_per_second': 46.679, 'train_steps_per_second': 0.389, 'total_flos': 2.143305955958661e+17, 'train_loss': 0.5313287478850508, 'epoch': 2.9991160872127285})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This took around 5 hours to train on 2 x NVIDIA RTX 3090 GPUs\n",
    "trainer_mlm_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='746' max='746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [746/746 04:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.38791826367378235,\n",
       " 'eval_runtime': 293.0877,\n",
       " 'eval_samples_per_second': 101.755,\n",
       " 'eval_steps_per_second': 2.545,\n",
       " 'epoch': 2.9991160872127285}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer_mlm_base.save_model(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")\n",
    "tokenizer_base.save_pretrained(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")\n",
    "\n",
    "trainer_mlm_base.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the trainer, model, and tokenizer for the base BERT model to None\n",
    "# This helps free up memory by removing references to these objects\n",
    "trainer_mlm_base = None\n",
    "model_mlm_base = None\n",
    "tokenizer_base = None\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Clear the CUDA memory cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation\n",
    "batch_size_large = 14\n",
    "\n",
    "# Extract the model name from the model checkpoint path\n",
    "# This will be used to name the output directory for the trained model\n",
    "model_name_large = model_checkpoint_large.split(\"/\")[-1]\n",
    "\n",
    "# Define the training arguments for the large masked language model (MLM)\n",
    "training_args_mlm_large = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\",  # Output directory for the trained model\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it already exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size_large,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size_large,  # Batch size for evaluation\n",
    "    bf16=True,  # Use bf16 precision. Change to \"fp16\" if using a free GPU\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints and delete the older ones\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_steps=1,  # Log the training loss after every 1 step\n",
    "    eval_steps=1,  # Evaluate the model after every 1 step\n",
    "    save_steps=1,  # Save the model after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use the evaluation loss to determine the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating the model parameters\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the large masked language model (MLM)\n",
    "trainer_mlm_large = Trainer(\n",
    "    model=model_mlm_large,  # The pre-trained large BERT model for masked language modeling\n",
    "    args=training_args_mlm_large,  # The training arguments defined earlier for the large model\n",
    "    train_dataset=dataset_train_tokenized_mlm_large,  # The tokenized training dataset for the large model\n",
    "    eval_dataset=dataset_valid_tokenized_mlm_large,  # The tokenized validation dataset for the large model\n",
    "    data_collator=data_collator_mlm_large,  # The data collator for dynamic masking during training\n",
    "    tokenizer=tokenizer_large,  # The tokenizer used to process the input text for the large model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meliasjacob\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nas-elias/drive/UFRN/Disciplinas/2024-1/Dell Deep Learning and Gen-AI/wandb/run-20240727_062917-mpbgvodr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eliasjacob/huggingface/runs/mpbgvodr' target=\"_blank\">outputs/transformers_basics/bert_masked_lm_large/bert-large-portuguese-cased-finetuned-mlm</a></strong> to <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eliasjacob/huggingface/runs/mpbgvodr' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface/runs/mpbgvodr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7272' max='7272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7272/7272 13:25:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.381371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>0.309468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7272, training_loss=0.4201359052510217, metrics={'train_runtime': 48363.8901, 'train_samples_per_second': 16.842, 'train_steps_per_second': 0.15, 'total_flos': 7.590524853366497e+17, 'train_loss': 0.4201359052510217, 'epoch': 2.999381315735203})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the large masked language model (MLM)\n",
    "# This process involves multiple epochs of training on the training dataset\n",
    "# Note: This training process took almost 14 hours on 2 x NVIDIA RTX 3090 GPUs\n",
    "trainer_mlm_large.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1066' max='1066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1066/1066 11:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30810099840164185,\n",
       " 'eval_runtime': 709.2879,\n",
       " 'eval_samples_per_second': 42.046,\n",
       " 'eval_steps_per_second': 1.503,\n",
       " 'epoch': 2.999381315735203}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained large masked language model (MLM) to the specified directory\n",
    "trainer_mlm_large.save_model(path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\")\n",
    "\n",
    "# Save the tokenizer used for the large MLM to the same directory\n",
    "tokenizer_large.save_pretrained(path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\")\n",
    "\n",
    "# Evaluate the trained large MLM on the validation dataset\n",
    "# This will return a dictionary containing the evaluation metrics\n",
    "trainer_mlm_large.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/transformers_basics/bert_masked_lm_large/bert-large-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing a Language Model\n",
    "\n",
    "To ensure that a language model is effective and reliable, we need to assess its performance. This is usually done by evaluating how well the model can predict a word in a sentence. The primary metric used for this purpose is known as 'Perplexity'.\n",
    "\n",
    "### Understanding Perplexity\n",
    "\n",
    "Perplexity is a quantitative measure of how well a probability model predicts a sample. In the context of language models, it gauges how surprised or 'thrown-off' the model is upon encountering new data. Essentially, it is a measure of \"surprise\".\n",
    "\n",
    "A lower perplexity indicates that the model was less surprised by the new data, signifying that it was better trained and has a good understanding of the language patterns in the provided data. Therefore, a lower perplexity value is indicative of better training.\n",
    "\n",
    "### Calculating Perplexity\n",
    "\n",
    "Perplexity is defined as the exponentiation of the entropy. Entropy is a measure of the uncertainty associated with a random variable. Since the loss function of the language model is the cross-entropy loss, we can use the loss value to calculate the perplexity. The formula for perplexity is:\n",
    "\n",
    "$$Perplexity = e^{loss}$$\n",
    "\n",
    "Where:\n",
    "- $e$ is the base of the natural logarithm (Euler's number, approximately 2.71828)\n",
    "- $loss$ is the cross-entropy loss\n",
    "\n",
    "### Choice of Logarithm Base\n",
    "\n",
    "The choice of base for the logarithm in calculating perplexity or entropy often depends on the context or the historical convention of the field.\n",
    "\n",
    "- In information theory, the base of the logarithm is typically 2, resulting in units of bits (binary digits). This is because information was originally conceptualized in the context of binary decisions (yes/no, true/false, 0/1), and thus, using a base-2 logarithm is intuitive: a message space of $2^n$ messages each carry $n$ bits of information.\n",
    "\n",
    "- The rationale behind using $e$ as the base is somewhat unclear. In numerous domains of machine learning, $e$ possesses unique attributes, however, these properties do not hold relevance here. Euler's number ($e$) exhibits several intriguing properties, especially in machine learning, where a majority of the basic mathematical principles and techniques (like calculus and optimization methods) often function more efficiently or are simpler with natural logarithms.\n",
    "\n",
    "> It's important to note that the base of the logarithm doesn't change the fundamental interpretation of entropy or perplexity - it's merely a scaling factor. However, base-2 logarithms will give you a measure in bits, while natural logarithms will give you a measure in nats (natural units of information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the trainer for the large masked language model (MLM) to None to free up memory\n",
    "trainer_mlm_large = None\n",
    "\n",
    "# Set the large masked language model (MLM) to None to free up memory\n",
    "model_mlm_large = None\n",
    "\n",
    "# Set the tokenizer for the large MLM to None to free up memory\n",
    "tokenizer_large = None\n",
    "\n",
    "# Collect garbage to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the base model is 1.4739093074325733\n",
      "The perplexity for the large model is 1.3608384245024145\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(f'The perplexity for the base model is {math.exp(0.38791826367378235)}')\n",
    "print(f'The perplexity for the large model is {math.exp(0.30810099840164185)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Define the path to save the base masked language model (MLM)\n",
    "# This path points to the directory where the base MLM model will be saved\n",
    "path_to_save_lm_base = Path('./outputs/transformers_basics/bert_masked_lm_base')\n",
    "\n",
    "# Define the path to save the large masked language model (MLM)\n",
    "# This path points to the directory where the large MLM model will be saved\n",
    "path_to_save_lm_large = Path('./outputs/transformers_basics/bert_masked_lm_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned base masked language model (MLM) from the specified directory\n",
    "# This model is a BERT base model fine-tuned on a Portuguese dataset\n",
    "model_base = AutoModelForMaskedLM.from_pretrained(path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base MLM from the same directory\n",
    "# The tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Load the fine-tuned large masked language model (MLM) from the specified directory\n",
    "# This model is a BERT large model fine-tuned on a Portuguese dataset\n",
    "model_large = AutoModelForMaskedLM.from_pretrained(path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large MLM from the same directory\n",
    "# The tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for the base masked language model (MLM)\n",
    "# The pipeline is used to fill in the masked tokens in the input text\n",
    "# 'fill-mask' specifies the task type for the pipeline\n",
    "# model_base is the fine-tuned base MLM model\n",
    "# tokenizer_base is the tokenizer for the base MLM model\n",
    "# top_k=5 specifies that the top 5 predictions for the masked token will be returned\n",
    "pipe_base = pipeline(\n",
    "    'fill-mask',\n",
    "    model=model_base,\n",
    "    tokenizer=tokenizer_base,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Create a pipeline for the large masked language model (MLM)\n",
    "pipe_large = pipeline(\n",
    "    'fill-mask',\n",
    "    model=model_large,\n",
    "    tokenizer=tokenizer_large,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9281540513038635,\n",
       "  'token': 131,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de :'},\n",
       " {'score': 0.012005731463432312,\n",
       "  'token': 21982,\n",
       "  'token_str': 'homicdio',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de homicdio'},\n",
       " {'score': 0.0050378949381411076,\n",
       "  'token': 18144,\n",
       "  'token_str': 'roubo',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de roubo'},\n",
       " {'score': 0.0032502533867955208,\n",
       "  'token': 1112,\n",
       "  'token_str': '',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de '},\n",
       " {'score': 0.0027919497806578875,\n",
       "  'token': 184,\n",
       "  'token_str': 're',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de re'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base(\"O artigo 121 do Cdigo Penal prev o crime de [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9837086796760559,\n",
       "  'token': 131,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de :'},\n",
       " {'score': 0.006448815111070871,\n",
       "  'token': 21982,\n",
       "  'token_str': 'homicdio',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de homicdio'},\n",
       " {'score': 0.0011397271882742643,\n",
       "  'token': 119,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de.'},\n",
       " {'score': 0.0007863400387577713,\n",
       "  'token': 1386,\n",
       "  'token_str': 'morte',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de morte'},\n",
       " {'score': 0.0007723842863924801,\n",
       "  'token': 9566,\n",
       "  'token_str': 'corrupo',\n",
       "  'sequence': 'O artigo 121 do Cdigo Penal prev o crime de corrupo'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_large(\"O artigo 121 do Cdigo Penal prev o crime de [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3354406952857971,\n",
       "  'token': 17225,\n",
       "  'token_str': 'julgado',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em julgado para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.2519117295742035,\n",
       "  'token': 5370,\n",
       "  'token_str': 'aberto',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em aberto para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.2214008867740631,\n",
       "  'token': 21244,\n",
       "  'token_str': 'dobro',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em dobro para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.06615797430276871,\n",
       "  'token': 3418,\n",
       "  'token_str': 'curso',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em curso para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.03829769790172577,\n",
       "  'token': 4712,\n",
       "  'token_str': 'branco',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em branco para interposio de recurso pela Fazenda Pblica'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base(\"O Cdigo de Processo Civil prev prazo em [MASK] para interposio de recurso pela Fazenda Pblica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5983186960220337,\n",
       "  'token': 2241,\n",
       "  'token_str': 'lei',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em lei para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.31673961877822876,\n",
       "  'token': 21244,\n",
       "  'token_str': 'dobro',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em dobro para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.015298635698854923,\n",
       "  'token': 2502,\n",
       "  'token_str': 'Lei',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em Lei para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.01301574520766735,\n",
       "  'token': 20554,\n",
       "  'token_str': 'razovel',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em razovel para interposio de recurso pela Fazenda Pblica'},\n",
       " {'score': 0.009656175971031189,\n",
       "  'token': 5370,\n",
       "  'token_str': 'aberto',\n",
       "  'sequence': 'O Cdigo de Processo Civil prev prazo em aberto para interposio de recurso pela Fazenda Pblica'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_large(\"O Cdigo de Processo Civil prev prazo em [MASK] para interposio de recurso pela Fazenda Pblica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our Document Classifier Using Our Fine-Tuned Language Model\n",
    "\n",
    "### Understanding the Language Model Output Structure\n",
    "\n",
    "Before diving into the details of document classification, it's essential to grasp the structure of the output from the language model. The output is a vector with dimensions of `max_tokens` x `embedding_dimension`. Taking BERT-base as an example, the embedding dimension is 768. This means that for each token in the input text, there is a corresponding vector of size 768.\n",
    "\n",
    "In practical scenarios, utilizing the entire array of vectors as input for our classifier may not be feasible due to the vast amount of information involved. Instead, we focus on leveraging the vector corresponding to the `[CLS]` token.\n",
    "\n",
    "### The Significance of the `[CLS]` Token\n",
    "\n",
    "The `[CLS]` token is a special token that precedes the input text and represents the entirety of the input in the context of BERT models. This token's vector size is 768, which is significantly more manageable compared to the entire vector array. `[CLS]` stands for `CL`a`S`sification and is specifically designed for classification tasks.\n",
    "\n",
    "Here's an example to illustrate the usage of the `[CLS]` token:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "outputs = tokenizer('Eu gosto muito de farofa')\n",
    "tokenizer.decode(outputs['input_ids'])\n",
    "```\n",
    "\n",
    "Resulting output: `'[CLS] Eu gosto muito de farofa [SEP]'`\n",
    "\n",
    "In the above output, you'll notice that the `[CLS]` token is added to the start of the input text, while the `[SEP]` token is appended to the end. However, for classification purposes, we only need to focus on the `[CLS]` token and can ignore the `[SEP]` token. The role of the `[SEP]` token in BERT is to enable the separation of two sentences, but since our input text contains only one sentence, its usage is unnecessary here.\n",
    "\n",
    "### Implementing Classification Using the `[CLS]` Token\n",
    "\n",
    "Now that we know how to extract the vector for the `[CLS]` token, we can use it as input for our classifier. The classifier's output will be a vector of size `num_labels`, where `num_labels` refers to the number of labels present in our dataset. For example, if we have 4 labels, the classifier would output a vector of size 4.\n",
    "\n",
    "This output vector will be crucial in calculating the model's loss and updating its weights during the training process. By comparing the predicted label probabilities with the actual labels, we can measure the model's performance and make necessary adjustments to improve its accuracy.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "To summarize, the process of document classification using a fine-tuned language model involves the following steps:\n",
    "\n",
    "1. Tokenize the input text and add the `[CLS]` token at the beginning.\n",
    "2. Pass the tokenized input through the language model to obtain the output vector.\n",
    "3. Extract the vector corresponding to the `[CLS]` token.\n",
    "4. Use the `[CLS]` token vector as input for the classifier.\n",
    "5. Obtain the classifier's output vector, which represents the predicted label probabilities.\n",
    "6. Calculate the loss by comparing the predicted labels with the actual labels.\n",
    "7. Update the model's weights based on the calculated loss to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52026, 2), (13007, 2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the training dataset from a Parquet file\n",
    "# Only the 'text' and 'label' columns are read from the file\n",
    "df_train = pd.read_parquet('data/legal/train.parquet', columns=['text', 'label'])\n",
    "\n",
    "# Load the validation dataset from a Parquet file\n",
    "# Only the 'text' and 'label' columns are read from the file\n",
    "df_valid = pd.read_parquet('data/legal/valid.parquet', columns=['text', 'label'])\n",
    "\n",
    "# Define the path to save the base masked language model (MLM)\n",
    "# This path points to the directory where the base MLM model will be saved\n",
    "path_to_save_lm_base = Path('./outputs/transformers_basics/bert_masked_lm_base')\n",
    "\n",
    "# Define the path to save the large masked language model (MLM)\n",
    "# This path points to the directory where the large MLM model will be saved\n",
    "path_to_save_lm_large = Path('./outputs/transformers_basics/bert_masked_lm_large')\n",
    "\n",
    "# Display the shapes of the training and validation datasets\n",
    "# This shows the number of rows and columns in each dataset\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'IMPROCEDENTE': 0,\n",
       "  'PROCEDENTE': 1,\n",
       "  'PARCIALMENTE PROCEDENTE': 2,\n",
       "  'EXTINTO SEM MRITO': 3},\n",
       " {0: 'IMPROCEDENTE',\n",
       "  1: 'PROCEDENTE',\n",
       "  2: 'PARCIALMENTE PROCEDENTE',\n",
       "  3: 'EXTINTO SEM MRITO'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map each unique label in the training dataset to a unique ID\n",
    "# df_train.label.unique() returns an array of unique labels in the training dataset\n",
    "# The dictionary comprehension iterates over the unique labels and assigns an ID to each label\n",
    "label2id = {df_train.label.unique()[i]: i for i in range(len(df_train.label.unique()))}\n",
    "\n",
    "# Create a dictionary to map each unique ID back to its corresponding label\n",
    "# This is the reverse mapping of the label2id dictionary\n",
    "# The dictionary comprehension iterates over the items in label2id and swaps the keys and values\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Display the label-to-ID and ID-to-label mappings\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>SENTENA Vistos etc. Dispensado o relatrio, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17972</th>\n",
       "      <td>SENTENA Relatrio dispensado. No caso, no h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34527</th>\n",
       "      <td>SENTENA Vistos etc. Trata-se de pedido de res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58381</th>\n",
       "      <td>TERMO DE AUDINCIA DE INSTRUO Ao Especial ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56474</th>\n",
       "      <td>SENTENA Trata-se de ao em que a parte autor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1387   SENTENA Vistos etc. Dispensado o relatrio, a...      0\n",
       "17972  SENTENA Relatrio dispensado. No caso, no h...      0\n",
       "34527  SENTENA Vistos etc. Trata-se de pedido de res...      1\n",
       "58381  TERMO DE AUDINCIA DE INSTRUO Ao Especial ...      1\n",
       "56474  SENTENA Trata-se de ao em que a parte autor...      2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the labels in the training dataset to their corresponding IDs\n",
    "# This replaces the label names with their respective IDs using the label2id dictionary\n",
    "df_train['label'] = df_train['label'].map(label2id)\n",
    "\n",
    "# Map the labels in the validation dataset to their corresponding IDs\n",
    "# This replaces the label names with their respective IDs using the label2id dictionary\n",
    "df_valid['label'] = df_valid['label'].map(label2id)\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "# This shows the updated training dataset with labels replaced by their corresponding IDs\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the training DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for training and evaluation\n",
    "dataset_labeled_train = datasets.Dataset.from_pandas(df_train)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for validation and evaluation\n",
    "dataset_labeled_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Define a function to preprocess the input examples using a specified tokenizer\n",
    "# The function tokenizes the input text, truncates it to a maximum length of 512 tokens,\n",
    "# and pads the sequences to ensure they are of equal length\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a partial function for preprocessing using the base tokenizer\n",
    "preprocess_function_base = partial(preprocess_function, tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a partial function for preprocessing using the large tokenizer\n",
    "preprocess_function_large = partial(preprocess_function, tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5925c9a8644e7296aabf9398da73fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9354aaa1fd47589fee714c9ce37655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84446a6120aa47efacc5eb869d7ec795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4757dc5b7a741e39110d7f3e6c5fd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The preprocess_function_base tokenizes the text, truncates it to 512 tokens, and pads the sequences\n",
    "# The batched=True argument processes the dataset in batches for efficiency\n",
    "dataset_labeled_train_tokenized_base = dataset_labeled_train.map(preprocess_function_base, batched=True)\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_labeled_valid_tokenized_base = dataset_labeled_valid.map(preprocess_function_base, batched=True)\n",
    "\n",
    "# Tokenize the training dataset using the large tokenizer\n",
    "dataset_labeled_train_tokenized_large = dataset_labeled_train.map(preprocess_function_large, batched=True)\n",
    "\n",
    "# Tokenize the validation dataset using the large tokenizer\n",
    "dataset_labeled_valid_tokenized_large = dataset_labeled_valid.map(preprocess_function_large, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for the base tokenizer\n",
    "# The data collator dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "data_collator_base = DataCollatorWithPadding(tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a data collator for the large tokenizer\n",
    "data_collator_large = DataCollatorWithPadding(tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate module from the Hugging Face library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate module\n",
    "# This metric will be used to evaluate the performance of the model\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "# This function will be used to evaluate the performance of the model during training and validation\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert the model's output logits to predicted class labels\n",
    "    # np.argmax(predictions, axis=1) selects the index of the maximum logit for each prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute the accuracy metric using the predicted and true labels\n",
    "    # accuracy.compute() calculates the accuracy of the predictions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels in the training dataset\n",
    "# This will be used to configure the classification model\n",
    "n_labels = df_train.label.nunique()\n",
    "\n",
    "# Load the configuration for the base masked language model (MLM) and modify it for sequence classification\n",
    "# The configuration is loaded from the specified directory and the number of labels is set to n_labels\n",
    "config_base = AutoConfig.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\", \n",
    "    num_labels=n_labels\n",
    ")\n",
    "\n",
    "# Load the base masked language model (MLM) and modify it for sequence classification\n",
    "# The model is loaded from the specified directory and the configuration is set to config_base\n",
    "classifier_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\", \n",
    "    config=config_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2710' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2710/2710 1:03:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>0.608936</td>\n",
       "      <td>0.744599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.584451</td>\n",
       "      <td>0.759899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.558055</td>\n",
       "      <td>0.775967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.565196</td>\n",
       "      <td>0.777043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.580136</td>\n",
       "      <td>0.779657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=0.5146869243291031, metrics={'train_runtime': 3837.1369, 'train_samples_per_second': 67.793, 'train_steps_per_second': 0.706, 'total_flos': 6.844430787637248e+16, 'train_loss': 0.5146869243291031, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the base classifier\n",
    "# These arguments configure various aspects of the training process\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base/\"base_classifier_legal\",  # Directory to save the model and other outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=48,  # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation (adjust based on GPU memory)\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients before updating\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use 16-bit floating point precision for training (adjust based on GPU support)\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for the base classifier\n",
    "# The Trainer handles the training and evaluation of the model\n",
    "trainer_base = Trainer(\n",
    "    model=classifier_base,  # The model to be trained\n",
    "    args=training_args_base,  # Training arguments\n",
    "    train_dataset=dataset_labeled_train_tokenized_base,  # Training dataset\n",
    "    eval_dataset=dataset_labeled_valid_tokenized_base,  # Evaluation dataset\n",
    "    tokenizer=tokenizer_base,  # Tokenizer for preprocessing the input text\n",
    "    data_collator=data_collator_base,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5580551624298096,\n",
       " 'eval_accuracy': 0.7759667871146306,\n",
       " 'eval_runtime': 60.6451,\n",
       " 'eval_samples_per_second': 214.477,\n",
       " 'eval_steps_per_second': 1.682,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Can you guess why the accuracy is so low?`\n",
    "\n",
    "\n",
    "## Understanding Low Accuracy: The Limitation of 512 Tokens\n",
    "\n",
    "When working with transformer models, it's essential to be aware of a key limitation: most models can only process a maximum of **512 tokens**. This restriction has a significant impact on the accuracy of predictions, especially when dealing with longer texts.\n",
    "\n",
    "### The Self-Attention Mechanism and Quadratic Complexity\n",
    "\n",
    "The 512-token limit is a result of the *quadratic complexity* of the **self-attention mechanism**, which is a fundamental component of transformer models. Self-attention allows the model to weigh the importance of each token in relation to others, enabling it to capture context and dependencies within the input text.\n",
    "\n",
    "However, the computational cost of self-attention grows quadratically with the number of tokens. As the input length increases, the memory and computational requirements become prohibitively expensive. To mitigate this issue, most transformer models impose a maximum token limit of 512.\n",
    "\n",
    "### The Impact of Truncation on Accuracy\n",
    "\n",
    "When an input text exceeds 512 tokens, the model automatically truncates it by removing tokens until it fits within the limit. This truncation process can have a detrimental effect on the model's accuracy.\n",
    "\n",
    "Important information, such as key context or relevant details, may be lost during truncation. The model is forced to make predictions based on an incomplete representation of the original text, leading to lower accuracy scores.\n",
    "\n",
    "### Strategies for Handling Longer Texts\n",
    "\n",
    "While the 512-token limit can be challenging, there are several approaches to mitigate its impact:\n",
    "\n",
    "1. **Sliding Window Approach**:\n",
    "- Divide the long text into smaller, overlapping chunks (windows).\n",
    "- Process each window individually and aggregate the results.\n",
    "- This approach can help capture local context, but it may struggle with long-range dependencies.\n",
    "\n",
    "2. **Alternative Neural Network Architectures**:\n",
    "- Consider using other architectures, such as Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs).\n",
    "- These architectures can handle longer sequences without the same token limit constraints.\n",
    "- However, they may not capture long-range dependencies as effectively as transformers.\n",
    "\n",
    "3. **Transformer Variants for Longer Sequences**:\n",
    "- Explore transformer-based models specifically designed for handling longer texts, such as Longformer and BigBird.\n",
    "- These models introduce modifications to the self-attention mechanism to reduce computational complexity.\n",
    "- Keep in mind that these models are relatively new and may have limitations or trade-offs compared to standard transformers.\n",
    "\n",
    "\n",
    "To make informed decisions about handling longer texts, it's crucial to understand the characteristics of your dataset. Analyze the average number of tokens per text and the distribution of text lengths.\n",
    "\n",
    "If a significant portion of your texts exceeds the 512-token limit, consider applying one of the strategies mentioned above. Experiment with different approaches and evaluate their impact on accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52026.000000\n",
       "mean      2373.339407\n",
       "std       1822.717847\n",
       "min        151.000000\n",
       "25%       1133.000000\n",
       "50%       1799.000000\n",
       "75%       3031.000000\n",
       "max      11434.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Initialize an empty list to store the sizes of tokenized input sequences\n",
    "sizes = []\n",
    "\n",
    "# Iterate over each text in the training dataset\n",
    "for txt in df_train.text:\n",
    "    # Tokenize the text without truncation and get the length of the tokenized input sequence\n",
    "    # Append the length of the tokenized input sequence to the sizes list\n",
    "    sizes.append(len(tokenizer_base(txt, truncation=False)['input_ids']))\n",
    "\n",
    "# Convert the sizes list to a Pandas Series and display descriptive statistics\n",
    "# This provides an overview of the distribution of tokenized input sequence lengths\n",
    "pd.Series(sizes).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`As we can see above, the average number of tokens in our dataset is 2,373. This is significantly higher than the 512 token limit. Therefore, we need to employ a workaround to handle this limitation. We won't cover more complex approaches in this class, but we can use a simple and effective workaround - understanding our data! Let's see how we can do this.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SENTENA Tipo A RELATRIO Trata-se de ao declaratria de inexistncia de dbito e indenizatria por danos morais, com pedido de repetio de indbito, ajuizada por Lcia Matias de Souza em face do Instituto Nacional do Seguro Social  INSS e do Banco Bradesco S/A, em razo da existncia de contrato de emprstimo consignado celebrado perante a aludida instituio financeira que, segundo diz a autora, no foi por ela contratado.  o que importa relatar. Passo a decidir. FUNDAMENTAO Das preliminares arguidas Quanto  preliminar de ilegitimidade passiva alegada pelo INSS (anexo 11), entendo que a Autarquia r detm legitimidade para figurar no plo passivo da ao, tendo em vista que  responsvel pelo gerenciamento e pagamento dos descontos realizados nos benefcios previdencirios em decorrncia de emprstimo consignado. Assim, a partir do momento em que opera o desconto nos valores tem interesse e legitimidade para figurar no plo passivo da presente demanda. Ademais, s o INSS tem poder para fazer cessar os descontos efetuados, sendo a cessao um dos objetos dessa demanda. No que diz respeito  suposta falta de interesse de agir,  preciso dizer que o caso sob exame no cuida de concesso de beneficio previdencirio, razo pela qual se faz desnecessrio prvio requerimento administrativo. Desse modo, afastadas as preliminares suscitadas, passo ao exame do mrito. Mrito A questo posta em Juzo dispensa maiores esclarecimentos, eis que os fatos objetos de controvrsia encontram-se suficientemente provados. Os emprstimos consignados, disponibilizados aos aposentados e pensionistas do INSS, so feitos com descontos na folha de pagamento, conforme autorizado pela Lei 10.820/2003. O valor das parcelas  descontado diretamente do benefcio previdencirio. Nas relaes de consumo, que so estabelecidas entre instituies financeiras e beneficirios pactuantes dos contratos de emprstimos em consignao (Smula n.o 297 do STJ), no se pode atribuir ao consumidor, considerado parte hipossuficiente na relao jurdica, a responsabilidade pelos encargos financeiros decorrentes de ocorrncia de suspeita de fraude na operacionalizao dos emprstimos consignados. Ademais, nessas relaes, entre beneficirios e as instituies financeiras, consoante disposto no art. 6o, VIII, da Lei n.o 8.078/90, deve ser facilitada a defesa do consumidor, sobretudo considerando que as instituies financeiras detm os documentos que comprovam a celebrao dos contratos, restando dificultosa qualquer comprovao de fraude pelo beneficirio. Os doutrinadores afirmam, atualmente, que os fatos negativos so possveis de serem provados e o devem ser pela parte que os alega, diferente do que acontecia outrora, quando vigorava a aplicao da mxima negativa non sunt probanda, ou seja, que os fatos negativos no precisavam ser provados, sendo impossvel a produo de sua prova. Hoje, somente os fatos absolutamente negativos  que so insuscetveis de prova, no pela sua negatividade, mas por sua indefinio. Tem-se como impossvel se provar que nunca se esteve em determinado local. Ento, nessas hipteses, tem-se que a parte que alega o fato positivo  que tem o nus probatrio, aplicando-se a teoria da distribuio dinmica do nus da prova. Em relao aos fatos relativamente negativos, estes sim podem ser provados. Se, a ttulo de exemplo, algum afirma que no compareceu ao trabalho em um dia determinado,  possvel provar indiretamente a sua ausncia ao trabalho, comprovando que foi a um consultrio mdico. A chamada 'certido negativa', expedida pelas autoridades fiscais,  um meio de prova de que 'no h dbitos fiscais pendentes'. (DIDIER JR., Fredie. Curso de Direito Processual Civil. Vol. 2. Salvador: Editora Jus Podivm, 2007). Todavia, conforme leciona Luiz Guilherme Marinoni, quando se inverte o nus,  preciso supor que aquele que vai assumi-lo ter a possibilidade de cumpri-lo, pena de a inverso do nus da prova significa a imposio de uma perda, e no apenas a transferncia de um nus. Nessa perspectiva, a inverso do nus da prova somente deve ocorrer quando o ru tem a possibilidade de demonstrar a no existncia do fato constitutivo (Processo de Conhecimento. So Paulo: RT, 2007, p.269/270). No caso em anlise, o contrato impugnado  o Contrato no 808431996, no valor de R$565,91 (quinhentos e sessenta e cinco reais e noventa e um centavos).O extrato do NB1061609828 (anexo 07) confirma os descontos no beneficio da parte autora. Pois bem. Tanto o INSS quanto o Banco ru no carrearam os contratos em tela nem qualquer documento que pudesse comprovar a celebrao dos negcios jurdicos realizados pela parte autora. Ante a no comprovao da legitimidade dos contratos em tela, entendo que o Banco ru, como beneficiria do presente contrato, deve ser condenada a repetio de indbito. Quanto ao valor da indenizao devida, tenho que a reparao pecuniria visa a proporcionar uma espcie de compensao que atenue a ofensa causada, atentando-se, que ao beneficirio no  dado tirar proveito do sinistro, posto que no se destina a indenizao ao seu enriquecimento. Portanto, o valor deve ser apenas suficiente ao reparo, sob pena de estar o Judicirio autorizando o enriquecimento sem causa da vtima. O valor da indenizao por danos morais deve ser suficiente para, a um s tempo, desestimular reiterao da conduta lesiva pelo ru e abrandar, na medida do possvel, o constrangimento e a humilhao causados ao autor lesado. No caso em apreo, entendo que o pagamento de R$ 5.000,00 (cinco mil reais),  suficiente para atender  pretenso formulada. Com efeito, a indenizao nos valores indicados demonstra-se adequada, na medida em que, deve-se ter em mente o seu carter pedaggico, a fim de desestimular os rus a proceder, com desdia, em suas atividades, mormente quando envolvido interesses de pessoas que se encontrem em posio de maior vulnerabilidade. Portanto, tenho que resta configurada a responsabilidade do Banco Bradesco de indenizar, em dobro, o valor indevidamente descontado do benefcio da parte promovente, nos termos do art. 42, pargrafo nico do CDC, bem como indenizar os danos morais suportados pela parte promovente. Registre-se que, nas demandas referentes a emprstimos consignados celebrados fraudulentamente, a responsabilidade do INSS cinge-se to somente a cessao dos descontos no benefcio previdencirio do prejudicado, sendo a instituio financeira responsvel pela reparao dos danos materiais e morais eventualmente suportados. DISPOSITIVO Isso posto, julgo PROCEDENTE o pedido para determinar que o INSS cesse os descontos das parcelas do Contratono 808431996. Condeno, tambm, a ttulo de danos materiais, o Banco Bradesco a devolver os valores descontados com relao aos citados contratos de emprstimo, em dobro, nos termos do art. 42, pargrafo nico, do CDC, devendo tais valores serem acrescidos de juros de mora de 1% ao ms desde o evento danoso (smula 54  STJ) e correo monetria com base no IPCA-E desde o efetivo prejuzo (smula 43  STJ). Condeno, ainda, o bancorua pagar, a ttulo de indenizao por danos morais, a quantia de R$ 5.000,00 (cinco mil reais), valor este que deve ser atualizado exclusivamente pela taxa SELIC desde a publicao desta sentena. Declaro a inexistncia do contrato no808431996. Declaro extinto o processo com resoluo do mrito, nos termos do art. 487, I, do Cdigo de Processo Civil. Custas e honorrios advocatcios indevidos em primeiro grau de jurisdio (art. 55 da Lei no 9.099/95, c/c art. 1o da Lei no 10.259/01). Registre-se. Intimem-se as partes (Lei no 10.259/01, art. 8o). Campina Grande-PB, data supra. JUIZ FEDERAL\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10, random_state=271828)['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Can you notice that the really relevant information for our classification task is not in the beginning of the text, but in the end?`\n",
    "\n",
    "> (....)\n",
    ">\n",
    "> DISPOSITIVO Isso posto, `julgo PROCEDENTE` o pedido para determinar que o INSS cesse os descontos das parcelas do Contratono 808431996. Condeno, tambm, a ttulo de danos materiais, o Banco Bradesco a devolver os valores descontados com relao aos citados contratos de emprstimo, em dobro, nos termos do art. 42, pargrafo nico, do CDC, devendo tais valores serem acrescidos de juros de mora de 1% ao ms desde o evento danoso (smula 54  STJ) e correo monetria com base no IPCA-E desde o efetivo prejuzo (smula 43  STJ). Condeno, ainda, o bancorua pagar, a ttulo de indenizao por danos morais, a quantia de R$ 5.000,00 (cinco mil reais), valor este que deve ser atualizado exclusivamente pela taxa SELIC desde a publicao desta sentena. Declaro a inexistncia do contrato no808431996. Declaro extinto o processo com resoluo do mrito, nos termos do art. 487, I, do Cdigo de Processo Civil. Custas e honorrios advocatcios indevidos em primeiro grau de jurisdio (art. 55 da Lei no 9.099/95, c/c art. 1o da Lei no 10.259/01). Registre-se. Intimem-se as partes (Lei no 10.259/01, art. 8o). Campina Grande-PB, data supra. JUIZ FEDERAL\n",
    ">\n",
    "\n",
    "This is very common in this kind of documents. The judge starts with a thorough description of the case and then goes to the decision. So, we can use the last 512 tokens of the text to train our model. We just need to change the truncation_side parameter to 'left' in the tokenizer.\n",
    "\n",
    "Let's see how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_base.truncation_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Eu gosto muito [SEP]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input text using the base tokenizer\n",
    "# The padding=True argument ensures that the sequence is padded to the maximum length\n",
    "# The truncation=True argument ensures that the sequence is truncated to the maximum length if it exceeds it\n",
    "# The max_length=5 argument sets the maximum length of the tokenized sequence to 5 tokens\n",
    "out_len5 = tokenizer_base('Eu gosto muito de farofa com banana', padding=True, truncation=True, max_length=5) # This is to simulate the truncation\n",
    "\n",
    "# Decode the tokenized input IDs back to a string\n",
    "# This converts the token IDs back to the corresponding text\n",
    "# The decoded text will be truncated to the first 5 tokens\n",
    "tokenizer_base.decode(out_len5['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the truncation side for the base tokenizer to 'left'\n",
    "# This means that if the input text needs to be truncated, tokens will be removed from the beginning (left side) of the sequence\n",
    "# This setting is useful when the most important information is at the end of the sequence\n",
    "tokenizer_base.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] com banana [SEP]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input text using the base tokenizer\n",
    "# The padding=True argument ensures that the sequence is padded to the maximum length\n",
    "# The truncation=True argument ensures that the sequence is truncated to the maximum length if it exceeds it\n",
    "# The max_length=5 argument sets the maximum length of the tokenized sequence to 5 tokens\n",
    "out_len5 = tokenizer_base('Eu gosto muito de farofa com banana', padding=True, truncation=True, max_length=5)\n",
    "\n",
    "# Decode the tokenized input IDs back to a string\n",
    "# This converts the token IDs back to the corresponding text\n",
    "# The decoded text will be truncated to the first 5 tokens\n",
    "tokenizer_base.decode(out_len5['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the training DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for training and evaluation\n",
    "dataset_labeled_train = datasets.Dataset.from_pandas(df_train)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for validation and evaluation\n",
    "dataset_labeled_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Define a function to preprocess the input examples using a specified tokenizer\n",
    "# The function tokenizes the input text, truncates it to a maximum length of 512 tokens,\n",
    "# and pads the sequences to ensure they are of equal length\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a partial function for preprocessing using the base tokenizer\n",
    "# This partial function allows us to call preprocess_function with only the examples argument,\n",
    "# as the tokenizer argument is already set to tokenizer_base\n",
    "preprocess_function_base = partial(preprocess_function, tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a partial function for preprocessing using the large tokenizer\n",
    "preprocess_function_large = partial(preprocess_function, tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb6f4c92f9944a69fef4b63c528b4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59850c67d6cf46c4877ea48a4d9e6d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The preprocess_function_base tokenizes the text, truncates it to 512 tokens, and pads the sequences\n",
    "# The batched=True argument processes the dataset in batches for efficiency\n",
    "dataset_labeled_train_tokenized_base = dataset_labeled_train.map(preprocess_function_base, batched=True)\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_labeled_valid_tokenized_base = dataset_labeled_valid.map(preprocess_function_base, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for the base tokenizer\n",
    "# The data collator dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "data_collator_base = DataCollatorWithPadding(tokenizer=tokenizer_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate module from the Hugging Face library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate module\n",
    "# This metric will be used to evaluate the performance of the model\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "# This function will be used to evaluate the performance of the model during training and validation\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert the model's output logits to predicted class labels\n",
    "    # np.argmax(predictions, axis=1) selects the index of the maximum logit for each prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute the accuracy metric using the predicted and true labels\n",
    "    # accuracy.compute() calculates the accuracy of the predictions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels in the training dataset\n",
    "# This will be used to configure the classification model\n",
    "n_labels = df_train.label.nunique()\n",
    "\n",
    "# Load the configuration for the base masked language model (MLM) and modify it for sequence classification\n",
    "# The configuration is loaded from the specified directory and the number of labels is set to n_labels\n",
    "config_base = AutoConfig.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\", \n",
    "    num_labels=n_labels\n",
    ")\n",
    "\n",
    "# Load the base masked language model (MLM) and modify it for sequence classification\n",
    "classifier_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\", \n",
    "    config=config_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2710' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2710/2710 1:04:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.125238</td>\n",
       "      <td>0.955178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.121206</td>\n",
       "      <td>0.957408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.121330</td>\n",
       "      <td>0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.118434</td>\n",
       "      <td>0.961252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.121680</td>\n",
       "      <td>0.961790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=0.11554007523614102, metrics={'train_runtime': 3846.022, 'train_samples_per_second': 67.636, 'train_steps_per_second': 0.705, 'total_flos': 6.844430787637248e+16, 'train_loss': 0.11554007523614102, 'epoch': 5.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the base classifier\n",
    "# These arguments configure various aspects of the training process\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base/\"base_classifier_legal\",  # Directory to save the model and other outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=48,  # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation (adjust based on GPU memory)\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients before updating\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use 16-bit floating point precision for training (adjust based on GPU support)\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for the base classifier\n",
    "# The Trainer handles the training and evaluation of the model\n",
    "trainer_base = Trainer(\n",
    "    model=classifier_base,  # The model to be trained\n",
    "    args=training_args_base,  # Training arguments\n",
    "    train_dataset=dataset_labeled_train_tokenized_base,  # Training dataset\n",
    "    eval_dataset=dataset_labeled_valid_tokenized_base,  # Evaluation dataset\n",
    "    tokenizer=tokenizer_base,  # Tokenizer for preprocessing the input text\n",
    "    data_collator=data_collator_base,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/weak_ner_health/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11843354254961014,\n",
       " 'eval_accuracy': 0.9612516337356808,\n",
       " 'eval_runtime': 60.1118,\n",
       " 'eval_samples_per_second': 216.38,\n",
       " 'eval_steps_per_second': 1.697,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achieved a significant improvement in our model's accuracy, which soared from 77.9% to an impressive 96.1%. This upswing is indeed fantastic news!\n",
    "\n",
    "Let's gain a better understanding of this improvement by examining it in terms of the error rate. The error rate is simply calculated as (1 - accuracy). With this formula, our initial error rate was 22.5%, and our improved error rate dropped dramatically to 3.9%.\n",
    "\n",
    "To put this into perspective, we've effectively reduced the error rate by nearly six-fold! In other words, our model is now making far fewer mistakes than before, indicating an exponential enhancement in its overall performance.\n",
    "\n",
    "By using the last 512 tokens in the text data, we were able to direct the focus of our model towards the most relevant information. This approach is a simple yet effective workaround to overcome the 512 token limitation in transformers.\n",
    "\n",
    "This method may seem simple, but it's proven to be an effectively strategic approach to overcome such limitations and handle large amounts of data proficiently. `Remember, sometimes simplicity is the key to master complex challenges!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is the key advantage of transformers compared to traditional sequential models like RNNs and LSTMs?\n",
    "\n",
    "2. What is the role of the attention mechanism in transformers?\n",
    "\n",
    "3. What are the main components of a typical transformer architecture?\n",
    "\n",
    "4. What is the impact of quadratic complexity on the performance of transformers for longer texts?\n",
    "\n",
    "5. How have transformers been applied beyond natural language processing (NLP)?\n",
    "\n",
    "6. What are some common transformer architectures used for NLP tasks?\n",
    "\n",
    "7. How can transformers be used as feature extractors?\n",
    "\n",
    "8. What are the key steps for using transformers on a specific task?\n",
    "\n",
    "9. What is the limitation of the 512-token limit in transformers, and how does it impact accuracy?\n",
    "\n",
    "10. What is a simple workaround to handle the 512-token limit and improve accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "<!--\n",
    "1. Transformers can handle long-range dependencies effectively and parallelize computations, avoiding the vanishing gradient problem that plagues RNNs and LSTMs.\n",
    "\n",
    "2. The attention mechanism allows the model to focus on the most relevant parts of the input sequence when predicting a specific output, enabling it to capture complex relationships and dependencies between words.\n",
    "\n",
    "3. A typical transformer consists of an encoder and a decoder, each composed of multiple identical layers. The key components include self-attention layers, feed-forward neural networks, and positional encoding.\n",
    "\n",
    "4. The quadratic complexity of transformers results in slower training times and high memory consumption when dealing with longer sequences, hindering their practicability in scenarios involving extensive texts.\n",
    "\n",
    "5. Transformers have been successfully applied in various domains, including computer vision (e.g., Image Transformer, Vision Transformer), music generation (e.g., MuseNet), speech recognition (e.g., Speech-Transformer), and video processing (e.g.,\n",
    "Video Transformer).\n",
    "\n",
    "6. Common transformer architectures for NLP include BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pretrained Transformer), RoBERTa (Robustly Optimized BERT Approach), T5 (Text-to-Text Transfer Transformer), and\n",
    "XLNet.\n",
    "\n",
    "7. Transformers can be used as feature extractors by leveraging their ability to capture rich syntactical and contextual information from text data. The extracted features, typically represented by the vector corresponding to the [CLS] token, can\n",
    "be used as input for downstream tasks like classification or regression.\n",
    "\n",
    "8. The key steps for using transformers include starting with a pretrained model, optionally fine-tuning the model on domain-specific text, training the model for the specific task using task-specific data, and using the model as a feature\n",
    "extractor if needed.\n",
    "\n",
    "9. Most transformer models can only process a maximum of 512 tokens due to the quadratic complexity of the self-attention mechanism. When an input text exceeds this limit, it is truncated, potentially losing important information and leading to\n",
    "lower accuracy in predictions.\n",
    "\n",
    "10. A simple workaround is to focus on the most relevant information in the text data. For example, in legal documents where the decision is often at the end, using the last 512 tokens of the text can significantly improve accuracy by directing the model's attention to the most important part of the document. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class_applied_ml_tre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
