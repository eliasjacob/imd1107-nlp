{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "## IMD1107 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](htttps://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Keypoints\n",
    "- Transfer learning leverages pre-trained models as a starting point, significantly reducing training time and data requirements for NLP tasks.\n",
    "\n",
    "- Tokenization breaks down text into meaningful semantic units (tokens), enabling models to understand the base units and context of words. Subword tokenization handles complex morphologies and unseen words.\n",
    "\n",
    "- Language modeling is a self-supervised learning task that captures language structure and patterns by predicting the next or masked words in a sequence, providing valuable insights for various NLP tasks.\n",
    "\n",
    "- ULMFiT is a transfer learning approach that pre-trains a language model on a large corpus, fine-tunes it on domain-specific text, and then uses the fine-tuned model for downstream NLP tasks, improving performance even with limited labeled data.\n",
    "\n",
    "- Whole Word Masking masks entire words instead of subwords or tokens, forcing the model to understand the contextual meaning of the masked word as a whole, leading to better semantic understanding.\n",
    "\n",
    "- Fine-tuning a pre-trained language model on domain-specific text before training a classifier is crucial for adapting the model to the target domain and capturing task-specific nuances, resulting in superior performance compared to using the pre-trained model directly or training from scratch.\n",
    "\n",
    "## Takeaways\n",
    "- Transfer learning greatly improves the performance and efficiency of NLP models, especially when labeled data is limited, by employing knowledge from large-scale pre-training on vast amounts of unlabeled text.\n",
    "\n",
    "- Self-supervised learning techniques like language modeling enable models to learn valuable information about language structure and semantics from unlabeled data, reducing reliance on labeled data and fostering a deep understanding of language that can be transferred to various NLP tasks.\n",
    "\n",
    "- Proper tokenization strategies, such as subword tokenization and Whole Word Masking, are essential for models to effectively process and understand the semantic meaning of text, handling complex morphologies and out-of-vocabulary words.\n",
    "\n",
    "- The practical example demonstrates the power of transfer learning and domain-specific fine-tuning, with the fine-tuned pre-trained model outperforming both the model trained from scratch and the pre-trained model without fine-tuning, highlighting the importance of adapting models to the target domain for optimal performance in NLP tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language Processing (NLP)\n",
    "\n",
    "Deep learning is a powerful tool to process and analyze large amounts of data, which has been particularly effective in the field of Natural Language Processing (NLP). In this document, we will cover the basics of deep learning for NLP, including understanding key concepts like neural networks, perceptrons, feedforward neural networks, and the backpropagation algorithm.\n",
    "\n",
    "\n",
    "For a thorough yet accessible approach, please check the [Deep Learning for Coders](https://github.com/fastai/fastbook) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Overview\n",
    "\n",
    "Deep learning is a subfield of machine learning that relies on artificial neural networks, which are inspired by biological neural networks. By learning from examples, deep learning enables computers to perform tasks that come naturally to humans. It is a crucial technology behind innovations like driverless cars, voice-controlled devices, and advanced recommendation systems.\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron. In their [paper](https://link.springer.com/article/10.1007/BF02478259) \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" they declared that:\n",
    "\n",
    "A neural network is an interconnected group of neurons, or nodes. These networks can be biological, consisting of real neurons, or artificial, designed for solving AI problems. The connections between neurons in the network are modeled as weights, with positive values representing excitatory connections and negative values representing inhibitory connections. Inputs are modified by weights and summed up in a process called linear combination. An activation function then controls the output's amplitude, usually within a specific range (e.g., 0-1 or -1-1).\n",
    ">\n",
    "> Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms.\n",
    ">\n",
    "McCulloch and Pitts realized that a simplified model of a real neuron could be represented using simple addition and thresholding. Pitts was self-taught, and by age 12, had received an offer to study at Cambridge University with the great Bertrand Russell. He did not take up this invitation, and indeed throughout his life did not accept any offers of advanced degrees or positions of authority. Most of his famous work was done while he was homeless. Despite his lack of an officially recognized position and increasing social isolation, his work with McCulloch was influential, and was taken up by a psychologist named Frank Rosenblatt.\n",
    "\n",
    "<img alt=\"Natural and artificial neurons\" width=\"500\" caption=\"Natural and artificial neurons\" src=\"images/chapter7_neuron.png\" id=\"neuron\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "\n",
    "Rosenblatt's perceptron, often considered one of the earliest forms of machine learning, had its origin as an attempt to teach a machine to recognize images. Although we might not consider his original creation as a computer in the contemporary sense due to its hardware composition, the basic concept remains relevant and foundational.\n",
    "\n",
    "#### Structure and Function of the Perceptron\n",
    "\n",
    "The perceptron was essentially an assembly of photo-receptors and potentiometers, designed to mimic the biological neuron's structure and function. It effectively operated through identifying and processing features captured from small subdivisions of an entire image. Each feature, correlating to part of the image, would be assigned a weight indicating its significance or relevance to the overall interpretation of the image.\n",
    "\n",
    "##### Perceiving the Image: The Role of Photo-Receptors\n",
    "\n",
    "For the perceptual process, an image was exposed to a grid of photo-receptors. Each photo-receptor would perceive a minuscule section of the image and interpret the brightness level within that section. The perceived brightness would subsequently determine the strength of the signal that the photo-receptor would relay to its associated \"dendrite.\"\n",
    "\n",
    "##### Processing the Signal: The Role of Dendrites\n",
    "\n",
    "In Rosenblatt's design, each dendrite was equipped with a potentiometer, functioning as a form of adjustable weight. These weights were critical in governing whether the signal received by the dendrite was deemed sufficiently sturdy to pass on to the next stage, the \"nucleus\" or main body of the \"cell.\"\n",
    "\n",
    "##### Classifying the Image: The Decision at the Nucleus\n",
    "\n",
    "At the nucleus, signals from various dendrites amalgamated. If the combined strength of all incoming signals crossed a predetermined threshold, the perceptron would activate, transmitting a signal down its \"axon.\" This activation was equivalent to a positive classification match for the input image. Simply put, the perceptron recognized the image based on its training. Conversely, if the combined signals failed to meet the threshold, the perceptron would not activate, indicating a negative classification match.\n",
    "\n",
    "Through this process, the perceptron could perform binary classification tasks such as distinguishing between \"dog\" and \"not dog\" images. The perceptron's ability to recognize images was dependent on the weights assigned to each dendrite. These weights were adjusted during the training process, which involved exposing the perceptron to a series of images and adjusting the weights based on the perceptron's performance. The training process was iterative, and the perceptron's performance improved with each iteration.\n",
    "\n",
    "essentially, the perceptron model gives us an insight into how complex pattern recognition can begin with simple, feature-based image analysis and how weighted importance of these features can contribute towards accurate image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Networks\n",
    "\n",
    "An artificial neural network is reminiscent of a human brain in its function to process and analyze information. Among the many types of networks, a specific one, called the **Feedforward Neural Network**, is noteworthy for its simplicity and efficiency.\n",
    "\n",
    "A feedforward neural network is often called so because it allows information to travel in just one direction - from the input nodes, through any hidden layers (if present), and finally to the output nodes. As the name implies, 'feed forward' suggests that there are no loops or cycles; the data 'feeds forward' through the network. The absence of cycles makes the architecture of the network relatively simpler when compared to recurrent neural networks yet still remarkably efficient.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "Training an artificial neural network revolves around adjusting the individual weights of connections within the network until it can accurately make predictions. The key question that arises here then is - How do we adjust these weights optimally?\n",
    "\n",
    "This is where the **Backpropagation Algorithm** steps in. This term \"backpropagation\" stands for \"backward propagation of errors,\" indicating that the error in prediction starts at the output end and works its way backwards while updating the weights. Mainly used to train deep neural networks, networks with more than one hidden layer, the backpropagation algorithm plays a crucial role.\n",
    "\n",
    "The algorithm calculates the gradient (i.e., the rate of change) of the loss function concerning each weight in the network, which guides the adjustment of the weights. It uses the mathematical principle known as the chain rule to compute gradients one layer at a time, iterating backward from the last layer. \n",
    "\n",
    "This process prevents redundant calculations of intermediate terms, thereby enhancing computational efficiency. Due to these characteristics, backpropagation is deemed to be a classic example of dynamic programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Video Resources to Enhance Understanding\n",
    "\n",
    "To further aid your understanding of machine learning concepts and especially neural networks, I have curated a list of video resources. These resources range from brief overviews to more in-depth explanations and visual examples.\n",
    "\n",
    "### 1. **The Significance of Neural Networks**\n",
    "\n",
    "This super-short video gives a concise yet effective explanation on the importance of neural networks.\n",
    "\n",
    "- [Why Neural Networks are Important (in 45 seconds)](https://www.youtube.com/watch?v=PAZTIAfaNr8)\n",
    "\n",
    "### 2. **Demystifying Neural Networks' Learning Ability**\n",
    "   \n",
    "Want to know how Neural Networks gain their remarkable learning abilities? This video breaks it down in an understandable manner.\n",
    "\n",
    "- [Why Neural Networks Can Learn (Almost) Anything?](https://www.youtube.com/watch?v=0QczhVg5HaI)\n",
    "\n",
    "### 3. **Observing Neural Networks in Action**\n",
    "\n",
    "Watching neural networks learn can be a fascinating experience and this video offers exactly that.\n",
    "\n",
    "- [Watching Neural Networks Learn](https://www.youtube.com/watch?v=TkwXa7Cvfr8)\n",
    "\n",
    "### 4. **A thorough playlist about neural networks**\n",
    "\n",
    "For those who are looking for a deeper dive into how neural networks function, this playlist is filled with well-crafted videos covering various aspects of neural networks.\n",
    "\n",
    "- [Playlist with Four Insightful Videos about How Neural Networks Work](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "Taking some time out and watching these videos will surely assist you in understanding the workings of neural networks more intuitively. Happy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization: The Breakdown of Semantic Units\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization, serving as a fundamental step in Natural Language Processing (NLP), is the process of segmenting running text into individual meaningful elements, referred to as 'tokens'. Depending on the complexity of the language and the task at hand, these tokens could be simple words, sentences, and even subwords.\n",
    "\n",
    "#### Subword Tokenization\n",
    "\n",
    "Subword tokenization is a powerful technique that essentially splits words into smaller, more manageable units. This process allows NLP models to handle languages with large vocabularies, elaborate morphologies, and even words not seen during training - a common occurrence in languages with rich vocabularies like Portuguese.\n",
    "\n",
    "The inclusion of subword tokenization methods allows the model to understand the base units of each word better, leading to an improved comprehension of the semantic meaning of each sentence.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/multifit_vocabularies.png\" alt=\"\" style=\"width: 30%; height: 30%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Examining Tokens in Detail\n",
    "\n",
    "To grasp the concept of tokenization better, let's consider an example. Let's look at the Portuguese word \"cacimbinha\". Depending on the kind of tokenizer we use, this one word can be broken down into varying numbers of tokens.\n",
    "\n",
    "Consider this Python code:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "tokens = tokenizer.tokenize(\"cacimbinha\")\n",
    "print(tokens)\n",
    "```\n",
    "This generates the following output:\n",
    "```['ca', '##ci', '##mb', '##inha']```\n",
    "\n",
    "However, when using a different tokenizer, we receive a different number of tokens:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"cacimbinha\")\n",
    "print(tokens)\n",
    "```\n",
    "Output:\n",
    "```['ca', '##ci', '##mb', '##in', '##ha']```\n",
    "\n",
    "The tokenization output is highly dependent on the training of the specific tokenizer used. Hence, a single word might end up containing multiple tokens based on how the tokenizer was originally trained.\n",
    "\n",
    "Tokenization operates as the foundation for higher-level language tasks, delivering nuanced understanding to NLP models by recognizing and analyzing the fundamental semantic units within a sentence. Therefore, choosing the correct tokenizer and understanding its operations forms an essential part of building successful and effective NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is transfer learning?\n",
    "\n",
    "Transfer learning is a potent machine learning methodology in which we capitalize on a model that was developed for one task as the basis or starting point for a model designed to tackle another task. It has gained popularity, particularly in the realms of deep learning, where pre-trained models are utilized as initial points on tasks related to computer vision and natural language processing (NLP).\n",
    "\n",
    "The primary reason behind this approach lies in the immense computational and time resources necessary to develop neural network models geared toward these areas. Also, it's worth noting that such models provide substantial improvements in ability when applied to related issues.\n",
    "\n",
    "### Origins and Applications of Transfer Learning\n",
    "\n",
    "Historically, the first methodologies for transfer learning were developed for tasks related to computer vision. However, its practical applications have since then diversified significantly. For instance, in Computer Vision (CV), the pre-trained model typically takes the form of a neural network that underwent training on a benchmark dataset, such as ImageNet. This trained model can then be repurposed as the foundation for an image classification or object detection model, which will be trained on a new dataset.\n",
    "\n",
    "The basic rationale here is that the model has acquired feature extraction capabilities. Meaning, it learned how to extract relevant features from photographs that are generally handy for describing a vast variety of photograph distributions. These might include different times, varying lighting conditions, distinct objects present within the scene, amongst many others. This concept is popularly referred to as 'feature extraction'.\n",
    "\n",
    "In more recent years, specifically around 2018, transfer learning proved its mettle in the field of Natural Language Processing (NLP) and is now arguably deemed as the dominant technique within the NLP sphere. The team at [FastAI](https://www.fast.ai/) demonstrated that transfer learning could yield effective results for NLP-related tasks by introducing the [Universal Language Model Fine-Tuning (ULMFiT)](https://arxiv.org/abs/1801.06146).\n",
    "\n",
    "With ULMFiT, the process starts with pre-training a language model on a substantial corpus of unlabeled text (e.g., Wikipedia). Afterward, this pre-trained model is fine-tuned for a specific domain. This refined model can then serve as the basis for tackling other NLP tasks such as sentiment analysis, document classification, and question answering.\n",
    "\n",
    "## The Value of Transfer Learning\n",
    "\n",
    "Transfer learning introduces a myriad of benefits to the field of deep learning. Predominantly, it furnishes a means to construct accurate models in a time-efficient fashion. As such, it stands as the quickest path toward developing practical models designed to resolve common NLP problems.\n",
    "\n",
    "Also, transfer learning proves especially useful when there's scarcity in labeled training data. In such cases, we can utilize a model that underwent training on a larger dataset within the same domain as our initial point. This frequently yields better performance results in comparison to training a model from scratch.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/ulmfit_imdb.png\" alt=\"\" style=\"width: 30%; height: 30%\"/>\n",
    "</p>\n",
    "\n",
    "Even in scenarios where ample labeled data exists, applying transfer learning may enhance our models' performance. This is attributable to the depth of information the pre-trained model has already learned about the language. For instance, a pre-trained model can offer insight into the relationships between words, syntax, sentiment, and sentence structure - all of which are crucial elements for tasks involving part-of-speech tagging, named entity recognition, solving analogy tasks, question answering, sentiment analysis, and machine translation.\n",
    "\n",
    "Transfer learning is increasingly becoming a cornerstone in today's machine learning and artificial intelligence advancements, owing to several practical advantages:\n",
    "\n",
    "- **Reduced Training Time**: Pre-trained models have already learned features from massive datasets. Hence, time required to train these models on new tasks can be significantly reduced compared to training models from scratch.\n",
    "\n",
    "- **Lower Computational Requirements**: Because pre-trained models require less time to train, the computational resources necessary are also substantially decreased.\n",
    "\n",
    "- **Better Performance with Limited Data**: In scenarios where available data for training is limited, transfer learning offers better model performance because it leverages knowledge from related tasks.\n",
    "\n",
    "- **Broad Applicability**: One of the key strengths of transfer learning is its broad applicability across many domains including image processing, natural language processing, and audio recognition, among others.\n",
    "\n",
    "## The ULMFiT Approach\n",
    "\n",
    "The ULMFiT methodology is executed by following these key steps:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/ulmfit.svg\" alt=\"\" style=\"width: 60%; height: 60%\"/>\n",
    "</p>\n",
    "\n",
    "1. **Step 1: Pre-training** - This is the first step in the ULMFiT approach, which involves training a language model on a large corpus of unlabeled text. The language model here tries to predict the next word in a sentence, which inherently forces the model to learn about grammar, semantics, and even some world facts.\n",
    "2. **Step 2: Fine-tuning** - The second step involves fine-tuning the previously trained language model according to specific needs of the target task. During this stage, the general-domain language model adapts itself to the idiosyncrasies of the domain-specific text.\n",
    "3. **Step 3: Classifier Fine-tuning** - Finally, we add a classifier to the model and fine-tune it for the target task. This way, the model leverages the transfer learning capabilities and, along with its classification layer and tuned parameters, can classify or predict outcomes.\n",
    "\n",
    "\n",
    "## Precautions in Using Transfer Learning\n",
    "\n",
    "While transfer learning is certainly powerful, users should be aware of a few considerations:\n",
    "\n",
    "- **Data Similarity**: The success of transfer learning highly depends on the similarities between the source and target data. If the data sets have little in common, the performance gain from transfer learning will likely be diminished.\n",
    "\n",
    "- **Task Complexity**: Transfer learning might not yield significant benefits for simple tasks that can be performed with minimal computational resources and time.\n",
    "\n",
    "- **Fine-tuning**: Care should be taken when fine-tuning the pre-trained model. Over-fine-tuning may lead to overfitting, where the model performs poorly on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to kickstart your NLP journey with Transfer Learning by using self-supervised learning\n",
    "\n",
    "### The Power of Pre-training\n",
    "As a primer, when initiating your neural network training, it's highly recommended to start with a pre-trained model. This approach is much more effective than launching the process with random weights, as starting from that point would entail training a model that fundamentally knows nothing! With pre-training, you greatly diminish the amount of data required for the training phase by almost 1000 times compared to starting from scratch.\n",
    "\n",
    "### Pre-trained Models and Domain-Specific Challenges\n",
    "However, you might wonder what can be done in scenarios where no pre-trained models exist within your specific domain. For instance, medical imaging is one such field with a scarce availability of pre-trained models. A recent insightful paper, [Transfusion: Understanding Transfer Learning for Medical Imaging](https://arxiv.org/abs/1902.07208), tackled this question. It found that even incorporating a few early layers from a pre-trained ImageNet model could potentially increase the speed of training and boost the final accuracy of medical imaging models. Therefore, regardless of the domain-specificity of your problem, employing a general-purpose pre-trained model would still benefit you.\n",
    "\n",
    "Nonetheless, as indicated in the paper, the improvement achieved through applying an ImageNet pre-trained model on medical imaging is modest. We ideally need a method that works more effectively without requiring vast amounts of data. Enter \"self-supervised learning\".\n",
    "\n",
    "### Unveiling Self-Supervised Learning\n",
    "\n",
    "Self-supervised learning becomes our secret weapon under these circumstances. Definition-wise, self-supervised learning involves training a model using labels inherently contained in the input data, which eliminates the need for separate external labels.\n",
    "\n",
    "ULMFiT leverages self-supervised learning to dramatically enhance the latest in this crucial field.\n",
    "\n",
    "### How to use Self-Supervised Learning for NLP\n",
    "In ULMFiT, we start by pre-training a \"language model\". Essentially, this is a model that learns to predict the subsequent word in a sentence. Now, our primary focus is not necessarily on the language model itself. However, it has been observed that a model capable of completing this task invariably gains insights about the nature of language and somewhat about the world during its training process. This pre-trained language model can then be fine-tuned for another task (like sentiment analysis, for instance). What's even more amazing is that this method quickly yields latest results with very little data.\n",
    "\n",
    ">\n",
    "> Language modeling is the \"de facto\" way to apply self-supervised learning to NLP. However, it's worth noting that there are several ways to capitalize on self-supervised learning for language modeling tasks, which we'll explore in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "Language modeling is a fundamental task in natural language processing that involves predicting the next word or character in a sequence of text. The goal of language modeling is to capture the fundamental structure and patterns of language, which can be used to generate new text that is similar to the original corpus. Language modeling is a key component in many NLP applications such as speech recognition, machine translation, part-of-speech tagging, parsing, optical character recognition, handwriting recognition, and information retrieval.\n",
    "\n",
    "To model language, we need to frame it as a self-supervised learning problem. Self-supervised learning is a type of unsupervised learning that uses a pretext task to train a model on a large amount of unlabeled data. The pretext task is designed to solve a simpler problem than the target task, which is the actual task we want to solve. For example, in the case of language modeling, the pretext task is to predict the next word in a sequence of text, and the target task is to classify a document into a category. The pretext task is easier to solve than the target task because it requires less information about the data. However, the pretext task is still useful because it provides a way to learn the basic structure of the data, which can be used to solve the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Word Prediction\n",
    "\n",
    "Next word prediction is a specific type of language modeling that involves predicting the next word in a sequence of text. This task is typically performed by training a model on a large corpus of text and then using that model to predict the probability distribution of the next word given the previous words in the sequence. Next word prediction is a key component in many NLP applications such as autocomplete, text completion, and text generation.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/nwp.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Masked Language Modeling\n",
    "\n",
    "Masked language modeling is a task in natural language processing that involves predicting a missing word in a sentence. In this task, a random word in a sentence is replaced with a mask token, and the model is trained to predict the original word. This task is useful in applications such as language translation and text generation, where it is important to understand the context of a sentence and predict missing words. Masked language modeling is typically performed using a pre-trained language model such as BERT or GPT-2.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/MLM.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving MLM with Whole Word Masking\n",
    "\n",
    "Whole Word Masking can be considered as an advanced stage of Masked Language Modeling. This refinement is a significant stride away from traditional approaches where individual tokens or random subwords are hidden by a mask token. Instead, Whole Word Masking takes into account and isolates complete words, considering the entire semantic unit in its approach.\n",
    "\n",
    "### Understanding Whole Word Masking\n",
    "\n",
    "In Whole Word Masking, a random word within a given sentence is masked, that is concealed. The goal of the model then becomes, to correctly predict this original, concealed word. This methodology compels the model to deeply comprehend the contextual connotations of the sentence while attempting to predict the masked word.\n",
    "\n",
    "Consider this illustration: We have the phrase, `\"Eu amo viajar para Cacimbinha\"``. with respect to this sentence, \"Cacimbinha\" is a proper noun, and maintaining the integrity of the full word is crucial for understanding the sentence's context. If we were to employ a traditional masking technique, \"Cacimbinha\" would become fragmented into several tokens. This shattering results in lost context and meaning, as seen below:\n",
    "\n",
    "`['eu', 'am', '##o', 'via', '##jar', 'para', 'ca', '##ci', '##mb', '##in', '[MASK]']`\n",
    "\n",
    "Here, \"Cacimbinha\" is broken down into numerous tokens and the model finds it too simple to predict the masked token. However, if Whole Word Masking is activated, \"Cacimbinha\" can be masked as an entire token, obligating the model to decipher the sentence's context to predict the concealed word.\n",
    "\n",
    "`['eu', 'am', '##o', 'via', '##jar', 'para', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]']`\n",
    "\n",
    "In this scenario, the model has to grapple with the overall context – \"Cacimbinha\" being a place the speaker loves travelling to. This method bolsters the understanding of the complex relationship between words, subsequently improving comprehension of sentence structure and semantics.\n",
    "\n",
    "### Advantages of Whole Word Masking\n",
    "\n",
    "Whole Word Masking's importance is particularly palpable when utilized for tasks that entail language translation and text generation; tasks where preserving context and maintaining meaningful sentence structure are of utmost importance.\n",
    "\n",
    "By masking complete words rather than fractions of words or random tokens, the NLP model pays greater attention to elaborate word relationships as well as broader syntactic and semantic structures within the sentences. As a result, it's able to provide more precise predictions about the masked words, which ultimately results in superior translations or generated texts.\n",
    "\n",
    "Further refining its efficacy, Whole Word Masking improves the model's ability to handle unseen or out-of-vocabulary words. Additionally, it deals better with complicated phrases, idioms, and multi-word expressions that might otherwise be troublesome to process.\n",
    "\n",
    "### Collaboration of Whole Word Masking and Tokenization Strategies\n",
    "\n",
    "Pairing effective tokenization strategies with advanced mechanisms like Whole Word Masking provides the necessary tools to build stable and efficient NLP models. These models can get deeper linguistic nuances and offer more precise translations and text generation.\n",
    "\n",
    "Whole Word Masking combined with thoughtful tokenization forms the backbone of fostering successful NLP models. They collectively pave the path for greater accuracy in comprehending and translating human languages, making machine interaction increasingly natural and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Transfer-Learning through Practice\n",
    "\n",
    "We will check practical application of transfer learning using the renowned [HuggingFace Transformers](https://huggingface.co/transformers/) library. This is an advanced tool for Natural Language Processing (NLP) that comes equipped with pre-trained models, fine-tuning mechanisms and supports numerous NLP tasks.\n",
    "\n",
    "Our task at hand is to predict whether product reviews in Portuguese are positive or negative by using a simple sentiment analysis dataset. The goal here is to demonstrate how transfer learning can be leveraged to create a sentiment analysis model with minimal data available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setting the Scene\n",
    "\n",
    "Let's suppose you work as a Machine Learning engineer at `Americanas`. The company is facing a financial challenge due to a 40 billion dollars debt which restricts them from labeling a large dataset to train a sentiment analysis model. However, they do possess a modest dataset of 600 product reviews. Their objective is to utilize this small dataset to train a model that can accurately predict whether a given review is positive or negative.\n",
    "\n",
    "Alongside the labeled examples, they also have a massive unlabeled dataset consisting roughly of 100,000 reviews. As unlabeled data is cost-free, it can be used extensively without concerns about budget constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Approach\n",
    "\n",
    "We'll adopt the following sequence of steps to address this scenario:\n",
    "\n",
    "### Option 1 - Develop a Classifier from Scratch\n",
    "\n",
    "To set the baseline, we'll start by creating a transformer classifier from scratch, without relying on any pre-trained models. This method, prevalent before 2018, will help us understand the significance of transfer learning in text classification.\n",
    "\n",
    "By building a classifier from the ground up, we can gain valuable insights into the challenges and limitations of training models with limited labeled data. This approach will require significant computational resources and time, as the model will need to learn all the necessary features and patterns from our small dataset of 600 examples.\n",
    "\n",
    "### Option 2 - Use a Pretrained Language Model\n",
    "\n",
    "We'll employ a pretrained transformer model for Portuguese known as **BERTimbau** and train a classifier with our limited labeled dataset (600 examples). This will serve as our initial transfer learning model. Despite the small amount of labeled data, this model will likely outperform the baseline due to the massive volume of Portuguese text that the language model was trained on.\n",
    "\n",
    "By employing the pretrained BERTimbau model, we can take advantage of the rich linguistic knowledge it has acquired from extensive training on large-scale Portuguese text corpora. This transfer learning approach allows us to build upon the model's existing understanding of the language, enabling it to better capture the nuances and semantics of our specific classification task.\n",
    "\n",
    "### Option 3 - Use a Pretrained Language Model and Fine-tune it on Domain-Specific Text\n",
    "\n",
    "For this step, we'll need to:\n",
    "\n",
    "1. **Fine-tune the Pretrained Language Model:** We'll fine-tune the BERTimbau pretrained language model on our 100,000 unlabeled reviews, creating our second transfer learning model. Although this adaptation doesn't directly aid classification, it enables the model to comprehend the peculiarities of our domain (product reviews) and transfer this understanding to the classifier.\n",
    "\n",
    "By fine-tuning BERTimbau on our domain-specific text, we can further adapt the model to the unique characteristics and vocabulary of product reviews. This process allows the model to capture the subtle nuances, sentiment expressions, and language patterns commonly found in customer feedback, ultimately enhancing its ability to accurately classify reviews.\n",
    "\n",
    "2. **Train a Classifier with Fine-tuned Pretrained Language Model:** Lastly, using the domain fine-tuned language model, we'll train another classifier on our concise labeled dataset of 600 examples. This step will fully unleash the potential of transfer learning, as it will allow our model to recognize domain-specific features (product reviews) which aren't ordinarily present in a general language model.\n",
    "\n",
    "By training the classifier on top of the fine-tuned language model, we can capitalize on the model's deep understanding of both the Portuguese language and the product review domain. This combination of general linguistic knowledge and domain-specific insights will enable the classifier to make more accurate predictions, even with a limited amount of labeled data.\n",
    "\n",
    "> We'll then compare the performance of these three approaches using a holdout dataset containing 41,354 reviews. This will simulate what would happen if we were to deploy our model in a real-world scenario.\n",
    "\n",
    "By evaluating the models on a substantial holdout dataset, we can assess their generalization capabilities and robustness in handling unseen data. This comparison will provide valuable insights into the effectiveness of each approach and help us determine the most suitable model for real-world deployment.\n",
    "\n",
    "Through this systematic exploration of different transfer learning strategies, you'll notice the power of  pretrained language models and domain-specific fine-tuning in text classification tasks, particularly when labeled data is scarce. By comparing the performance of these approaches, we can make informed decisions on the best practices for developing accurate and reliable sentiment analysis models concerning product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Steps\n",
    "\n",
    "- **Option 1:** Develop a classifier from scratch.\n",
    "- **Option 2:** Use a pretrained language model (BERTimbau) and train a classifier on top of it.\n",
    "- **Option 3:** Fine-tune the pretrained language model on domain-specific text, then train a classifier on the labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing ourselves for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "split",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0089a5e7-eae1-4234-909c-0e2b5ce7aacd",
       "rows": [
        [
         "20938",
         "b2w",
         "47dd8d461db193a7050331933268cc536925fa3fb3fc21584d789232d1c47187_113048617",
         "nao gostei _##_ quando faz duas xicaras transborda. e joga o po para a xicara",
         "0",
         "train"
        ],
        [
         "112876",
         "b2w",
         "a922bcb60952b82664bada9c66c461603d43fce6e9a4fc2505cc1456e8db964b_119370826",
         "otimo aparelho _##_ nao e barulhenta... simples de usar, rapida muito rapido...",
         "1",
         "train"
        ],
        [
         "30813",
         "olist",
         "fa810fdb06c8ca30b4b596a6f5a13aa1",
         "_##_ meu produto n chegou entao n vou falar nada",
         "0",
         "train"
        ],
        [
         "108072",
         "olist",
         "a898722ed49b88ab4b45b35776ad1180",
         "nota 10!!! _##_ otimo site! entrega no prazo! ou melhor chegou antes do prazo!! super recomendo!!",
         "1",
         "train"
        ],
        [
         "22060",
         "b2w",
         "a121941db8eedaa20b5e0e16e38ed9025d48e56f89001846edace2c0d986a747_122606086",
         "muito bom. _##_ excelente produto, com boa performance e com boa apresentacao.",
         "1",
         "train"
        ],
        [
         "130738",
         "olist",
         "1163320b6754899045bd9079f8e0ce3b",
         "_##_ produto veio em otimo estado, tudo certo",
         "1",
         "train"
        ],
        [
         "125109",
         "b2w",
         "e07c0de4f7bf55d7e4e16f8d2785d5fb74c5ff7909f5bec6fb5e5cf08a95f01c_132517706",
         "produto incompleto _##_ produto nao possui as caracteristicas q necessito, inclusive o produto e tao ruim qto o atendimento das americanas, efetuei a troca por telefone, o prazo para coleta do mesmo e ate hoje (15/01/2018) e ate agora as 13:11hs nao apareceu ninguem aqui p fazer essa coleta e nao consigo falar c ninguem q tenha competencia p resolver nem por e-mail nem por telefone., provavelmente americanas nunca mais. e muita propaganda e pouquissima competencia.",
         "0",
         "train"
        ],
        [
         "29682",
         "olist",
         "f83fbe44623915112622c82798d0980f",
         "_##_ tudo ocorreu, como esperado.",
         "1",
         "train"
        ],
        [
         "90503",
         "b2w",
         "63f6df8865d8cfaded03a07e48aec9d9f6f04f3fea66b45a0a479a3fed7a0d84_132444092",
         "a melhor loja da internet sem duvida nenhuma _##_ as americanas ha um bom tempo vem sendo a melhor loja de se comprar na internet, entrega antes do prazo, preco justo.",
         "1",
         "train"
        ],
        [
         "132382",
         "b2w",
         "ec6f3244b9a59ec335d6c1c3e79aca7ff3493d84b44ec8b96b8dca1ecfcba69d_108963132",
         "nao recebi _##_ nao recebi o produto peco que entrem em contato nem satisfacao estao me dando",
         "0",
         "train"
        ],
        [
         "95967",
         "b2w",
         "3722683a9d8be73dfa2a4e38ec4445599d7deb5d8c15ea4c1a296dbdd709bea8_120962571",
         "produto veio com peca faltando,nao obtive retorno das lojas americanas.com. _##_",
         "0",
         "train"
        ],
        [
         "50319",
         "b2w",
         "6ca87312e6a10836df498e8a2a5040ee821cb95ff692c94d3f57e3a75a8f793e_119899711",
         "produto de acordo com a descricao. _##_ produto muito bom, de acordo com a descricao do vendedor, recomendo.",
         "1",
         "train"
        ],
        [
         "8074",
         "b2w",
         "3b2caebf9b10b8aabe2bd807af001430a6924d2ba92186d94a6d0d6bda764c06_113535618",
         "nem chegou _##_ ha quase um mes que eu fiz o pedido do produto e ainda nao chegou! a data de entrega estava marcada para 18/12/2017 e ja estamos em 04/01/2018. indignacao!!!",
         "0",
         "train"
        ],
        [
         "66802",
         "b2w",
         "c1d7814f99a03eb7a494227880ba237a2b9d42d9efa3605a068b1f6d79b47d4a_131157788",
         "muito bom _##_ produto muito bom. apenas a entrega que demorou um pouco..",
         "1",
         "train"
        ],
        [
         "12206",
         "b2w",
         "0676d0d566364c1c59e9fd8360720bf500f1b23e9b786b75f5c12a1917884398_132990587",
         "pre-venda _##_ deveria ter uma logistica melhor para se fazer uma pre-venda desse tamanho, demora para enviar o produto a transportadora, um prazo muito grande, poderia ter se planejado com um lancamento desse, pois lojas fisicas proximo ao cliente tambem recebem o jogo na mesma data, poderia ja enviar o jogo dos clientes que fizeram a pre-venda online para as lojas especificas na data estipulada de lancamento, assim melhorando a logistica e a pre-venda.",
         "0",
         "train"
        ],
        [
         "82240",
         "b2w",
         "7bc57ede255636fd3ce97a8662ca1131188bc5ded38891671da093416d3e331d_129705378",
         "incompleto : falta componente usb _##_ estava em viagem quando o produto chegou, quando cheguei observei que veio faltando o conector usb para que eu possa utiliza-lo.",
         "0",
         "train"
        ],
        [
         "134998",
         "b2w",
         "ffdfdf4a3e344d9cd2711a208fb39ca57f13488c2f2a863bcf6e9bea04f0732a_132608561",
         "otimo produto _##_ e um otimo produto. gostei bastante. recomendarei para todos os amigos.",
         "1",
         "train"
        ],
        [
         "49019",
         "b2w",
         "e8fdfe113d6955d0c22738245d3a5a9092cfde618ec94705a28b09b080567505_117700318",
         "pessimo _##_ estou solicitando a troca e nao estou sendo atendida.",
         "0",
         "train"
        ],
        [
         "25050",
         "b2w",
         "9a70887bda4a6df76072f60fadfee23c85fbbca3ebf9edf54ed74ac5fe7c2ea3_116796362",
         "amei _##_ produto de otima qualidade a preco justo, simplesmente estou amando!",
         "1",
         "train"
        ],
        [
         "52580",
         "b2w",
         "e0871159e398ee3b4c37715731dcec8534dea47430e9e58d802ff6e3922a9b3d_132384801",
         "bom para quem deseja estudar _##_ ele atende as minhas necessidades, como programas pesados que preciso que eu preciso baixar ele roda tranquilo sem nenhum problema, a unica coisa e que de vez em quando ele buga um pouco, tipo alguns icones ele nao aparece e apos ter ligado ele demora pra responder comandos .",
         "0",
         "train"
        ],
        [
         "75840",
         "b2w",
         "550d14a101b7d3fefe58e7feb9a21112d1b111b54fd70dab672558c047f13ad7_128479073",
         "muito bom _##_ demorou um pouco para chegar mas o produto e excelente otima televisao qualidade de imagem excelente",
         "1",
         "train"
        ],
        [
         "124755",
         "b2w",
         "e08d588a61d2d214089e022b6b40e26a4fec9102bae6a8afe9d04bc8deb62008_17746569",
         "perfeito _##_ otimo produto, comprei e instalei no portao de casa e ficou show.",
         "1",
         "train"
        ],
        [
         "77702",
         "b2w",
         "a2fc2026ae4532cb34cba25697edc410d47559e5184b66816123ace783fb24e4_132390047",
         "muito bom _##_ a camera dele e otima o design do celular e lindo eu recomendo e a entrega foi antes da data prevista",
         "1",
         "train"
        ],
        [
         "7930",
         "olist",
         "e9f150b261f893e0db906a331fdf263b",
         "_##_ demorou um pouco mais que os outros produtos comprado mesmo dia",
         "0",
         "train"
        ],
        [
         "52755",
         "b2w",
         "684f91f286b491be17ebf2deb31e8e58feb3885cdcc5c7ee47e807eaec4f1eaf_21202428",
         "nao e igual o da foto insatisfeito _##_ nao gostei, pessimo produto nao e igual da foto, fui enganado.",
         "0",
         "train"
        ],
        [
         "25288",
         "olist",
         "0e2023d49b13eebcca2fb73a1332466d",
         "otimo _##_",
         "1",
         "train"
        ],
        [
         "75278",
         "olist",
         "9071d89abb31272247510acf5d8cbbed",
         "_##_ muito bonita as panelas, e material e muito bom tambem.",
         "1",
         "train"
        ],
        [
         "131349",
         "b2w",
         "6ad175d16d7d2f63ba7b8df4a6b074c5527d2e729705d762f62b6e0bd89fe003_20139315",
         "produto falsificado _##_ recebi o produto errado e fizeram a troca. recebi ontem o novo produto e percebi que que existiam avarias como se o produto estivesse gasto ou ja tivesse sido utilizado. pude verificar que o peoduto era de ma qualidade podendo ser classificado como nao sendo de primeira linha. assim que coloquei em uso o mesmo nao funcionou. estou indignada com a procedencia da loja credenciada as lojas americanas.",
         "0",
         "train"
        ],
        [
         "26362",
         "b2w",
         "07a022344589bc8b7369ca870f305099456c5f3d6224eb2d45a0fdd52924f0fa_122807591",
         "maravilhoso _##_ amei, melhor do que outro que tenho e paguei bem mais caro. esquenta bem, o vapor e bom e o spray facil de alcancar com o dedao. podem comprar!",
         "1",
         "train"
        ],
        [
         "30539",
         "b2w",
         "fd6282ddf85270348bbc995d2aab69ce509d6e34d39563bb31fa5453788edfbc_29129979",
         "eu amei esse produto _##_ a cortina e linda,o tecido e otimo, qualidade excelente, super recomendo esse produto,a entrega foi dentro do prazo estabelecido!",
         "1",
         "train"
        ],
        [
         "107543",
         "b2w",
         "e3c0c9274c10e434dd23164e07ebfd51a11e6fe1f6e200fd807ac99e05cfcf92_132444092",
         "excelente _##_ otima aquisicao, camera top, armazenamento favoravel, nao trava.",
         "1",
         "train"
        ],
        [
         "96953",
         "b2w",
         "91a743acfc6fff6cf303994650a0778bb3ef490b55735392b13c561cf1e8587d_132444738",
         "moto g5s plus _##_ excelente! a entrega rapida como sempre! celular excelente...otima aquisicao..vale a pena o investimento.",
         "1",
         "train"
        ],
        [
         "24137",
         "olist",
         "473824106deeab4c05a629cca2727662",
         "_##_ produto chegou em 3 dias, muito rapido, a principio esta funcionamento bem, mas ainda nao utilizei.",
         "0",
         "train"
        ],
        [
         "25878",
         "b2w",
         "b8cb8201041289e1272d1e82676cc8b947a3984e25c805f4079728d194bbd3a5_131664641",
         "otimo produto _##_ produto otimo. entrega antes do prazo determinado. super recomendo ..",
         "1",
         "train"
        ],
        [
         "67652",
         "olist",
         "419edb27236d7b3c83003a934370b6de",
         "recomendo _##_ chegou antes do prazo e ainda nao pude utilizar todos os cabos, mas ja usei o da bmw e funcionou certinho",
         "1",
         "train"
        ],
        [
         "19413",
         "b2w",
         "82329e6ac41dac8a9f1ad38fe97a0a4612fb63b6223483f52be142ce28baf413_12837261",
         "servico de entrega pobre _##_ eu nao os recebi ainda e sao 2 semanas apos a data prometida",
         "0",
         "train"
        ],
        [
         "54952",
         "b2w",
         "cb9b5280e5a34556ad2fafbd07159358c42220409152433370846e1eddb1a2e7_29271319",
         "timer analogico otimo. _##_ otimo, funcionou exatamente como eu precisava. obrigado.",
         "1",
         "train"
        ],
        [
         "60143",
         "b2w",
         "7620920a10c1ec879d785c0209e2e6b5309093df94a8ce9a888e06cca18d55bc_15707246",
         "muito bom! _##_ superou as minhas expectativas. produto de qualidade, cumpre o que promete..na minha opiniao a pilha dura bastante tempo! recomendo!",
         "1",
         "train"
        ],
        [
         "60034",
         "olist",
         "7d30f2b012baed50af3a8348f416b802",
         "_##_ a manta e uma porcaria nao vale nada, e pior que as mantas do bras. nunca comprei um produto tao ruim.",
         "0",
         "train"
        ],
        [
         "65032",
         "b2w",
         "7d1ce07154f4de230c1a59ef17f00327dcd2659405f853dd7e6143b299083f5e_28743719",
         "nao recebi o produto _##_ comprei o produto e por ser uma loja bem renomeada achei que nao fosse ter problemas, voce espera receber seu produto na data prevista e nao recebe, eu mando mensagem e demoram pra responder, e quando respondem falam que o produto foi devolvido ao remetente, nao me informaram nada sobre o procedimento que iriam fazer mediante isso, falta de respeito com o consumidor.",
         "0",
         "train"
        ],
        [
         "1741",
         "b2w",
         "7a756105fff85aec66b1c503edea68de4e9e73fbf3d874e8bf2f841684441144_25411233",
         "produto errado _##_",
         "0",
         "train"
        ],
        [
         "104240",
         "b2w",
         "603d0d57056219fa786569d000de5ff90c45013335337ebc28db4fbd3afb71ae_132769701",
         "excelente _##_ excelente trabalho e obrigada pela precisao e agilidade de entrega. grata!",
         "1",
         "train"
        ],
        [
         "82882",
         "b2w",
         "2ad3ef1a56c575ead9c08b96618d65d2c7d07da7a857a4c189a3544687229545_19446815",
         "fiquei decepcionado com o produto _##_ no site a apresentacao das banquetas era muito boa, bonitas, quando recebi fiquei decepcionado com a baixa qualidade, mal acabada,",
         "0",
         "train"
        ],
        [
         "128035",
         "b2w",
         "2000cc07fd349a5634c9ee09336809234e4b0c7b1f11c7613b51d9007502612f_7422896",
         "cafeteira oster _##_ bom produto. somente um senao; nao existe filtro de papel para esse tipo de cafeteira.",
         "1",
         "train"
        ],
        [
         "91852",
         "b2w",
         "204406c0034743be7a98527a83f0b4c1ced046f6713e5508ada5408a807a4764_25794028",
         "nao da para viver sem esse acidificante _##_ nao adianta usar a melhor mascara capilar do mundo e fazer milhoes de tratamento. se seu cabelo esta com o ph irregular e poroso nao vai resolver, cuticula aberta nao segura hidratacao. k.pro profissional foi desenvolvido exatamente para isso, ele equilibra o ph dos cabelos, sela as cuticulas garantindo um efeito perfeito para seu tratamento capilar, seja ele, hidratacao, reconstrucao, ou quimica. adorei, salvou meu cabelo.",
         "0",
         "train"
        ],
        [
         "20595",
         "b2w",
         "6ca0a1b3fa3b50ffa21e61a01633fd785172ebb04268d1ec97c7c958608902ea_30164274",
         "gostei muito do produto . _##_ entrega super rapida . parabens pela agilidade na entrega e qualidade no produto .",
         "1",
         "train"
        ],
        [
         "18425",
         "olist",
         "5130a8740359ec09ecad342d3d83b4f5",
         "_##_ muito rapido",
         "1",
         "train"
        ],
        [
         "101381",
         "b2w",
         "b1a6c1a303e4a97d485aa9dea59f51e7f4b71e386886f327afe002df1ffa1875_20733584",
         "produto ainda nao chegou _##_",
         "0",
         "train"
        ],
        [
         "79493",
         "b2w",
         "c9299ad05b761d793e48d523ec42a1752d9970c23621b301ab54022875f21e60_16722049",
         "o produto nao foi entregue _##_ ainda nao recebi o produto!!! e nao consigo fazer o cancelamento e receber o dinheiro de volta!!",
         "0",
         "train"
        ],
        [
         "31583",
         "b2w",
         "a380629ddb1509ca18669d666c2053ca721eb8ff4453aaad0d9320b9e30e572f_124491219",
         "pratica e eficiente _##_ e uma impressora que resolve os meus problemas, otimo custo beneficio.",
         "1",
         "train"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 137845
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20938</th>\n",
       "      <td>b2w</td>\n",
       "      <td>47dd8d461db193a7050331933268cc536925fa3fb3fc21...</td>\n",
       "      <td>nao gostei _##_ quando faz duas xicaras transb...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112876</th>\n",
       "      <td>b2w</td>\n",
       "      <td>a922bcb60952b82664bada9c66c461603d43fce6e9a4fc...</td>\n",
       "      <td>otimo aparelho _##_ nao e barulhenta... simple...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30813</th>\n",
       "      <td>olist</td>\n",
       "      <td>fa810fdb06c8ca30b4b596a6f5a13aa1</td>\n",
       "      <td>_##_ meu produto n chegou entao n vou falar nada</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108072</th>\n",
       "      <td>olist</td>\n",
       "      <td>a898722ed49b88ab4b45b35776ad1180</td>\n",
       "      <td>nota 10!!! _##_ otimo site! entrega no prazo! ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22060</th>\n",
       "      <td>b2w</td>\n",
       "      <td>a121941db8eedaa20b5e0e16e38ed9025d48e56f890018...</td>\n",
       "      <td>muito bom. _##_ excelente produto, com boa per...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16421</th>\n",
       "      <td>b2w</td>\n",
       "      <td>c7c4904a68cd57abb0ff33bed6e5a6e0faa8c137f15d8c...</td>\n",
       "      <td>produto otimo _##_ deixa seus cabelos lisos e ...</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71452</th>\n",
       "      <td>b2w</td>\n",
       "      <td>c435fb3bd2c9eaf371dcb8d29fdd6fcede54f45427054e...</td>\n",
       "      <td>produto riscado e com avarias _##_ o guarda ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135326</th>\n",
       "      <td>b2w</td>\n",
       "      <td>9ab66ba65f4036d4bdbcaf7fc2cc883ec9a979f44b8dd8...</td>\n",
       "      <td>otimo produto _##_ celular de otima qualidade....</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>olist</td>\n",
       "      <td>b12d5e7fb052eca4b86fd61f42a73ac7</td>\n",
       "      <td>_##_ nao recebi o produto</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41653</th>\n",
       "      <td>b2w</td>\n",
       "      <td>81bebaa3396fe6f958752474e4fc486c0c288c2cea4830...</td>\n",
       "      <td>amei a comoda _##_ super indico,so deveriam te...</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137845 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                          review_id  \\\n",
       "20938     b2w  47dd8d461db193a7050331933268cc536925fa3fb3fc21...   \n",
       "112876    b2w  a922bcb60952b82664bada9c66c461603d43fce6e9a4fc...   \n",
       "30813   olist                   fa810fdb06c8ca30b4b596a6f5a13aa1   \n",
       "108072  olist                   a898722ed49b88ab4b45b35776ad1180   \n",
       "22060     b2w  a121941db8eedaa20b5e0e16e38ed9025d48e56f890018...   \n",
       "...       ...                                                ...   \n",
       "16421     b2w  c7c4904a68cd57abb0ff33bed6e5a6e0faa8c137f15d8c...   \n",
       "71452     b2w  c435fb3bd2c9eaf371dcb8d29fdd6fcede54f45427054e...   \n",
       "135326    b2w  9ab66ba65f4036d4bdbcaf7fc2cc883ec9a979f44b8dd8...   \n",
       "995     olist                   b12d5e7fb052eca4b86fd61f42a73ac7   \n",
       "41653     b2w  81bebaa3396fe6f958752474e4fc486c0c288c2cea4830...   \n",
       "\n",
       "                                                     text  label  split  \n",
       "20938   nao gostei _##_ quando faz duas xicaras transb...      0  train  \n",
       "112876  otimo aparelho _##_ nao e barulhenta... simple...      1  train  \n",
       "30813    _##_ meu produto n chegou entao n vou falar nada      0  train  \n",
       "108072  nota 10!!! _##_ otimo site! entrega no prazo! ...      1  train  \n",
       "22060   muito bom. _##_ excelente produto, com boa per...      1  train  \n",
       "...                                                   ...    ...    ...  \n",
       "16421   produto otimo _##_ deixa seus cabelos lisos e ...      1  valid  \n",
       "71452   produto riscado e com avarias _##_ o guarda ro...      0  valid  \n",
       "135326  otimo produto _##_ celular de otima qualidade....      1  valid  \n",
       "995                             _##_ nao recebi o produto      0  valid  \n",
       "41653   amei a comoda _##_ super indico,so deveriam te...      1  valid  \n",
       "\n",
       "[137845 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/dataset_reviews.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 5), (95891, 4), (41354, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 600 random samples from the training data where 'split' column is 'train'. The idea here is to simulate having only 600 labeled samples and lots of unlabeled samples.\n",
    "df_train_labeled = df.query('split == \"train\"').sample(600, random_state=271828)\n",
    "\n",
    "# Select the remaining training data and drop the 'label' column\n",
    "df_train_unlabeled = (\n",
    "    df.query('split == \"train\"').drop(df_train_labeled.index).drop(columns=[\"label\"])\n",
    ")\n",
    "\n",
    "# Select all data where 'split' column is not 'train' (validation data)\n",
    "df_valid = df.query('split != \"train\"')\n",
    "\n",
    "# Display the shapes of the labeled training data, unlabeled training data, and validation data\n",
    "df_train_labeled.shape, df_train_unlabeled.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the labeled training DataFrame to a Hugging Face Dataset\n",
    "dataset_train_labeled = datasets.Dataset.from_pandas(df_train_labeled)\n",
    "\n",
    "# Convert the unlabeled training DataFrame to a Hugging Face Dataset\n",
    "dataset_train_unlabeled = datasets.Dataset.from_pandas(df_train_unlabeled)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "dataset_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'review_id', 'text', 'label', 'split', '__index_level_0__'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8da3a23024d4e39a5894890851c8382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c0c66e2654497fb93161fda292142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer for Portuguese with specific settings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    use_fast=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "\n",
    "# Define a preprocessing function to tokenize the text data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the labeled training dataset\n",
    "dataset_train_tokenized_classification = dataset_train_labeled.map(\n",
    "    preprocess_function, batched=True\n",
    ")\n",
    "\n",
    "# Apply the preprocessing function to the validation dataset\n",
    "dataset_valid_tokenized_classification = dataset_valid.map(\n",
    "    preprocess_function, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator that dynamically pads the inputs to the maximum length in the batch\n",
    "# This ensures that all inputs in a batch have the same length, which is required for efficient processing\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Convert the prediction probabilities to predicted class indices\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute and return the accuracy metric using the predicted and true labels\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - Develop a Classifier from Scratch\n",
    "\n",
    "For simplicity, we'll use the same tokenizer from neuralmind/bert-base-portuguese-cased for all three approaches. This will ensure that the tokenization process is consistent across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the path where the language model will be saved\n",
    "path_to_save_lm = Path(\"./outputs/nlp_deep_learning/bert_masked_lm\")\n",
    "\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a non pretrained bert model\n",
    "\n",
    "# Import necessary libraries from the transformers package\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertConfig,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Define the configuration for the BERT model\n",
    "config = BertConfig(\n",
    "    attention_probs_dropout_prob=0.1,  # Dropout probability for the attention probabilities\n",
    "    directionality=\"bidi\",  # Model directionality\n",
    "    hidden_act=\"gelu\",  # Activation function to use in the hidden layers\n",
    "    hidden_dropout_prob=0.1,  # Dropout probability for the hidden layers\n",
    "    hidden_size=768,  # Size of the hidden layers\n",
    "    initializer_range=0.02,  # Range of the weight initializer\n",
    "    intermediate_size=3072,  # Size of the \"intermediate\" (i.e., feed-forward) layer\n",
    "    layer_norm_eps=1e-12,  # Epsilon for the layer normalization layers\n",
    "    max_position_embeddings=512,  # Maximum number of position embeddings to use\n",
    "    model_type=\"bert\",  # Type of the model\n",
    "    num_attention_heads=12,  # Number of attention heads\n",
    "    num_hidden_layers=12,  # Number of hidden layers\n",
    "    output_past=True,  # Whether or not to output the past hidden states\n",
    "    pad_token_id=0,  # The ID of the padding token\n",
    "    pooler_fc_size=768,  # Size of the pooling fully connected layer\n",
    "    pooler_num_attention_heads=12,  # Number of attention heads for the pooling layer\n",
    "    pooler_num_fc_layers=3,  # Number of fully connected layers in the pooling layer\n",
    "    pooler_size_per_head=128,  # Size per head in the pooling layer\n",
    "    pooler_type=\"first_token_transform\",  # Type of pooling to use\n",
    "    position_embedding_type=\"absolute\",  # Type of position embedding to use\n",
    "    type_vocab_size=2,  # Size of the type vocabulary\n",
    "    use_cache=True,  # Whether or not to use caching\n",
    "    vocab_size=29794,  # Size of the vocabulary\n",
    "    num_labels=2,  # Number of labels for the classification task\n",
    ")\n",
    "\n",
    "# Initialize a BERT model for sequence classification with the defined configuration\n",
    "# This is a randomly initialized model with a linear layer on top of the BERT model for classification\n",
    "model_random = BertForSequenceClassification(config)\n",
    "\n",
    "# Initialize all weights of the BERT model with a random normal distribution\n",
    "model_random.bert.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model name from the model checkpoint path\n",
    "# This will be used to name the output directory for the trained model\n",
    "model_name = \"bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 13:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.683252</td>\n",
       "      <td>0.623495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.619714</td>\n",
       "      <td>0.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>0.537847</td>\n",
       "      <td>0.745007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.522069</td>\n",
       "      <td>0.764448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.487131</td>\n",
       "      <td>0.773710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.6109645938873292, metrics={'train_runtime': 806.0846, 'train_samples_per_second': 3.722, 'train_steps_per_second': 0.062, 'total_flos': 369999921600000.0, 'train_loss': 0.6109645938873292, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args_random = TrainingArguments(\n",
    "    output_dir=path_to_save_lm\n",
    "    / f\"{model_name}-random_model\",  # Output directory for the trained model\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=32,  # Batch size for training. May need to be lowered for free GPUs\n",
    "    per_device_eval_batch_size=256,  # Batch size for evaluation. May need to be lowered for free GPUs\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    bf16=True,  # Use bf16 precision. May need to be changed to fp16 for free GPUs\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer_random = Trainer(\n",
    "    model=model_random,  # The model to train\n",
    "    args=training_args_random,  # The training arguments\n",
    "    train_dataset=dataset_train_tokenized_classification,  # The training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_classification,  # The evaluation dataset\n",
    "    processing_class=tokenizer,  # The tokenizer\n",
    "    data_collator=data_collator,  # The data collator\n",
    "    compute_metrics=compute_metrics,  # The function to compute the metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_random.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 02:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.48713117837905884,\n",
       " 'eval_accuracy': 0.7737099192339314,\n",
       " 'eval_runtime': 136.2978,\n",
       " 'eval_samples_per_second': 303.409,\n",
       " 'eval_steps_per_second': 0.594,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_random = trainer_random.evaluate()\n",
    "results_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 - Use a Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Set the random model to None to free up memory\n",
    "model_random = None\n",
    "\n",
    "# Set the trainer for the random model to None to free up memory\n",
    "trainer_random = None\n",
    "\n",
    "# Collect garbage to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes from the transformers library\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load the configuration for the pre-trained BERT model\n",
    "# This configuration includes model settings such as the number of hidden layers, attention heads, etc.\n",
    "config = AutoConfig.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Set the number of labels for the classification task (binary classification in this case)\n",
    "config.num_labels = 2\n",
    "\n",
    "# Initialize a pre-trained BERT model for sequence classification\n",
    "# This model includes a linear layer on top of the BERT model for classification tasks\n",
    "model_pretrained = BertForSequenceClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\", config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 13:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.542800</td>\n",
       "      <td>0.415471</td>\n",
       "      <td>0.880278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.261947</td>\n",
       "      <td>0.921604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.223440</td>\n",
       "      <td>0.927480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.192697</td>\n",
       "      <td>0.933404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.934565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.28772231817245486, metrics={'train_runtime': 785.4887, 'train_samples_per_second': 3.819, 'train_steps_per_second': 0.064, 'total_flos': 369999921600000.0, 'train_loss': 0.28772231817245486, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the pretrained model\n",
    "training_args_pretrained = TrainingArguments(\n",
    "    output_dir=path_to_save_lm\n",
    "    / f\"{model_name}-pretrained_model\",  # Output directory for the trained model\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=32,  # Batch size for training. May need to be lowered for free GPUs\n",
    "    per_device_eval_batch_size=256,  # Batch size for evaluation. May need to be lowered for free GPUs\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use bf16 precision. May need to be changed to fp16 for free GPUs\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for the pretrained model\n",
    "trainer_pretrained = Trainer(\n",
    "    model=model_pretrained,  # The pretrained model to train\n",
    "    args=training_args_pretrained,  # The training arguments\n",
    "    train_dataset=dataset_train_tokenized_classification,  # The training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_classification,  # The evaluation dataset\n",
    "    processing_class=tokenizer,  # The tokenizer\n",
    "    data_collator=data_collator,  # The data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # The function to compute the evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the pretrained model\n",
    "trainer_pretrained.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 02:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.18739964067935944,\n",
       " 'eval_accuracy': 0.9345649755767278,\n",
       " 'eval_runtime': 126.9486,\n",
       " 'eval_samples_per_second': 325.754,\n",
       " 'eval_steps_per_second': 0.638,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the pretrained model on the validation dataset\n",
    "# This will return a dictionary containing evaluation metrics such as loss and accuracy\n",
    "results_pretrained = trainer_pretrained.evaluate()\n",
    "\n",
    "# Display the evaluation results\n",
    "results_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3 - Use a Pretrained Language Model and Fine-tune it on Domain-Specific Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fine-tune the Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the pretrained model to None to free up memory\n",
    "model_pretrained = None\n",
    "\n",
    "# Set the trainer for the pretrained model to None to free up memory\n",
    "trainer_pretrained = None\n",
    "\n",
    "# Collect garbage to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset from a Parquet file, selecting only the 'text' column. This data is unlabeled. Our task will be self-supervised learning (MLM)\n",
    "df_unlabeled = pd.read_parquet(\"data/dataset_reviews.parquet\", columns=[\"text\"])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "# Use 10% of the data for validation and the remaining 90% for training\n",
    "# Set a random seed for reproducibility\n",
    "df_unlabeled_train, df_unlabeled_valid = train_test_split(\n",
    "    df_unlabeled, test_size=0.10, random_state=271828\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the unlabeled training DataFrame to a Hugging Face Dataset\n",
    "dataset_unlabeled_train = datasets.Dataset.from_pandas(df_unlabeled_train)\n",
    "\n",
    "# Convert the unlabeled validation DataFrame to a Hugging Face Dataset\n",
    "dataset_unlabeled_valid = datasets.Dataset.from_pandas(df_unlabeled_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 124060\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 13785\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the path where the language model will be saved\n",
    "path_to_save_lm = Path(\"./outputs/nlp_deep_learning/bert_masked_lm\")\n",
    "\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer for Portuguese without truncation\n",
    "# This tokenizer will not truncate the input text, which means it will keep the full length of the text\n",
    "# The 'use_fast=True' parameter enables the fast version of the tokenizer for better performance\n",
    "tokenizer_no_truncation = AutoTokenizer.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\", use_fast=True, truncation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ede8ca9cdd4db7a00d4bd514e9ceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d937bff68bb242a6ad985f089caefb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function_no_truncation(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text in the given examples using the tokenizer object.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the tokenized input text.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text without truncation\n",
    "    result = tokenizer_no_truncation(examples[\"text\"])\n",
    "\n",
    "    # If using the fast tokenizer, also include word IDs for each token\n",
    "    if tokenizer_no_truncation.is_fast:\n",
    "        result[\"word_ids\"] = [\n",
    "            result.word_ids(i) for i in range(len(result[\"input_ids\"]))\n",
    "        ]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Tokenize the unlabeled training dataset\n",
    "# This step converts the text data into numerical representations (tokens) that the model can process\n",
    "# The 'batched=True' parameter processes the data in batches for efficiency\n",
    "# The 'remove_columns' parameter removes the original text and index columns from the dataset\n",
    "dataset_train_tokenized_mlm = dataset_unlabeled_train.map(\n",
    "    tokenize_function_no_truncation,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"__index_level_0__\"],\n",
    ")\n",
    "\n",
    "# Tokenize the unlabeled validation dataset\n",
    "# Similar to the training dataset, this step converts the text data into tokens\n",
    "dataset_valid_tokenized_mlm = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_no_truncation,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"__index_level_0__\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 124060\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_tokenized_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feafcaa880d64324b181a30ab1ad379a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e31a57702424888b0f3cc9013971444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    This function groups together a set of texts as contiguous text of fixed length (chunk_size).\n",
    "    It's useful for training masked language models.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the examples to group. Each key corresponds to a feature,\n",
    "      and each value is a list of lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the grouped examples. Each key corresponds to a feature,\n",
    "      and each value is a list of lists of tokens.\n",
    "    \"\"\"\n",
    "    # Concatenate all texts into a single list for each feature\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "\n",
    "    # Compute the total length of the concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Adjust the total length to be a multiple of chunk_size, dropping the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "\n",
    "    # Split the concatenated texts into chunks of size chunk_size\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create a new 'labels' column that is a copy of the 'input_ids' column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Define the chunk size for grouping texts\n",
    "chunk_size = 512\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset\n",
    "# This step groups the tokenized texts into chunks of size chunk_size\n",
    "dataset_train_tokenized_mlm = dataset_train_tokenized_mlm.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset\n",
    "# This step groups the tokenized texts into chunks of size chunk_size\n",
    "dataset_valid_tokenized_mlm = dataset_valid_tokenized_mlm.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM)\n",
    "# This collator will dynamically mask tokens in the input text with a probability of 15%\n",
    "# The masked tokens will be replaced with a special [MASK] token, which the model will try to predict during training\n",
    "data_collator_mlm = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_no_truncation,  # The tokenizer used to process the input text\n",
    "    mlm_probability=0.15,  # The probability of masking a token in the input text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Define the model checkpoint for the pre-trained BERT model\n",
    "# This checkpoint corresponds to a BERT model pre-trained on Portuguese text\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "# Load the pre-trained BERT model for masked language modeling (MLM)\n",
    "# This model will be used to predict masked tokens in the input text\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation\n",
    "batch_size = 32\n",
    "\n",
    "# Extract the model name from the model checkpoint path\n",
    "# This will be used to name the output directory for the trained model\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "# Define the training arguments for the masked language model (MLM)\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=path_to_save_lm\n",
    "    / f\"{model_name}-finetuned-mlm\",  # Output directory for the trained model\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it already exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size,  # Batch size for training. May need to be lowered for free GPUs\n",
    "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation. May need to be lowered for free GPUs\n",
    "    bf16=True,  # Use bf16 precision. May need to be changed to fp16 for free GPUs\n",
    "    num_train_epochs=20,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints and delete the older ones\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use the evaluation loss to determine the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=2,  # Number of steps to accumulate gradients before updating the model parameters\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the masked language model (MLM)\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,  # The pre-trained BERT model for masked language modeling\n",
    "    args=training_args_mlm,  # The training arguments defined earlier\n",
    "    train_dataset=dataset_train_tokenized_mlm,  # The tokenized training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_mlm,  # The tokenized validation dataset\n",
    "    data_collator=data_collator_mlm,  # The data collator for dynamic masking during training\n",
    "    processing_class=tokenizer_no_truncation,  # The tokenizer used to process the input text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 1:05:49, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.348200</td>\n",
       "      <td>1.253211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.234200</td>\n",
       "      <td>1.160850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.168500</td>\n",
       "      <td>1.113159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.129800</td>\n",
       "      <td>1.088677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.101300</td>\n",
       "      <td>1.074122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.081100</td>\n",
       "      <td>1.038989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.065700</td>\n",
       "      <td>1.043078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.068500</td>\n",
       "      <td>1.040189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.016487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.053100</td>\n",
       "      <td>1.039519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.039800</td>\n",
       "      <td>1.042215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=1.1465044491820866, metrics={'train_runtime': 3951.4596, 'train_samples_per_second': 46.697, 'train_steps_per_second': 0.364, 'total_flos': 4.82434530357289e+16, 'train_loss': 1.1465044491820866, 'epoch': 19.862068965517242})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm/tokenizer_config.json',\n",
       " 'outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm/special_tokens_map.json',\n",
       " 'outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm/vocab.txt',\n",
       " 'outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm/added_tokens.json',\n",
       " 'outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer_mlm.save_model(path_to_save_lm / f\"{model_name}-finetuned-mlm\")\n",
    "tokenizer_no_truncation.save_pretrained(path_to_save_lm / f\"{model_name}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm / f\"{model_name}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a Classifier with Fine-tuned Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pretrained model to None to free up memory\n",
    "model_pretrained = None\n",
    "\n",
    "# Set the trainer for the pretrained model to None to free up memory\n",
    "trainer_pretrained = None\n",
    "\n",
    "# Set the masked language model (MLM) to None to free up memory\n",
    "model_mlm = None\n",
    "\n",
    "# Set the trainer for the masked language model (MLM) to None to free up memory\n",
    "trainer_mlm = None\n",
    "\n",
    "# Set the tokenizer to None to free up memory\n",
    "tokenizer = None\n",
    "\n",
    "# Collect garbage to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoConfig, AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the configuration for the pre-trained BERT model\n",
    "# This configuration is loaded from the directory where the fine-tuned masked language model (MLM) is saved\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"./outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Set the number of labels for the classification task\n",
    "# In this case, we are setting it to 2 for binary classification\n",
    "config.num_labels = 2\n",
    "\n",
    "# Initialize a BERT model for sequence classification using the fine-tuned MLM model\n",
    "# This model will have a linear layer on top of the BERT model for classification\n",
    "model_ft = BertForSequenceClassification.from_pretrained(\n",
    "    \"./outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned MLM model\n",
    "# The tokenizer will be used to preprocess the input text for the BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./outputs/nlp_deep_learning/bert_masked_lm/bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    use_fast=True,  # Use the fast version of the tokenizer for better performance\n",
    "    truncation=True,  # Enable truncation to ensure the input text does not exceed the maximum length\n",
    "    padding=True,  # Enable padding to ensure the input text is of uniform length\n",
    "    max_length=512,  # Set the maximum length for the input text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/50 00:03 < 00:16, 2.41 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/81 00:52 < 00:54, 0.75 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.24602191686630248, metrics={'train_runtime': 730.0486, 'train_samples_per_second': 4.109, 'train_steps_per_second': 0.068, 'total_flos': 369999921600000.0, 'train_loss': 0.24602191686630248, 'epoch': 5.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the fine-tuned model\n",
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=path_to_save_lm\n",
    "    / f\"{model_name}-ft_model\",  # Output directory for the trained model\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=32,  # Batch size for training. May need to be lowered for free GPUs\n",
    "    per_device_eval_batch_size=256,  # Batch size for evaluation. May need to be lowered for free GPUs\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use bf16 precision. May need to be changed to fp16 for free GPUs\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for the fine-tuned model\n",
    "trainer_ft = Trainer(\n",
    "    model=model_ft,  # The fine-tuned model to train\n",
    "    args=training_args_ft,  # The training arguments defined above\n",
    "    train_dataset=dataset_train_tokenized_classification,  # The tokenized training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_classification,  # The tokenized evaluation dataset\n",
    "    processing_class=tokenizer,  # The tokenizer used to preprocess the input text\n",
    "    data_collator=data_collator,  # The data collator for dynamic padding and batching\n",
    "    compute_metrics=compute_metrics,  # The function to compute the evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the fine-tuned model\n",
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/81 00:58 < 00:55, 0.70 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15792906284332275,\n",
       " 'eval_accuracy': 0.9431977559607293,\n",
       " 'eval_runtime': 122.4647,\n",
       " 'eval_samples_per_second': 337.681,\n",
       " 'eval_steps_per_second': 0.661,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the evaluation dataset\n",
    "# This will return a dictionary containing the evaluation metrics\n",
    "results_ft = trainer_ft.evaluate()\n",
    "\n",
    "# Display the evaluation results\n",
    "# The results will include metrics such as loss, accuracy, etc.\n",
    "results_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.487131</td>\n",
       "      <td>0.773710</td>\n",
       "      <td>136.2978</td>\n",
       "      <td>303.409</td>\n",
       "      <td>0.594</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained</th>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.934565</td>\n",
       "      <td>126.9486</td>\n",
       "      <td>325.754</td>\n",
       "      <td>0.638</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finetuned</th>\n",
       "      <td>0.157929</td>\n",
       "      <td>0.943198</td>\n",
       "      <td>122.4647</td>\n",
       "      <td>337.681</td>\n",
       "      <td>0.661</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "random       0.487131       0.773710      136.2978                  303.409   \n",
       "pretrained   0.187400       0.934565      126.9486                  325.754   \n",
       "finetuned    0.157929       0.943198      122.4647                  337.681   \n",
       "\n",
       "            eval_steps_per_second  epoch  \n",
       "random                      0.594    5.0  \n",
       "pretrained                  0.638    5.0  \n",
       "finetuned                   0.661    5.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to store the evaluation results of different models\n",
    "# The DataFrame will have three rows, each corresponding to a different model:\n",
    "# - 'random': A model with random weights (trained from sctach)\n",
    "# - 'pretrained': A pre-trained model without fine-tuning\n",
    "# - 'finetuned': A pre-trained model that has been fine-tuned on a specific task\n",
    "# The columns of the DataFrame will contain the evaluation metrics for each model\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    [\n",
    "        results_random,\n",
    "        results_pretrained,\n",
    "        results_ft,\n",
    "    ],  # List of dictionaries containing the evaluation results\n",
    "    index=[\"random\", \"pretrained\", \"finetuned\"],  # Index labels for the rows\n",
    ")\n",
    "\n",
    "# Display the DataFrame with the evaluation results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "\n",
    "The finetuned model from step 3 demonstrates the best overall performance, achieving the lowest evaluation loss and highest evaluation accuracy among the three models. This indicates that the finetuning process effectively adapts the pretrained model to the specific task at hand, employing the knowledge gained from pretraining on a large corpus of text.\n",
    "\n",
    "The pretrained model from step 2 also exhibits strong performance, albeit slightly inferior to the finetuned model. Its ability to perform well can be attributed to the extensive pretraining on a vast amount of text data, which provides a sturdy foundation for understanding language patterns and semantics. However, without the task-specific finetuning, it may not fully capture the nuances and characteristics of the downstream task.\n",
    "\n",
    "On the other hand, the random model yields the poorest performance across all evaluation metrics. This is expected, as the random model lacks any prior knowledge or training on language understanding tasks. It serves as a baseline to highlight the significance of pretraining and finetuning in improving model performance.\n",
    "\n",
    "These results underscore the power and effectiveness of transfer learning in natural language processing (NLP) tasks, particularly when labeled data is limited. Transfer learning allows models to utilize the knowledge acquired from pretraining on large, diverse datasets and apply it to specific downstream tasks. By finetuning the pretrained model on domain-specific text, the model can further adapt and specialize its understanding to the target task.\n",
    "\n",
    "It is important to note that the cost of labeling data (600 instances) was the same across all models. However, the performance outcomes varied significantly. This emphasizes the value of transfer learning in improving model performance while minimizing the need for extensive labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is transfer learning and how does it benefit NLP tasks?\n",
    "\n",
    "2. Explain the role of tokenization in NLP and how it helps models understand the semantic meaning of text.\n",
    "\n",
    "3. What is language modeling and how does it enable models to learn from unlabeled data?\n",
    "\n",
    "4. Describe the ULMFiT approach and its three main steps for transfer learning in NLP.\n",
    "\n",
    "5. How does Whole Word Masking differ from traditional masking techniques and what advantages does it offer?\n",
    "\n",
    "6. Compare and contrast training a classifier from scratch, using a pre-trained language model, and fine-tuning a pre-trained language model on domain-specific text. Which approach would likely yield the best performance and why?\n",
    "\n",
    "7. In the Americanas scenario, why is it beneficial to fine-tune the pre-trained language model on the unlabeled reviews before training the classifier?\n",
    "\n",
    "8. What are some key considerations and precautions to keep in mind when applying transfer learning to NLP tasks?\n",
    "\n",
    "9. How does self-supervised learning, such as language modeling, enable models to learn valuable information about language structure and semantics without requiring labeled data?\n",
    "\n",
    "10. Based on the comparison of the three model approaches (random, pre-trained, fine-tuned), what conclusions can you draw about the effectiveness of transfer learning and domain-specific fine-tuning for sentiment analysis tasks with limited labeled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "\n",
    "\n",
    "<!-- 1. Transfer learning is a powerful technique in NLP that leverages pre-trained models as a starting point for training models on related tasks. By utilizing the knowledge gained from large-scale pre-training on vast amounts of text data, transfer learning significantly reduces the training time and data requirements for new tasks. This is particularly beneficial in NLP because pre-trained language models capture detailed patterns and relationships in language, which can be effectively transferred to downstream tasks.\n",
    "\n",
    "2. Tokenization is a fundamental step in NLP that involves breaking down text into meaningful semantic units called tokens. It is crucial because it enables models to understand and process the base units of language. Subword tokenization is a specific technique that splits words into smaller, more manageable units. This approach allows models to handle languages with large vocabularies, complex morphologies, and even unseen words by recognizing the building blocks of words, leading to improved understanding of the semantic meaning of each sentence.\n",
    "\n",
    "3. Language modeling is a self-supervised learning task that aims to capture the basic structure and patterns of language. It involves training a model to predict the next word or masked words in a sequence of text. By learning to predict the next word, the model gains insights into grammar, semantics, and even world knowledge. Language modeling helps in understanding the elaborate relationships between words, syntax, and sentence structure, which is essential for various NLP tasks such as part-of-speech tagging, named entity recognition, sentiment analysis, and machine translation.\n",
    "\n",
    "4. ULMFiT (Universal Language Model Fine-tuning) is a transfer learning approach that revolutionized the field of NLP. It involves pre-training a language model on a large corpus of unlabeled text, fine-tuning it on domain-specific text, and then using the fine-tuned model for downstream NLP tasks. ULMFiT improves the process of transfer learning by adapting the pre-trained model to the specific characteristics and nuances of the target domain. This fine-tuning step allows the model to capture domain-specific features and patterns, leading to improved performance on the target task, even with limited labeled data.\n",
    "\n",
    "5. Whole Word Masking is an advanced masking technique used in language modeling that masks entire words instead of individual subwords or tokens. Unlike traditional token masking, where random subwords are masked, Whole Word Masking ensures that all the subwords corresponding to a word are masked together. This approach forces the model to understand the contextual meaning of the masked word as a whole, rather than relying on individual subwords. Whole Word Masking improves the model's ability to capture the semantic relationships between words and leads to better performance on downstream tasks.\n",
    "\n",
    "6. Fine-tuning a pre-trained language model on domain-specific text is a crucial step in transfer learning for NLP tasks. By exposing the pre-trained model to text from the target domain, the model learns the specific vocabulary, writing style, and linguistic patterns of that domain. This fine-tuning process allows the model to adapt its knowledge to the characteristics of the target task, capturing domain-specific features that may not be present in the general pre-trained model. As a result, the fine-tuned model exhibits improved performance on the target task compared to using the pre-trained model directly.\n",
    "\n",
    "7. Transfer learning offers several key benefits when dealing with limited labeled data in NLP tasks. Firstly, it enables the utilization of large-scale pre-trained models that have learned rich representations of language from vast amounts of unlabeled text. These pre-trained models can be fine-tuned on the target task with a relatively small labeled dataset, as they have already captured general language patterns. Secondly, transfer learning reduces the need for extensive labeled data by employing the knowledge transferred from the pre-training phase. This is particularly advantageous in domains where labeled data is scarce or expensive to obtain.\n",
    "\n",
    "8. Self-supervised learning is a powerful approach in NLP that allows models to learn from unlabeled data by solving pretext tasks. regarding NLP, language modeling is a common self-supervised learning task. The process involves training a model to predict the next word or masked words in a sequence of text. By learning to solve this pretext task, the model captures valuable information about the structure and semantics of language without requiring explicit labels. Self-supervised learning enables models to learn from vast amounts of unlabeled text data, which is abundantly available. This approach reduces the reliance on labeled data and allows models to develop a deep understanding of language that can be transferred to various downstream NLP tasks.\n",
    "\n",
    "9. In the practical example provided, three different approaches are used to build a sentiment analysis model. The first approach involves developing a classifier from scratch, without using any pre-trained models. This serves as a baseline to understand the importance of transfer learning. The second approach utilizes a pre-trained language model (BERTimbau) and trains a classifier on top of it using the limited labeled dataset. This approach leverages the knowledge captured by the pre-trained model to improve performance. The third approach goes a step further by fine-tuning the pre-trained language model on domain-specific text (unlabeled product reviews) before training the classifier. This fine-tuning step adapts the model to the specific characteristics of the target domain, leading to further improvements in performance.\n",
    "\n",
    "10. In the given example, the performance of the three approaches is compared using a holdout dataset. The model trained from scratch, without utilizing any pre-trained knowledge, serves as a baseline. The pre-trained model (BERTimbau) outperforms the baseline model, even with limited labeled data, due to the extensive knowledge it has acquired during pre-training on large amounts of text. However, the fine-tuned pre-trained model, which is adapted to the specific domain of product reviews, achieves the best performance among the three approaches. This demonstrates the effectiveness of transfer learning and the importance of fine-tuning on domain-specific text to capture task-specific nuances and improve model performance. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imd1107-nlp-1oxVnwDa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
