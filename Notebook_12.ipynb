{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "## IMD1107 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](htttps://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "\n",
    "- Large Language Models (LLMs) are advanced AI systems trained on vast text datasets to understand and generate human-like text, revolutionizing human-machine interactions across various industries.\n",
    "\n",
    "- LLMs learn language nuances like grammar, syntax, and semantics through deep learning techniques and billions of parameters, enabling them to generate text that closely resembles human language.\n",
    "\n",
    "- The evolution of LLMs includes milestones such as the Transformer architecture, GPT, BERT, and GPT-3, each advancing the field in terms of performance, efficiency, and scale.\n",
    "\n",
    "- Foundation models are trained on large, diverse datasets and acquire general knowledge through self-supervised learning, making them versatile and capable of performing a wide array of tasks with minimal fine-tuning.\n",
    "\n",
    "- Prompt engineering is crucial for guiding LLMs to produce accurate and contextually appropriate responses by designing and refining text inputs.\n",
    "\n",
    "- LangChain and LlamaIndex are platforms that simplify interaction with LLMs, offering features like continuous assimilation, instinctive APIs, task-specific modules, and indexing capabilities for large document collections.\n",
    "\n",
    "- LLMs are stateless, meaning they do not maintain context across interactions, requiring developers to explicitly manage conversation flow and message history to ensure coherence.\n",
    "\n",
    "- Techniques like prompt engineering, external memory systems, stateful wrappers, and message history classes help mitigate the challenges posed by the stateless nature of LLMs.\n",
    "\n",
    "- Enhancing LLM performance involves strategies such as using prompt templates, few-shot and zero-shot learning, chain-of-thought prompting, role assignment, and interactive prompts with feedback loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this class, students will be able to:\n",
    "\n",
    "1) **Explain the fundamentals of Large Language Models (LLMs)** by describing their architecture, training approaches, and key milestones in their evolution, including the role of the Transformer model.\n",
    "\n",
    "2) **Analyze the differences between foundation models and traditional AI models** and explain how self-supervised learning enables LLMs to perform versatile language tasks with minimal fine-tuning.\n",
    "\n",
    "3) **Apply prompt engineering techniques** by constructing clear, context-rich prompts (including zero-shot, few-shot, chain-of-thought, and role assignment strategies) to guide LLM outputs toward specific, measurable goals.\n",
    "\n",
    "4) **Utilize integration platforms like LangChain and LlamaIndex** by setting up and executing runnable chains that connect models with prompt templates, output parsers, and message history managers to build coherent, context-aware AI applications.\n",
    "\n",
    "5) **Manage conversation flow in stateless LLMs** by implementing message history strategies and stateful wrappers to maintain context across multiple interactions, ensuring coherent and personalized dialogue in applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models\n",
    "\n",
    "Large Language Models (LLMs) represent a significant advancement in artificial intelligence (AI) and natural language processing (NLP). By being trained on extensive text datasets, these models can generate human-like text, understand context, and perform various language-related tasks with exceptional proficiency. The emergence of LLMs has introduced new opportunities in AI, potentially transforming human-machine interactions.\n",
    "\n",
    "\n",
    "## Why Large Language Models Matter\n",
    "\n",
    "LLMs have the potential to make a significant impact in many areas. Key points include:\n",
    "\n",
    "1. **Humanlike Language Understanding:**  \n",
    "   - LLMs generate and comprehend text in a manner that resembles human communication.  \n",
    "   - They support applications such as chatbots and virtual assistants, promoting more natural interactions.\n",
    "\n",
    "2. **Versatility:**  \n",
    "   - A single model can handle translation, summarization, question answering, and content generation with minimal adjustments.\n",
    "   - This general-purpose ability is beneficial to both research and industry settings.\n",
    "\n",
    "3. **Knowledge Acquisition:**  \n",
    "   - By being exposed to billions of words during training, these models learn grammar, syntax, and semantics.\n",
    "   - Their acquired knowledge is useful when providing responses with high accuracy and relevance.\n",
    "\n",
    "4. **Efficiency and Scalability:**  \n",
    "   - LLMs operate quickly and can serve millions of users simultaneously.  \n",
    "   - Their capacity to process large volumes of text enables cost-effective solutions for large-scale language tasks.\n",
    "\n",
    "5. **Innovation and Creativity:**  \n",
    "   - They support creative processes such as writing, brainstorming new ideas, and solving complex problems.\n",
    "   - Their outputs can help stimulate research, education, and the arts.\n",
    "\n",
    "6. **Industry Applications:**  \n",
    "   - Sectors like customer service, healthcare, finance, and marketing benefit from improved decision-making and task automation.\n",
    "\n",
    "\n",
    "## Formal Definition and Fundamental Concepts\n",
    "\n",
    "LLMs are AI systems that employ deep learning techniques to process and generate language. These systems are characterized by a large number of parameters—often in the billions—which help capture the complex statistical patterns in language. In mathematical terms, these models enhance a function:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg \\min_{\\theta} \\mathcal{L}\\left( f(X; \\theta), Y \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ represents model parameters,\n",
    "- $X$ is the input text,\n",
    "- $Y$ is the target output,\n",
    "- $\\mathcal{L}$ is a loss function measuring the difference between the model’s output and the expected result.\n",
    "\n",
    "The high parameter count enables these models to adapt to various language tasks with minimal further training.\n",
    "\n",
    "\n",
    "## History and Evolution\n",
    "\n",
    "The development of LLMs has been a progressive journey, with each iteration improving upon previous models. Key milestones include:\n",
    "\n",
    "- **Transformer Architecture (2017):** Introduced by Vaswani et al., this architecture revolutionized NLP by allowing models to process input sequences in parallel, improving training speed and performance.\n",
    "- **GPT (Generative Pre-trained Transformer) (2018):** Developed by OpenAI, GPT was one of the first large-scale models to demonstrate the potential of unsupervised pre-training on diverse text data.\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers) (2018):** Google's BERT model introduced bidirectional training, enabling the model to learn from both left and right contexts simultaneously, enhancing language understanding.\n",
    "- **GPT-3 (2020):** OpenAI's GPT-3, with 175 billion parameters, showcased the power of scaling up language models, demonstrating remarkable language generation capabilities and the ability to perform tasks with minimal fine-tuning.\n",
    "\n",
    "The evolution continues with even larger and more powerful models being developed, and anything I try to write here will become obsolete way too fast for me to keep up with the pace of the field in a jupyter notebook.\n",
    "\n",
    "## The Impact of Large Language Models\n",
    "Large Language Models have significantly impacted various industries and applications. Some key areas where LLMs are making a difference include:\n",
    "\n",
    "LLMs have created opportunities and challenges across various sectors:\n",
    "- **Natural Language Processing:**  \n",
    "  LLMs have improved translation, summarization, and sentiment analysis.\n",
    "- **Content Creation:**  \n",
    "  They automate drafting and editing tasks in journalism and marketing.\n",
    "- **Customer Service:**  \n",
    "  Their combination into chatbots and virtual assistants raises the standard for user interactions.\n",
    "- **Healthcare:**  \n",
    "  They assist in medical research, diagnosis explanations, and patient communication.\n",
    "- **Education:**  \n",
    "  LLMs support personalized tutoring systems and adaptive learning environments.\n",
    "\n",
    "## Challenges and Limitations of Large Language Models\n",
    "While LLMs have shown remarkable capabilities, they also face several challenges and limitations:\n",
    "\n",
    "- **Bias and Fairness:**  \n",
    "  Models may reflect or increase biases from their training data.\n",
    "- **Hallucination:**  \n",
    "  They can produce text that sounds plausible but is factually inaccurate.\n",
    "- **Common Sense Reasoning:**  \n",
    "  These models may struggle with context-dependent or subtle reasoning tasks.\n",
    "- **Computational Demand:**  \n",
    "  Training and real-time applications can require significant hardware resources.\n",
    "- **Privacy Concerns:**  \n",
    "  The use of large unfiltered datasets may unintentionally expose sensitive information.\n",
    "\n",
    "> **Note:** The emergence of Large Language Models is a significant milestone in AI and NLP. Their ability to understand and generate human-like text has the potential to transform human-machine interactions and change radically various industries. As research progresses, LLMs are expected to become even more powerful and versatile, pioneering of AI and language capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation Models\n",
    "\n",
    "Foundation models represent a class of AI models trained on extensive and diverse datasets, enabling them to perform a wide array of tasks with minimal fine-tuning. Unlike traditional models, which are trained on specific tasks using labeled data, foundation models acquire general knowledge and patterns from unlabeled data through self-supervised learning.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/fms.png\" alt=\"\" style=\"width: 80%; height: 80%\"/>\n",
    "</p>\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The architecture of foundation models is grounded in the transformer model, which has significantly impacted natural language processing (NLP) and other domains. Key components of the transformer architecture include:\n",
    "\n",
    "- **Attention Mechanisms:** These mechanisms allow the model to focus on relevant parts of the input when processing each element, enabling it to capture long-range dependencies and context.\n",
    "- **Encoders:** The encoder layers process the input sequence and generate hidden representations that capture the meaning and context of each element.\n",
    "- **Decoders:** The decoder layers take the encoded representations and generate the output sequence, attending to relevant parts of the input.\n",
    "\n",
    "The transformer architecture enables foundation models to process and generate sequential data efficiently, making them suitable for many tasks.\n",
    " \n",
    "## Training Data\n",
    "\n",
    "A critical factor in the success of foundation models is the utilization of large-scale datasets. These models are trained on massive amounts of diverse data, often spanning multiple domains and modalities. The training data can include:\n",
    "\n",
    "- Unstructured text from web pages, books, and articles\n",
    "- Structured data from databases and knowledge bases\n",
    "- Images, videos, and audio data\n",
    "- Code snippets and programming language data\n",
    "\n",
    "By training on such diverse data, foundation models can capture a broad range of knowledge and develop a deep understanding of language, concepts, and relationships.\n",
    "\n",
    "## Pre-training\n",
    "\n",
    "Foundation models undergo an unsupervised pre-training phase, where they learn general knowledge and patterns from the training data without explicit supervision. During pre-training, the models are typically trained on tasks such as:\n",
    "\n",
    "- **Language Modeling:** Predicting the next word or token in a sequence.\n",
    "- **Masked Language Modeling:** Predicting masked or hidden tokens in a sequence.\n",
    "- **Contrastive Learning:** Learning to distinguish positive examples from negative ones.\n",
    "\n",
    "Through pre-training, foundation models develop a rich understanding of language and can capture complex relationships and patterns in the data.\n",
    "\n",
    "## Reinforcement Learning in ChatGPT and Similar Models\n",
    "\n",
    "In addition to pre-training, models like ChatGPT often undergo a fine-tuning phase that includes reinforcement learning to enhance their performance and alignment with user expectations. This phase typically involves the following steps:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/rlhf.jpg\" alt=\"\" style=\"width: 80%; height: 80%\"/>\n",
    "</p>\n",
    "\n",
    "1. **Supervised Fine-Tuning:** The model is first fine-tuned on a dataset curated by human annotators. This dataset includes examples of desired behavior, guiding the model towards generating more appropriate and relevant responses.\n",
    "\n",
    "2. **Reinforcement Learning from Human Feedback (RLHF):** This further refines the model's behavior by incorporating feedback from human users. The process involves:\n",
    "    - **Collecting Feedback:** Human users interact with the model and provide feedback on its responses, indicating preferences or highlighting issues.\n",
    "    - **Training a Reward Model:** A separate model is trained to predict the quality of responses based on the collected feedback. This reward model assigns scores to the generated responses.\n",
    "    - **Enhancing the Policy:** The main model is then fine-tuned using reinforcement learning techniques, enhancing its responses to maximize the reward scores predicted by the reward model.\n",
    "\n",
    "Through RLHF, models like ChatGPT can better align with human preferences, generating more useful, accurate, and contextually appropriate responses.\n",
    "\n",
    "## Versatility and Adaptation\n",
    "\n",
    "Foundation models are adaptable to a broad range of tasks, but their general design may not be optimal for every application. Adaptation strategies include:\n",
    "\n",
    "- **Prompting:**  \n",
    "  Providing instructions or examples to guide the model during inference. This method is efficient but may have limitations in performance.\n",
    "  \n",
    "- **In-Context Learning:**  \n",
    "  Feeding a few examples directly into the input prompt to help the model perform a particular task.\n",
    "  \n",
    "- **Fine-Tuning:**  \n",
    "  Updating all model parameters using task-specific data. This approach typically yields the best performance but requires more resources.\n",
    "  \n",
    "- **Low-Rank Adaptation (LoRA):**  \n",
    "  Adjusting only a subset of the model parameters (often represented in a low-dimensional space) to save on computational costs while still achieving improved performance.\n",
    "\n",
    "When adapting foundation models, consider:\n",
    "- **Compute Budget:**  \n",
    "  Models with billions or trillions of parameters may be challenging to adapt fully; selective tuning (for example, just the last layer or bias terms) can offer a practical balance.\n",
    "  \n",
    "- **Data Availability:**  \n",
    "  Domain-specific tasks may require manually labeled datasets. In cases with limited data, in-context learning might be more fitting.\n",
    "\n",
    "\n",
    "> **Note:** The versatility of foundation models stems from their ability to generalize across different types of data and tasks, making them a cornerstone of modern AI research and application.\n",
    "> By adhering to these principles, foundation models continue to push the boundaries of what is possible in AI, offering stable solutions across various domains and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Foundation Models\n",
    "\n",
    "### Pretrained Language Models (PLMs)\n",
    "\n",
    "Pretrained Language Models (PLMs) are neural networks that undergo extensive pretraining on vast amounts of publicly available text data, such as web pages, books, and articles. The primary goal of this pretraining process is to enable the model to learn and understand the elaborate semantics and nuances of natural language. Some notable early examples of PLMs include **ELMo**, **BERT**, and **RoBERTA**. These models are trained using objectives like predicting masked words within a given text, allowing them to develop a deep understanding of language patterns and relationships. PLMs typically consist of hundreds of millions of parameters, which gives them the capacity to capture complex linguistic knowledge.\n",
    "\n",
    "#### Adapting PLMs to Downstream Tasks\n",
    "Once pretrained, PLMs can be adapted to perform specific downstream tasks through the following methods:\n",
    "\n",
    "- **Task-Specific Prediction Layer**: A task-specific prediction layer, such as a classification or regression layer, is added on top of the pretrained model. This allows the model to utilize its learned language understanding to make predictions tailored to the specific task at hand.\n",
    "\n",
    "- **Task-Specific Fine-Tuning**: During the fine-tuning process, all the weights of the pretrained model are updated using task-specific training data. This fine-tuning step helps the model to further specialize its knowledge and skills for the particular downstream application, refining its performance on that task.\n",
    "\n",
    "By adapting PLMs to downstream tasks, we can utilize the power of their extensive pretraining and apply it to numerous natural language processing applications, such as sentiment analysis, named entity recognition, and question answering.\n",
    "\n",
    "### Large Autoregressive Language Models\n",
    "\n",
    "Autoregressive models such as **GPT-4** and **Claude** differ from other PLMs by predicting the next word in a sequence. Their training objective is formulated by maximizing the likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t \\mid w_{1}, w_{2}, \\ldots, w_{t-1})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_t$ represents a token in a sequence,\n",
    "- $P(w_t \\mid \\cdot)$ is the predicted probability for that token.\n",
    "\n",
    "These models are capable of generating high-coherence text and solving a variety of tasks using either **few-shot prompting**—where a few examples are given—or **zero-shot prompting**—where only a task description is provided.\n",
    "\n",
    "\n",
    "### Arising Behaviors and Few-Shot Learning\n",
    "\n",
    "One of the most fascinating aspects of large autoregressive models is their ability to solve various natural language tasks with minimal examples or even just a task description. This capability is known as **few-shot learning** or **in-context learning**.\n",
    "\n",
    "- **Few-Shot Prompting**: In this approach, the model is provided with a small number of examples (typically fewer than 10) demonstrating the desired task. The model then leverages its extensive pretraining to \"locate\" the relevant knowledge and skills needed to perform the task, without requiring any updates to its parameters.\n",
    "\n",
    "- **Zero-Shot Prompting**: In some cases, these models have shown the ability to perform tasks with just a task description, without any examples at all. For instance, the model can translate between languages or perform arithmetic operations simply by being prompted with a natural language instruction.\n",
    "\n",
    "The emergence of few-shot and zero-shot learning capabilities in large autoregressive models has opened up new possibilities for natural language processing. These models can adapt to numerous tasks that differ significantly from their pretraining objectives, showcasing a level of generalization and versatility that was previously unattainable.\n",
    "\n",
    "### The Role of Smaller Models and Fine-Tuning\n",
    "\n",
    "While large autoregressive models have demonstrated impressive few-shot learning capabilities, smaller models with fewer parameters often require more specialized adaptation to achieve optimal performance on downstream tasks. For these models, task-specific fine-tuning remains an important technique for aligning their capabilities with the specific requirements of the target application.\n",
    "\n",
    "Fine-tuning allows smaller models to capitalize on their pretraining while also benefiting from exposure to task-specific data, enabling them to develop a more targeted understanding of the problem at hand. This process can help bridge the performance gap between smaller models and their larger counterparts, making them viable options for many natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation of Foundation Models\n",
    "\n",
    "Foundation models are designed for various applications. However, to achieve optimal performance for a specific use case, these models often need to be adapted. Adaptation can range in complexity from simple prompt design to significant modifications such as fine-tuning the model with domain-specific data.\n",
    "\n",
    "### Methods of Adaptation\n",
    "\n",
    "Several approaches exist to tailor foundation models. Each method involves trade-offs between computational cost and specialization performance. The primary techniques are:\n",
    "\n",
    "#### 1. Prompting\n",
    "\n",
    "- **Concept:**  \n",
    "  Prompting involves carefully crafting the input text to include instructions or contextual cues. The foundation model then produces an output based solely on the provided prompt. Think of it as asking a well-phrased question to get a targeted answer.\n",
    "\n",
    "- **Advantages:**  \n",
    "  - **Cost-Effective:** No extra training data or significant computing resources are needed.  \n",
    "  - **Simplicity:** The model remains unchanged, using its pre-trained capabilities.\n",
    "\n",
    "- **Limitations:**  \n",
    "  - **Performance Ceiling:** Since the model's parameters are not updated, performance might be limited compared to methods that allow internal adjustments.\n",
    "\n",
    "#### 2. In-Context Learning\n",
    "\n",
    "- **Concept:**  \n",
    "  In-context learning extends prompting by including several examples of the desired input-output pairs within the same session. These examples illustrate to the model how to process similar tasks.\n",
    "\n",
    "- **Advantages:**  \n",
    "  - **Improved Performance:** Provides additional guidance without altering the model's internal weights.  \n",
    "  - **Quick Adaptation:** Useful when task-specific data is scarce because only a few examples are required.\n",
    "\n",
    "- **Limitations:**  \n",
    "  - **Dependence on Pre-Trained Knowledge:** The model still relies entirely on what it learned during pre-training and may not capture complex patterns unique to the target task.\n",
    "\n",
    "#### 3. Fine-Tuning\n",
    "\n",
    "- **Concept:**  \n",
    "  Fine-tuning involves retraining the foundation model on a dataset tailored to the task or domain. During this process, the model's parameters are updated to better capture task-specific patterns.\n",
    "\n",
    "- **Process:**  \n",
    "  Let $ \\theta_0 $ represent the original parameters of the model. After fine-tuning with a specific dataset, the model's parameters become $ \\theta $, optimized to the new data distribution.\n",
    "\n",
    "- **Advantages:**  \n",
    "  - **Specialization:** The model learns more accurate representations for the target task.  \n",
    "  - **Higher Accuracy:** Tailored training data often results in improved performance.\n",
    "\n",
    "- **Limitations:**  \n",
    "  - **Computationally Intensive:** Requires a large dataset and substantial processing power.  \n",
    "  - **Data Requirements:** The quality and quantity of the training data are critical.\n",
    "\n",
    "#### 4. Low-Rank Adaptation (LoRA)\n",
    "\n",
    "- **Concept:**  \n",
    "  LoRA is a parameter-efficient alternative to full fine-tuning. Instead of modifying all model parameters, it introduces low-rank matrices to account for necessary adjustments.\n",
    "\n",
    "- **Process:**  \n",
    "  Suppose the original weight matrix is $ W_0 $. LoRA represents the update $ \\Delta W $ as:  \n",
    "  $$\n",
    "  \\Delta W = BA\n",
    "  $$\n",
    "  where $ B $ and $ A $ are low-rank matrices with much fewer parameters than $ W_0 $. The adjusted weight matrix becomes:\n",
    "  $$\n",
    "  W = W_0 + BA\n",
    "  $$\n",
    "\n",
    "- **Advantages:**  \n",
    "  - **Efficiency:** Reduces the number of parameters that need training.  \n",
    "  - **Lower Resource Demand:** Less computational power and memory are required relative to full fine-tuning.\n",
    "\n",
    "- **Limitations:**  \n",
    "  - **Optimal Balance:** While it often approaches the performance of full fine-tuning, it \n",
    "\n",
    "You can watch an amazing explanation of LoRA [here](https://www.youtube.com/watch?v=dA-NhCtrrVE).\n",
    "  \n",
    "### Key Considerations for Adaptation\n",
    "\n",
    "Several factors must be considered when adapting foundation models:\n",
    "\n",
    "#### Compute Budget\n",
    "\n",
    "- **Challenges:**  \n",
    "  Foundation models can have trillions of parameters. Fully adapting these models may require significant computational resources, including specialized hardware.\n",
    "\n",
    "- **Strategies:**  \n",
    "  - **Selective Adaptation:** Focus on updating only certain parts of the model, such as the last neural network layer or just the bias vectors.  \n",
    "  - **Example Equation:** In a neural network layer, the transformation is often written as:\n",
    "    $$\n",
    "    y = Wx + b\n",
    "    $$\n",
    "    By adapting only $ b $ or the parameters in the final layer, computational overhead is reduced.\n",
    "\n",
    "- **Note:**  \n",
    "  Partial adaptation may limit the degree of task specialization but offers faster experimentation when resources are limited.\n",
    "\n",
    "#### Data Availability\n",
    "\n",
    "- **Challenges:**  \n",
    "  - **Task-Specific Data Scarcity:** Specialized applications may not have enough relevant data, which is crucial for effective adaptation.\n",
    "  \n",
    "- **Strategies:**  \n",
    "  - **Data Acquisition:** Obtain labeled data through manual annotation or sourcing domain-specific datasets.\n",
    "  - **Data Augmentation:** Use techniques to artificially increase the variety and size of the dataset.\n",
    "\n",
    "- **Considerations:**  \n",
    "  The preparation of high-quality data often requires domain expertise to ensure that it accurately represents the nuances of the target domain.\n",
    "\n",
    "\n",
    "### Addressing Common Questions\n",
    "\n",
    "#### Why not use the foundation model without adaptation?\n",
    "\n",
    "- **General vs. Specific:**  \n",
    "  Foundation models are trained on broad datasets. While they can handle a range of tasks, their performance on a specific task may be suboptimal without further adaptation.  \n",
    "- **Need for Specialization:**  \n",
    "  By adapting the model, you tune it to the precise requirements of the task, leading to improved accuracy and efficiency.\n",
    "\n",
    "#### Is fine-tuning always better than prompting?\n",
    "\n",
    "- **Context-Driven Choices:**  \n",
    "  Neither method is universally superior.  \n",
    "  - **Fine-Tuning:**  \n",
    "    Best used for tasks that require deep modifications to the model, provided that there is sufficient task-specific data and resources available.\n",
    "  - **Prompting:**  \n",
    "    Can be effective when resources are limited or when the task is relatively straightforward.\n",
    "\n",
    "#### What if the task requires domain-specific knowledge?\n",
    "\n",
    "- **Specialized Tasks:**  \n",
    "  Tasks that demand precise domain knowledge may necessitate fine-tuning.\n",
    "- **Data Preparation:**  \n",
    "  The availability of high-quality, domain-specific data is essential for capturing the complexities of the subject area. Inputs from subject matter experts might be needed to ensure that the data accurately reflects the domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Explanations: Prompt Engineering\n",
    "\n",
    "Creating prompts is an important aspect of working with AI language models, as it directly influences the quality and relevance of the generated responses. By carefully designing and refining prompts, users can guide the AI to produce more accurate, useful, and contextually appropriate outputs. This practice involves considering various factors such as clarity, specificity, and context to create prompts that effectively communicate the desired task to the AI.\n",
    "\n",
    "### Designing Effective Prompts\n",
    "To create prompts that elicit the best possible responses from AI language models, several key considerations should be kept in mind:\n",
    "\n",
    "1. **Clarity**\n",
    "    - Ensure that the prompt is clear and unambiguous, using straightforward language and avoiding complex sentence structures.\n",
    "    - Ambiguous or confusing prompts can lead to misinterpretation by the AI, resulting in irrelevant or inaccurate responses.\n",
    "\n",
    "2. **Specificity**\n",
    "    - Be specific about the task you want the AI to perform, providing detailed instructions and requirements.\n",
    "    - Vague or open-ended prompts may result in incomplete or off-topic responses, as the AI lacks clear guidance on what is expected.\n",
    "\n",
    "3. **Context**\n",
    "    - Provide sufficient context within the prompt to help the AI understand the background and scope of the task.\n",
    "    - Include relevant details, constraints, or examples that can guide the AI's response and ensure it stays within the desired boundaries.\n",
    "\n",
    "*Example Analogy:* Designing a prompt is similar to giving instructions to a colleague. Just as you would provide clear, detailed, and context-rich instructions to ensure your colleague understands and completes the task effectively, the same approach should be applied when crafting prompts for AI language models.\n",
    "\n",
    "### Types of Prompts\n",
    "Depending on the desired outcome and the nature of the task, different types of prompts can be employed:\n",
    "\n",
    "1. **Zero-Shot Prompts**\n",
    "    - Zero-shot prompts do not provide any examples to the AI, requiring it to generate a response based solely on the instructions given in the prompt.\n",
    "    - These prompts test the AI's ability to generalize and apply its knowledge to new situations without explicit guidance.\n",
    "\n",
    "> *Example:* \"Translate the following sentence into Portuguese: 'Hello, how are you?'\"\n",
    "\n",
    "2. **Few-Shot Prompts**\n",
    "    - Few-shot prompts include a small number of examples within the prompt itself to demonstrate the desired pattern or structure of the response.\n",
    "    - By providing a few relevant examples, users can help the AI understand the specific requirements and expectations for the task at hand.\n",
    "\n",
    "> *Example:* \"Translate the following sentences into Portuguese. 'Hello, how are you?' -> 'Olá, como vai você?' 'Good morning.' -> 'Bom dia' Now, translate: 'Good night.'\"\n",
    "\n",
    "3. **Multi-Step Prompts**\n",
    "    - Multi-step prompts involve breaking down a complex task into smaller, more manageable steps.\n",
    "    - By guiding the AI through a series of sequential sub-tasks, users can help the AI tackle detailed problems more effectively.\n",
    "    - This approach allows for a more structured and controlled interaction with the AI, ensuring that each step is completed satisfactorily before moving on to the next.\n",
    "\n",
    "> *Example:* \"First, summarize the following paragraph. Then, provide a critical analysis. Finally, suggest improvements.\"\n",
    "\n",
    "### Evaluating Prompt Effectiveness\n",
    "To ensure that prompts are achieving the desired objectives and eliciting high-quality responses from the AI, it is crucial to evaluate their effectiveness:\n",
    "\n",
    "1. **Response Quality**\n",
    "    - Assess the relevance, accuracy, and completeness of the AI's responses to the given prompts.\n",
    "    - Determine whether the generated outputs align with the intended purpose and provide meaningful and useful information.\n",
    "\n",
    "2. **Consistency**\n",
    "    - Evaluate the AI's ability to provide consistent responses to similar prompts or variations of the same prompt.\n",
    "    - Inconsistent responses may indicate a lack of robustness or reliability in the AI's understanding of the task.\n",
    "\n",
    "3. **Adaptability**\n",
    "    - Test the AI's ability to handle variations or modifications of the prompt while still producing appropriate and relevant responses.\n",
    "    - A well-designed prompt should allow for some flexibility and adaptability to accommodate minor changes or variations in the input.\n",
    "\n",
    "### Practical Applications\n",
    "Prompt engineering has found numerous practical applications across various domains, demonstrating its value in enhancing the performance and utility of AI language models:\n",
    "\n",
    "1. **Customer Support**\n",
    "    - By crafting prompts that guide AI to provide accurate, helpful, and context-specific responses, businesses can improve their customer support capabilities.\n",
    "    - Well-designed prompts can enable AI to handle various customer inquiries, reducing response times and increasing customer satisfaction.\n",
    "\n",
    "2. **Content Generation**\n",
    "    - Prompt engineering can be leveraged to generate high-quality content, such as creative writing, marketing copy, or technical documentation.\n",
    "    - By providing clear instructions, relevant context, and examples within the prompts, users can guide AI to produce engaging and coherent content tailored to specific requirements.\n",
    "\n",
    "3. **Data Analysis**\n",
    "    - Prompts can be designed to assist AI in various data analysis tasks, such as summarizing large datasets, generating insights, or performing complex calculations.\n",
    "    - By breaking down the analysis process into smaller steps and providing specific instructions, users can exploit AI to extract meaningful information from data more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering Strategies and Examples\n",
    "\n",
    "Prompt engineering strategies are techniques used to design and structure prompts in ways that perfect the performance of AI language models. By employing various strategies, users can guide the AI to generate more accurate, coherent, and contextually relevant responses. This section will explore several prompt engineering strategies and provide examples to illustrate their application.\n",
    "\n",
    "### Prompt Engineering Strategies\n",
    "\n",
    "1. **Zero-Shot Learning**\n",
    "    - Description: The model is given a task without any prior examples or demonstrations.\n",
    "    - Example Prompt: \"Translate the following sentence from English to Portuguese: 'Good morning, everyone.'\"\n",
    "    - Zero-shot learning tests the model's ability to perform tasks without explicit training or examples. It relies on the model's pre-existing knowledge and understanding of language to generate appropriate responses.\n",
    "\n",
    "2. **Few-Shot Learning**\n",
    "    - Description: The model is given a few examples or demonstrations before being asked to perform the task.\n",
    "    - Example Prompt: \"Translate the following sentences from English to Portuguese: 'Good morning, everyone.' - 'Bom dia a todos' 'How are you?' - 'Como vai você?' Now translate: 'What is your name?'\"\n",
    "    - Few-shot learning provides the model with a small number of examples to guide its understanding of the task. By observing the patterns and relationships in the examples, the model can adapt its responses to similar tasks.\n",
    "\n",
    "3. **Chain-of-Thought Prompting**\n",
    "    - Description: The model is guided through a step-by-step reasoning process to arrive at the final answer.\n",
    "    - Example Prompt: \"To solve the math problem 12 + 5, first add 10 + 5 to get 15, then add 2 to get 17. So, 12 + 5 equals 17.\"\n",
    "    - Chain-of-thought prompting breaks down complex tasks into smaller, sequential steps. By providing a clear reasoning process, the model can generate more accurate and coherent responses, especially for tasks that require multi-step problem-solving.\n",
    "\n",
    "4. **React (Reasoning and Acting)**\n",
    "    - Description: The model is prompted to reason about the task and then act based on that reasoning.\n",
    "    - Example Prompt: \"To write a summary, first identify the main points of the article, then condense those points into a few sentences. Here's the article: [Article Text]\"\n",
    "    - The React strategy involves two stages: reasoning and acting. The model first analyzes the task and formulates a plan of action. It then executes the plan to generate the final response. This strategy helps the model produce more structured and purposeful outputs.\n",
    "\n",
    "5. **Plan-and-Solve**\n",
    "    - Description: The model is first asked to create a plan and then execute it.\n",
    "    - Example Prompt: \"First, outline the steps needed to bake a cake. Then, write a recipe following those steps.\"\n",
    "    - Similar to the React strategy, Plan-and-Solve involves two distinct phases. The model first generates a plan or outline for the task and then uses that plan to guide its response. This strategy is particularly effective for tasks that require a systematic approach or have multiple components.\n",
    "\n",
    "6. **Contextual Prompting**\n",
    "    - Description: The model is given a rich context to understand the task better.\n",
    "    - Example Prompt: \"Given the context of a restaurant review, summarize the following review: [Review Text]\"\n",
    "    - Contextual prompting provides the model with additional information or background to help it better understand the task at hand. By supplying relevant context, the model can generate more accurate and nuanced responses that align with the specific domain or scenario.\n",
    "\n",
    "7. **Task-Specific Templates**\n",
    "    - Description: Using templates tailored to specific tasks to guide the model.\n",
    "    - Example Prompt: \"Email template: 'Dear [Name], I am writing to inform you about [Event/Topic]. Please let me know if you have any questions. Best regards, [Your Name]'\"\n",
    "    - Task-specific templates provide a structured format for the model to follow when generating responses. By incorporating placeholders or variables, templates can be easily customized for different instances of the same task, ensuring consistency and efficiency.\n",
    "\n",
    "8. **Mixed-Task Prompting**\n",
    "    - Description: Combining multiple tasks within a single prompt to improve model performance.\n",
    "    - Example Prompt: \"Translate the following sentence to Spanish and then summarize it: 'The weather is nice today.'\"\n",
    "    - Mixed-task prompting involves presenting the model with multiple related tasks in a single prompt. By combining tasks, the model can capitalize on the shared context and generate more coherent and thorough responses. This strategy can also help improve efficiency by reducing the number of separate prompts needed.\n",
    "\n",
    "9. **Feedback Loop**\n",
    "    - Description: Incorporating feedback to refine and improve the prompts iteratively.\n",
    "    - Example Prompt: \"Generate a story based on the following prompt, and then refine the story based on feedback: [Initial Story]\"\n",
    "    - The feedback loop strategy involves generating an initial response, receiving feedback or critique, and then iteratively refining the prompt and response based on that feedback. This iterative process allows for continuous improvement and adaptation of the model's outputs.\n",
    "\n",
    "10. **Role Assignment**\n",
    "    - Description: Assigning specific roles to the model to guide its response.\n",
    "    - Example Prompt: \"You are a customer service representative. Respond to the following customer query: 'I have an issue with my order.'\"\n",
    "    - Role assignment prompts the model to adopt a specific persona or role when generating responses. By providing a clear context and expectations for the model's behavior, role assignment can lead to more consistent and appropriate responses within a given domain.\n",
    "\n",
    "11. **Data Augmentation**\n",
    "    - Description: Using various augmented data examples to enhance the prompt's effectiveness.\n",
    "    - Example Prompt: \"Given these examples of email subjects and their corresponding responses, generate a response for the new email subject: [Examples and New Subject]\"\n",
    "    - Data augmentation involves providing the model with additional examples or variations of the input data to improve its understanding and generalization capabilities. By exposing the model to a diverse range of examples, data augmentation can help the model generate more sturdy and accurate responses.\n",
    "\n",
    "12. **Interactive Prompts**\n",
    "    - Description: Engaging the model in a back-and-forth interaction to complete a task.\n",
    "    - Example Prompt: \"Let's brainstorm ideas for a new project. What do you think about focusing on environmental sustainability?\"\n",
    "    - Interactive prompts simulate a conversation or dialogue between the user and the model. By engaging in a series of exchanges, the model can progressively refine its understanding of the task and generate more relevant and coherent responses. Interactive prompts are particularly useful for tasks that require collaboration or iterative refinement.\n",
    "\n",
    "13. **Clarifying Questions**\n",
    "    - Description: Encouraging the model to ask clarifying questions before providing the answer.\n",
    "    - Example Prompt: \"If you're unsure about the task, ask questions to clarify. Task: Write a report on climate change.\"\n",
    "    - Clarifying questions prompt the model to seek additional information or clarification when the task or input is ambiguous or incomplete. By asking relevant questions, the model can gather the necessary details to provide a more accurate and thorough response. This strategy helps improve the model's understanding and reduces the likelihood of generating irrelevant or incorrect outputs.\n",
    "\n",
    "> Each of these strategies can be adapted and combined based on the specific requirements of the task at hand, using the strengths of large language models to achieve the desired outcomes. By carefully designing and structuring prompts using these strategies, users can guide the model to generate more accurate, coherent, and contextually relevant responses.\n",
    "> It's important to note that the effectiveness of each strategy may vary depending on the specific model and task. Experimentation and iteration are key to finding the most suitable combination of strategies for a given scenario. Additionally, as language models continue to grow and improve, new prompt engineering strategies may emerge, further expanding the possibilities for refining model performance.\n",
    ">\n",
    "> To learn more, [this site](https://www.promptingguide.ai/techniques) is a good starting point for exploring additional prompt engineering techniques and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Side note: Is it \"Engineering\"?\n",
    ">\n",
    "> The use of the term \"engineering\" concerning \"prompt engineering\" is a subject of debate. While prompt engineering involves crafting and refining text inputs to effectively guide AI behavior, it differs from traditional engineering disciplines in several key aspects.\n",
    ">\n",
    "> **Traditional Engineering:**\n",
    "> - Applies rigorous scientific principles and quantitative methods\n",
    "> - Focuses on designing and building physical structures and systems\n",
    "> - Relies heavily on mathematical modeling, testing, and validation\n",
    ">\n",
    "> **Prompt Engineering:**\n",
    "> - Requires creativity, strategy, and understanding of the AI model\n",
    "> - Involves crafting and refining text inputs to guide AI behavior\n",
    "> - Lacks the same level of rigorous, quantitative methods as traditional engineering\n",
    ">\n",
    "> Some argue that referring to prompt creation as \"engineering\" may overstate the technical rigor involved in the process. Prompt engineering is more akin to a form of art or creative writing, where the goal is to effectively communicate ideas and elicit desired responses from the AI model.\n",
    ">\n",
    "> However, others contend that the term \"engineering\" is appropriate, as prompt engineering does involve:\n",
    "> - Systematic design and refinement of prompts\n",
    "> - Consideration of the AI model's architecture and capabilities\n",
    "> - Iterative testing and optimization to achieve desired outcomes\n",
    ">\n",
    "> Ultimately, while prompt engineering may not adhere to the strict definition of traditional engineering, it does require a structured, iterative approach to designing effective prompts. The use of the term \"engineering\" highlights the strategic and methodical aspects of the process, even if it does not involve the same level of mathematical rigor as other engineering disciplines.\n",
    ">\n",
    "> Regardless of the terminology used, the importance of crafting clear, well-structured, and effective prompts cannot be overstated. The quality of the prompt directly impacts the quality of the AI's output, making prompt engineering a critical skill in the field of AI application and development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs with [LangChain](https://www.langchain.com/)\n",
    "\n",
    "LangChain is a powerful platform that simplifies the process of interacting with large language models (LLMs), enabling users to use their capabilities for various language-related tasks. By providing a user-friendly interface and a range of features, LangChain makes it easier to utilize LLMs for text generation, question answering, content summarization, and more.\n",
    "\n",
    "## Key Features of LangChain\n",
    "\n",
    "1. **Smooth Incorporation with LLMs**\n",
    "    - LangChain offers uninterrupted combination with popular LLMs, allowing users to access their capabilities without the need for complex setup or configuration.\n",
    "    - It supports numerous LLMs, including OpenAI's GPT models, Google's BERT, and others, ensuring compatibility with advanced language models.\n",
    "\n",
    "2. **Instinctive API and Documentation**\n",
    "    - LangChain provides a well-documented and natural API, making it easy for developers to incorporate LLMs into their applications.\n",
    "    - The platform offers complete documentation, including code examples and tutorials, to guide users through the process of working with LLMs.\n",
    "\n",
    "3. **Flexible Input and Output Handling**\n",
    "    - LangChain supports various input formats, such as text, documents, and even structured data, allowing users to process and analyze diverse types of content.\n",
    "    - It provides flexibility in handling output, enabling users to customize the generated text, control the length and style of the output, and integrate it seamlessly into their applications.\n",
    "\n",
    "4. **Task-Specific Modules**\n",
    "    - LangChain offers task-specific modules that are optimized for common language-related tasks, such as text summarization, question answering, and sentiment analysis.\n",
    "    - These modules encapsulate best practices and provide pre-configured settings, making it easier for users to achieve high-quality results without extensive fine-tuning.\n",
    "\n",
    "5. **Memory and Context Management**\n",
    "    - LangChain includes features for managing memory and context, allowing LLMs to maintain a coherent understanding of the conversation or document being processed.\n",
    "    - This enables more contextually relevant and consistent outputs, especially in scenarios involving multi-turn conversations or long-form text generation.\n",
    "\n",
    "## Interacting with LLMs using LangChain\n",
    "\n",
    "To interact with LLMs using LangChain, follow these general steps:\n",
    "\n",
    "1. **Installation and Setup**\n",
    "    - Install the LangChain library and its dependencies in your Python environment.\n",
    "    - Configure the necessary API keys and credentials for the LLMs you intend to use.\n",
    "\n",
    "2. **Importing and Initializing Models**\n",
    "    - Import the required LangChain modules and classes for the specific LLMs you want to work with.\n",
    "    - Initialize instances of the LLMs, specifying the desired configuration options.\n",
    "\n",
    "3. **Preparing Input Data**\n",
    "    - Preprocess and format your input data, such as text documents or user queries, to ensure compatibility with the LLMs.\n",
    "    - Utilize LangChain's input handling features to convert and structure the data as needed.\n",
    "\n",
    "4. **Generating Output**\n",
    "    - Use LangChain's API to pass the prepared input data to the LLMs and generate the desired output.\n",
    "    - Customize the generation parameters, such as the output length, temperature, or top-k sampling, to control the quality and diversity of the generated text.\n",
    "\n",
    "5. **Post-processing and Combination**\n",
    "    - Process the generated output to extract relevant information, perform additional analysis, or integrate it into your application's workflow.\n",
    "    - Capitalize On LangChain's output handling capabilities to format and structure the results as required.\n",
    "\n",
    "## [LlamaIndex](https://www.llamaindex.ai/): An Alternative to LangChain\n",
    "\n",
    "LlamaIndex is another platform that enables interaction with LLMs, particularly focusing on indexing and querying large document collections. Here are some key features of LlamaIndex:\n",
    "\n",
    "1. **Document Indexing**: LlamaIndex provides tools to efficiently index large collections of documents, making them searchable and accessible for querying using LLMs.\n",
    "\n",
    "2. **Semantic Search**: With LlamaIndex, users can perform semantic searches on the indexed documents, using the power of LLMs to understand the context and meaning of the queries.\n",
    "\n",
    "3. **Customizable Indexing**: LlamaIndex allows users to customize the indexing process, including document preprocessing, tokenization, and embedding generation, to enhance the indexing performance for specific use cases.\n",
    "\n",
    "4. **Query Optimization**: LlamaIndex employs various techniques to improve the querying process, such as query expansion, relevance ranking, and context-aware retrieval, to ensure accurate and efficient results.\n",
    "\n",
    "While LlamaIndex excels in indexing and querying large document collections, it may have a steeper learning curve compared to LangChain and may require more setup and configuration for custom use cases.\n",
    "\n",
    "## Why Choose LangChain?\n",
    "\n",
    "LangChain is a compelling choice for several reasons:\n",
    "\n",
    "1. **Versatility**: LangChain's support for numerous LLMs and its flexibility in handling various language-related tasks make it a versatile platform for diverse use cases.\n",
    "\n",
    "2. **Ease of Use**: With its instinctive API, complete documentation, and task-specific modules, LangChain simplifies the process of integrating LLMs into applications, even for developers with limited experience.\n",
    "\n",
    "3. **Extensibility**: LangChain's modular architecture allows users to extend and customize its functionality to suit their specific requirements, enabling the development of tailored language-based solutions.\n",
    "\n",
    "4. **Active Development**: LangChain has an active development community and regular updates, ensuring that users have access to the latest features, improvements, and bug fixes.\n",
    "\n",
    "5. **Popularity**: As I am writing this notebook, LlamaIndex has 33.9k stars and 4.8k forks on GitHub. LangChain, on the other hand, has 14.2k forks and 89.8k stars. This indicates that LangChain is more popular and widely used among developers.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "<h4>Google Trends Comparison Between LangChain and LlamaIndex</h4>\n",
    "</div>\n",
    "<p align=\"center\">\n",
    "<img src=\"images/langchain_llamaindex.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "> **Note:** The choice between LangChain and LlamaIndex ultimately depends on the specific requirements and goals of your project. If your primary focus is on indexing and querying large document collections, LlamaIndex might be a suitable choice. However, if you require a more versatile and user-friendly platform for interacting with LLMs across numerous tasks, LangChain is often the preferred option. It's recommended to evaluate both platforms based on your needs and explore their respective documentation and examples to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "## Ollama: A Versatile Framework for Running Large Language Models Locally\n",
    "\n",
    "Ollama is a powerful and flexible framework designed to run large language models (LLMs) locally on a user's machine. It supports numerous popular models, including Llama, Phi, Mistral, and Gemma, among others. With Ollama, users can create, manage, and deploy custom models easily, making it an ideal tool for developers and researchers looking to build original AI applications. For more detailed information and to get started, visit the [Ollama website](https://ollama.com).\n",
    "\n",
    "### Key Features of Ollama\n",
    "\n",
    "1. **Extensive Model Support**: Ollama supports several advanced models, such as Llama. These models are optimized for various tasks, including text generation and dialogue systems, allowing users to choose the most suitable model for their specific needs.\n",
    "\n",
    "2. **Customization through Modelfiles**: Users can create and customize their models using a `Modelfile`. This feature enables setting parameters like temperature, context length, and system messages to tailor the model's behavior. For example, a `Modelfile` can be configured to make the model respond as a specific character or follow certain dialogue styles, providing great flexibility in model customization.\n",
    "\n",
    "3. **Local Deployment for Privacy and Security**: One of the key advantages of Ollama is its ability to run models locally. This is particularly useful for applications requiring high privacy and security standards, as users can process data on their own hardware without relying on cloud services. Local deployment gives users full control over their data and reduces the risk of unauthorized access.\n",
    "\n",
    "4. **Multimodal Capabilities for Enhanced Functionality**: Some models within the Ollama framework, such as the LLaVA (Large Language-and-Vision Assistant), support multimodal inputs. This means they can process and generate text based on image inputs, opening up new possibilities for creative and interactive applications.\n",
    "\n",
    "5. **Optimized Performance and Efficiency**: Models like Phi-3 are designed for efficient processing, featuring Rotary Position Embeddings and long context lengths. These optimizations make them suitable for complex tasks like code completion and dialogue generation, ensuring smooth performance even with resource-intensive applications.\n",
    "\n",
    "### Practical Usage of Ollama\n",
    "\n",
    "1. **Running Models with Simple Commands**: To run a model like Llama 3.1, users can execute commands such as `ollama run llama3.1` for the 8B or 70B parameter versions. Ollama supports popular tooling integrations, including LangChain and LlamaIndex, allowing uninterrupted implementation into existing workflows.\n",
    "\n",
    "2. **Creating Custom Models with Modelfiles**: Ollama makes it straightforward to create custom models. Users can define their model configurations in a `Modelfile` and use commands like `ollama create` to generate the model. Once created, the model can be deployed using the `ollama run` command.\n",
    "\n",
    "3. **Integrating Models with REST API**: Ollama provides a thorough REST API for developers to integrate models into their applications. This API supports various functionalities, such as generating responses and engaging in interactive dialogues with the models, making it easy to build AI-powered features into existing software.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here's what you'll get after installing ollama.\n",
    "\n",
    "```bash\n",
    "(base) jacob@schrodinger ~ % ollama run llama3.1\n",
    ">>> Oi! Tudo bem?\n",
    "Tudo bem, obrigado! Eu sou um modelo de linguagem treinado para ajudar a\n",
    "responder perguntas e discutir tópicos. Não tenho sentimentos como o ser\n",
    "humano, mas estou aqui para ajudá-lo a qualquer momento. Como posso\n",
    "ajudá-lo hoje?\n",
    "\n",
    ">>> /bye\n",
    "(base) jacob@schrodinger ~ %\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Llama 3.1: Advanced Natural Language Processing from Meta\n",
    "\n",
    "Llama 3.1, developed by Meta, is the latest version of the Llama language model series. This model offers advanced natural language processing capabilities and is accessible to individuals, researchers, and businesses for experimentation and innovation.\n",
    "\n",
    "### Key Features of Llama 3.1\n",
    "\n",
    "1. **Multiple Model Sizes**: Llama 3.1 is available in various sizes, including 8 billion (8B), 70 billion (70B), and the new 405 billion (405B) parameter models. These models come in both pre-trained and instruction-tuned versions, serving to different applications and levels of customization.\n",
    "\n",
    "2. **Superior Performance**: Llama 3.1 has shown impressive performance compared to its predecessors and competitors. It outperforms OpenAI's GPT-4 on the HumanEval benchmark, scoring 81.7 compared to GPT-4's 67. However, it slightly trails behind GPT-4 in the MMLU knowledge assessment.\n",
    "\n",
    "3. **Serverless API Deployment**: The models can be deployed via serverless APIs on platforms like Azure AI, providing developers with tools to easily integrate these models into their applications. This assimilation includes enhanced security and compliance features, such as Azure AI Content Safety.\n",
    "\n",
    "4. **Safety Features and Guardrails**: Meta has carried out new safety features, including Code Shield, to catch insecure code that the model might produce. This ensures that applications built using Llama 3.1 adhere to ethical and security standards.\n",
    "\n",
    "5. **Versatility and Compatibility**: Llama 3.1 models are versatile, supporting various applications from text generation to chatbots. They are compatible with popular machine learning frameworks like Hugging Face's Transformers, enabling easy fine-tuning and deployment.\n",
    "\n",
    "### Practical Usage of Llama 3.1\n",
    "\n",
    "Llama 3.1 can be used for numerous tasks, such as:\n",
    "- **Text Generation**: Generating coherent and contextually relevant text based on prompts.\n",
    "- **Chatbots**: Creating conversational agents that can handle complex interactions.\n",
    "- **Coding Assistance**: Assisting in code generation and debugging with high accuracy.\n",
    "- **Content Creation**: Generating creative content for marketing, storytelling, and more.\n",
    "\n",
    "## Requirements and Considerations\n",
    "\n",
    "Running large language models like Llama 3.1 requires significant computational resources, especially for the larger model sizes. The table below shows the memory requirements for each model size:\n",
    "\n",
    "| Model Size | FP16 | FP8 | INT4 |\n",
    "|------------|-------|-------|-------|\n",
    "| 8B | 16 GB | 8 GB | 4 GB |\n",
    "| 70B | 140 GB| 70 GB | 35 GB |\n",
    "| 405B | 810 GB| 405 GB| 203 GB|\n",
    "\n",
    "<br>\n",
    "\n",
    "> Note: The numbers above indicate the GPU VRAM required just to load the model checkpoint. They don't include torch reserved space for kernels or CUDA graphs.\n",
    "> Note: FP16, FP8 and INT4 are the precision types used to store the model weights. FP16 is half precision, FP8 is 8-bit floating point, and INT4 is 4-bit integer. Lower precision types require less memory but may impact model performance.\n",
    "\n",
    "Depending on your computer configuration, you may be able to run the 8B or 70B model. For optimal performance, it is recommended to use a GPU with sufficient VRAM. If you don't have access to a GPU, the models can still run on CPU and RAM, but the processing speed will be slower. Apple devices with Mx series chips can capitalize on the Apple Silicon GPU to accelerate the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder: Stateless Nature of Language Models\n",
    "\n",
    "Language models, including Large Language Models (LLMs), are inherently **stateless**. This means they do not maintain an internal state or memory across different interactions or requests. Each input is processed independently, without retaining any context or information from previous exchanges.\n",
    "\n",
    "## Reasons for Statelessness\n",
    "\n",
    "The stateless design of language models is intentional and serves several purposes:\n",
    "\n",
    "1. **Efficient Scaling**: By treating each request as an independent interaction, language models can handle a large number of concurrent users without needing to manage and store individual conversation states. This allows for efficient resource allocation.\n",
    "\n",
    "2. **Design and Maintenance**: Stateless models are simpler to design, apply, and maintain compared to stateful models. They do not require complex mechanisms for tracking and updating conversation states, making the overall architecture more straightforward.\n",
    "\n",
    "3. **Versatility**: Stateless models can handle numerous tasks and topics without being constrained by previous interactions. They can switch between different contexts and respond to each input based on the current information provided.\n",
    "\n",
    "## Consequences of Statelessness\n",
    "\n",
    "The stateless nature of language models has significant consequences:\n",
    "\n",
    "1. **No Memory Retention**: Language models do not maintain an internal state, so they cannot remember or refer back to information from previous interactions. Each input is treated as a standalone request, and the model generates a response based solely on the current input and its pre-trained knowledge.\n",
    "\n",
    "2. **Coherence Challenges**: Without the ability to retain context across interactions, language models may struggle to maintain coherence and consistency in long-form conversations or multi-turn dialogues. They cannot build upon previous exchanges or understand the broader context of the conversation.\n",
    "\n",
    "3. **Response Variability**: Due to the lack of memory, language models may repeat information or provide inconsistent responses if asked similar questions multiple times. They cannot learn from or adapt to the specific user's preferences or previous interactions.\n",
    "\n",
    "4. **Impersonal Responses**: Stateless models cannot personalize their responses based on individual user profiles or preferences. Each interaction is treated independently, making it challenging to tailor the model's behavior to specific users.\n",
    "\n",
    "## Mitigating Statelessness\n",
    "\n",
    "To address the limitations of statelessness, developers and researchers employ various techniques:\n",
    "\n",
    "- **Contextual Prompts**: Carefully crafting prompts that provide sufficient context and information can help guide the language model's response and mitigate the lack of long-term memory. By including relevant details in the prompt, the model can generate more coherent and contextually appropriate responses.\n",
    "\n",
    "- **Information Retrieval**: Some approaches involve integrating external memory systems or databases with language models. By storing and retrieving relevant information from external sources, the model can simulate a form of memory and provide more consistent and contextually aware responses.\n",
    "\n",
    "- **Conversation Management**: Developers can build stateful wrappers around language models to maintain conversation states and enable more personalized interactions. These wrappers manage the conversation flow, store relevant information, and provide the necessary context to the language model for each interaction.\n",
    "\n",
    "> **Note**: Understanding the stateless nature of language models is crucial for setting appropriate expectations and designing effective strategies for utilizing them in various applications. By recognizing their limitations and employing suitable techniques, developers can use the power of language models while mitigating the challenges posed by statelessness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "`Enough of the marketing talk! Let's get to the real stuff!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 characters of API key: sk-proj-eJ\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# This is useful for keeping sensitive information like API keys out of your code\n",
    "load_dotenv()  # You are expected to have a .env file with the OpenAI API KEY `OPENAI_API_KEY`\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "# We're only displaying the first 10 characters for security reasons\n",
    "# This is a good practice to verify the key is loaded without exposing it entirely\n",
    "api_key_preview = os.getenv(\"OPENAI_API_KEY\")[:10]\n",
    "print(f\"First 10 characters of API key: {api_key_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using the model directly. ChatModels are examples of LangChain \"Runnables,\" meaning they provide a uniform interface for interaction. To call the model straightforwardly, we can provide a list of messages to the .invoke method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2850174/1753401483.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model_openai = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 13, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-e31c0745-fdd3-4d7a-980b-a5a167b2445e-0'\n"
     ]
    }
   ],
   "source": [
    "# Import the ChatOpenAI class from the appropriate module (not shown in the original code)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "model_openai = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Specify the model to use (GPT-4 mini variant)\n",
    "    temperature=0.7,  # Set the temperature for response generation (higher values increase randomness)\n",
    ")\n",
    "\n",
    "# Note on pricing:\n",
    "# This model costs approximately 0.15 USD per million tokens for input\n",
    "# and half of that for output. For more details on models and pricing,\n",
    "# visit: https://openai.com/api/pricing/\n",
    "\n",
    "# Invoke the model with a simple greeting\n",
    "response = model_openai.invoke(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2850174/3734595362.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model_llama = ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to help you with any questions or tasks you may have! How about you? How's your day going?\" additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-03-12T14:09:47.461074078Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 6063983653, 'load_duration': 5276028963, 'prompt_eval_count': 16, 'prompt_eval_duration': 302000000, 'eval_count': 50, 'eval_duration': 483000000} id='run-75da6386-d3c1-4eb4-b891-4014ac64982d-0'\n"
     ]
    }
   ],
   "source": [
    "# Import the ChatOllama class from the langchain library\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# Initialize the ChatOllama model\n",
    "model_llama = ChatOllama(\n",
    "    model=\"llama3.1\",  # Specify the model version\n",
    "    base_url=\"http://localhost:11434\",  # URL where Ollama is running locally\n",
    "    temperature=0.7,  # Control the randomness of the output (0.0 to 1.0)\n",
    ")\n",
    "\n",
    "# Note: Ensure Ollama is running on your computer before executing this code\n",
    "\n",
    "# If you encounter an OllamaEndpointNotFoundError, you may need to pull the model\n",
    "# Run the following command in your terminal:\n",
    "# ollama pull llama3.1\n",
    "\n",
    "# Generate a response from the model\n",
    "response = model_llama.invoke(\"Hello, how are you?\")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `temperature` in LLMs?\n",
    "\n",
    "regarding Large Language Models (LLMs), **temperature** is a crucial parameter that influences the randomness and creativity of the model's output. It plays a significant role in determining how the model generates text, impacting the balance between deterministic and stochastic behavior.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "- **Definition**: Temperature is a hyperparameter that controls the probability distribution of the next word in a sequence. It essentially modifies the logits (raw predictions) before they are converted into probabilities.\n",
    "\n",
    "- **Effect on Output**:\n",
    "    - **Low Temperature**: When the temperature is set to a low value (close to 0), the model's output becomes more deterministic and conservative. It tends to choose the highest probability word more consistently, resulting in repetitive or predictable text.\n",
    "    - **High Temperature**: A high temperature value makes the model's output more random and creative. It increases the likelihood of selecting less probable words, which can lead to more diverse and imaginative text, but also increases the risk of generating incoherent or irrelevant content.\n",
    "\n",
    "#### Detailed Explanation\n",
    "\n",
    "- **Mathematical Perspective**:\n",
    "    - The temperature parameter $ T $ adjusts the logits $ z $ before applying the softmax function to obtain probabilities $ P $.\n",
    "    - The formula used is: $ P(i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $\n",
    "    - When $ T $ is low, the differences between logits are amplified, making the highest probability word much more likely to be chosen.\n",
    "    - When $ T $ is high, the differences between logits are diminished, leading to a more uniform probability distribution.\n",
    "\n",
    "- **Practical Effects**:\n",
    "    - **Creative Writing**: A higher temperature setting can be useful for tasks requiring creativity, such as story generation or poetry.\n",
    "    - **Technical Writing**: For tasks that require precision and accuracy, a lower temperature is often preferred to avoid introducing errors or irrelevant information.\n",
    "\n",
    "#### Examples and Analogies\n",
    "\n",
    "- **Analogy**: Think of temperature as a dial that controls the \"creativity\" of the model. Turning the dial down makes the model more conservative and focused, while turning it up makes the model more adventurous and willing to take risks.\n",
    "\n",
    "- **Example Scenario**:\n",
    "    - **Low Temperature**: In a technical document summarization task, a low temperature setting ensures that the summary remains accurate and closely aligned with the source material.\n",
    "    - **High Temperature**: In a creative writing prompt, a high temperature setting can help generate novel and interesting ideas, even if they are less predictable.\n",
    "\n",
    "\n",
    "> **Tip**: Experimenting with different temperature settings can help you find the optimal balance for your specific application. Start with a moderate value and adjust as needed based on the output quality.\n",
    ">\n",
    "> You can significantly influence the behavior of LLMs to better suit your needs, whether for creative endeavors or precise tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Eu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para o aprendizado de máquina (ML).' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 35, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-136be90d-0972-4f74-9ee1-f1c6f4fc95a4-0'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary message types from langchain_core.messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Create a list of messages to establish the context for the AI model\n",
    "messages = [\n",
    "    # Set the system message to define the task for the AI\n",
    "    SystemMessage(content=\"Translate the following from English into Portuguese\"),\n",
    "    # Add a human message with the content to be translated\n",
    "    HumanMessage(\n",
    "        content=\"I'd love yo learn more about Euler's number and why it is so important for ML\"\n",
    "    ),\n",
    "]\n",
    "# Note: This list structure allows for maintaining conversation history\n",
    "# and providing context for each interaction with the model\n",
    "\n",
    "# Invoke the OpenAI model with the prepared messages\n",
    "# The model will then generate a response based on the given context\n",
    "response = model_openai.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aqui está a tradução:\\n\\nEu gostaria de aprender mais sobre o número de Euler e por que ele é tão importante para o Aprendizado de Máquina (ML).\\n\\nSe você quiser saber mais, posso fornecer informações adicionais!' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-03-12T14:10:49.888454616Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1444613592, 'load_duration': 21958532, 'prompt_eval_count': 40, 'prompt_eval_duration': 28000000, 'eval_count': 53, 'eval_duration': 496000000} id='run-a6ac8ac0-9fd6-4830-827c-543e1dcf12bb-0'\n"
     ]
    }
   ],
   "source": [
    "response = model_llama.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Aqui está a tradução para português:\\n\\nEu adoraria aprender mais sobre o número de Euler e por que ele é tão importante na área da Inteligência Artificial (Machine Learning - ML).' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-03-12T14:10:50.353446898Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 458237422, 'load_duration': 22392844, 'prompt_eval_count': 36, 'prompt_eval_duration': 5000000, 'eval_count': 43, 'eval_duration': 410000000} id='run-c3f85f7d-6bc3-491b-abcf-278f47816168-0'\n"
     ]
    }
   ],
   "source": [
    "# Define a list of messages for the language model\n",
    "messages_no_system = [\n",
    "    # First message: Instruction for translation\n",
    "    HumanMessage(content=\"Translate the following from English into Portuguese\"),\n",
    "    # Note: Some models may have issues with System messages, so we pass the instruction as a Human Message.\n",
    "    # Second message: The actual content to be translated\n",
    "    HumanMessage(\n",
    "        content=\"I'd love yo learn more about Euler's number and why it is so important for ML\"\n",
    "    ),\n",
    "    # This message contains the text we want to translate into Portuguese\n",
    "]\n",
    "\n",
    "# Invoke the language model (llama) with the defined messages\n",
    "response = model_llama.invoke(messages_no_system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputParsers: Extracting Relevant Information from Model Responses\n",
    "\n",
    "When working with language models, the response received is often in the form of an `AIMessage`. This message contains not only the string response but also additional metadata about the generated output. However, in many cases, we are primarily interested in extracting and using just the string response itself.\n",
    "\n",
    "#### Using the Simple Output Parser\n",
    "\n",
    "To parse out the desired string response, we can employ a simple output parser. Here's how you can use it:\n",
    "\n",
    "1. **Standalone Usage**:\n",
    "    - Import the output parser.\n",
    "    - Save the result of the language model call.\n",
    "    - Pass the saved result to the parser to extract the string response.\n",
    "\n",
    "2. **Chaining with the Language Model**:\n",
    "    - A more common and convenient approach is to \"chain\" the output parser with the language model.\n",
    "    - By chaining, the output parser is automatically invoked every time the language model is called within the chain.\n",
    "    - The chain takes the input type of the language model (string or list of messages) and returns the output type of the output parser (string).\n",
    "\n",
    "In LangChain, the `|` operator is used to combine two elements together, making it easy to create a chain. This way, we create a chain that automatically parses the model's output and returns the extracted string response.\n",
    "\n",
    "#### The Power of Chaining in LangChain\n",
    "\n",
    "Chaining elements is a fundamental pattern in LangChain that enables the creation of powerful processing pipelines. It allows you to combine different components seamlessly, passing the output of one element as the input to the next.\n",
    "\n",
    "This concept is known as **[LCEL - LangChain Expression Language](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel)**. LCEL provides a concise and expressive way to define and manipulate chains of elements in LangChain.\n",
    "\n",
    "By using chaining, you can create complex workflows that involve multiple stages of processing, such as:\n",
    "- Preprocessing input data\n",
    "- Calling language models\n",
    "- Parsing and transforming model outputs\n",
    "- Performing additional computations or integrations\n",
    "\n",
    "Chaining simplifies the process of building and managing these pipelines, making it easier to create sophisticated applications with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StrOutputParser class from the langchain_core.output_parsers module\n",
    "# This class is used to parse string outputs from language models\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create an instance of the StrOutputParser\n",
    "# This instance will be used to parse string outputs in subsequent code\n",
    "simple_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the OpenAI model with the provided messages\n",
    "# The model_openai.invoke method sends the messages_no_system to the OpenAI model\n",
    "# and returns the model's response, which is stored in output_openai_1\n",
    "output_openai_1 = model_openai.invoke(messages_no_system)\n",
    "\n",
    "# Invoke the LLaMA model with the same set of messages\n",
    "# The model_llama.invoke method sends the messages_no_system to the LLaMA model\n",
    "# and returns the model's response, which is stored in output_llama_1\n",
    "output_llama_1 = model_llama.invoke(messages_no_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the simple_parser instance to parse the output from the OpenAI model\n",
    "\n",
    "parsed_output_openai = simple_parser.invoke(output_openai_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s the translation:\\n\\n\"Eu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para a inteligência artificial (ML).\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the simple_parser instance to parse the output from the OpenAI model\n",
    "simple_parser.invoke(output_llama_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a processing chain for the OpenAI model\n",
    "# The '|' operator is used to chain together the model and the parser\n",
    "# This means that the output of model_openai will be automatically passed to simple_parser\n",
    "# The resulting chain_openai can be used to process inputs through the model and then parse the outputs\n",
    "chain_openai = model_openai | simple_parser\n",
    "\n",
    "# Create a processing chain for the LLaMA model\n",
    "# Similar to chain_openai, this chain will process inputs through model_llama and then parse the outputs using simple_parser\n",
    "chain_llama = model_llama | simple_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para a aprendizagem de máquina.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can invoke the whole chain to process inputs and parse outputs in a single step\n",
    "chain_openai.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aqui está a tradução:\\n\\nEu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para o Aprendizado de Máquina (ML).\\n\\nEuler's Number, também conhecido como e ou E, é uma constante matemática fundamental. Ele foi introduzido pelo matemático suíço Leonhard Euler em 1735 e é aproximadamente igual a 2,71828.\\n\\nO número de Euler tem várias aplicações importantes na Matemática e no Aprendizado de Máquina (ML). Aqui estão algumas delas:\\n\\n1. **Modelagem de crescimento**: O número de Euler é usado para modelar o crescimento exponencial de certos fenômenos, como a população de uma espécie ou a quantidade de alguma substância.\\n2. **Fórmula geral de Taylor**: A fórmula geral de Taylor utiliza o número de Euler para expandir funções matemáticas em séries infinitas.\\n3. **Modelagem de processos estocásticos**: O número de Euler é usado em modelos de processos estocásticos, como a simulação de movimentos aleatórios.\\n4. **Aprendizado de Máquina (ML)**: O número de Euler é utilizado em técnicas de ML, como o aprendizado por retropropagação e os algoritmos de optimização.\\n\\nEm particular, o número de Euler é fundamental para:\\n\\n* A modelagem de crescimento exponencial em problemas de regressão e classificação.\\n* A simulação de processos estocásticos em problemas de previsão e predição.\\n* O aprendizado por retropropagação em problemas de visão computacional e linguagem natural.\\n\\nPortanto, o número de Euler é uma ferramenta poderosa para resolver problemas complexos em Matemática e Aprendizado de Máquina.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_llama.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Prompt templates are essential in constructing the list of messages passed to a language model. They act as a bridge between raw user input and the structured data that the language model can process effectively. The primary role of prompt templates is to transform user input into a format that aligns with the application logic and the language model's requirements.\n",
    "\n",
    "Prompt templates take raw user input and apply a series of transformations to create a list of messages suitable for the language model. These transformations can include:\n",
    "\n",
    "1. **Adding a System Message**\n",
    "    - Prompt templates can prepend a system message to the list of messages. This system message provides context, instructions, or guidelines for the language model to follow when generating a response.\n",
    "\n",
    "2. **Formatting a Template**\n",
    "    - User input can be inserted into a predefined template using prompt templates. This template can include placeholders for the user input, along with additional text or instructions that provide structure and context for the language model.\n",
    "\n",
    "#### Benefits of Prompt Templates\n",
    "\n",
    "Using prompt templates offers several advantages:\n",
    "\n",
    "1. **Consistency**\n",
    "    - Prompt templates ensure that the input to the language model follows a consistent format and structure. This consistency helps the language model understand the context and generate more accurate and relevant responses.\n",
    "\n",
    "2. **Reusability**\n",
    "    - Prompt templates can be reused across different user inputs, reducing the need to manually format and structure the input each time. This reusability saves time and effort in constructing the list of messages.\n",
    "\n",
    "3. **Flexibility**\n",
    "    - Prompt templates allow for easy customization and modification of the input format. By adjusting the template, you can experiment with different structures and instructions to improve the language model's performance for specific tasks or domains.\n",
    "\n",
    "#### Using Prompt Templates in LangChain\n",
    "\n",
    "With LangChain's PromptTemplates, you can:\n",
    "\n",
    "1. **Define a Template**\n",
    "    - Create a template with placeholders for user input.\n",
    "\n",
    "2. **Specify the System Message**\n",
    "    - Include any system message or additional instructions.\n",
    "\n",
    "3. **Combine User Input with the Template**\n",
    "    - Easily integrate user input into the template to create a formatted prompt.\n",
    "\n",
    "4. **Pass the Formatted Prompt to the Language Model**\n",
    "    - Send the formatted prompt directly to the language model for processing.\n",
    "\n",
    "With PromptTemplates, you can focus on the application logic and user experience while ensuring that the input to the language model is well-structured and optimized for generating accurate and relevant responses. Incorporating prompt templates into your language model workflow can greatly enhance the quality and efficiency of your application. They provide a flexible and reusable approach to transforming user input, allowing you to focus on building reliable and user-friendly applications powered by language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatPromptTemplate class from the langchain_core.prompts module\n",
    "# This class is used to create templates for chat-based prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a template string for the system message\n",
    "# This template instructs the model to translate the given text into a specified language\n",
    "# The {language} placeholder will be replaced with the target language during execution\n",
    "system_template_text = (\n",
    "    \"Translate the following into {language} and output only the translated text: \"\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate instance using the from_messages method\n",
    "# This method takes a list of tuples, where each tuple represents a message in the chat\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_template_text,\n",
    "        ),  # System message template for translation instruction\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{text}\",\n",
    "        ),  # Human message template with a placeholder for the input text\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Translate the following into Portuguese and output only the translated text: ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Large Language Models are revolutionizing the way we interact with computers.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Invoke the prompt template with specific inputs\n",
    "# The invoke method replaces the placeholders in the template with the provided values\n",
    "# This will format the message according to the template defined earlier\n",
    "formatted_message = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"Portuguese\",  # Target language for translation\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\",  # Text to be translated\n",
    "    }\n",
    ")\n",
    "\n",
    "# The formatted_message now contains the system and human messages with the placeholders replaced by the provided values\n",
    "# This formatted message can be sent to a language model for translation\n",
    "print(formatted_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Portuguese and output only the translated text: ', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Large Language Models are revolutionizing the way we interact with computers.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the formatted message into a list of messages\n",
    "# The to_messages method converts the formatted message into a format suitable for sending to a language model\n",
    "messages = formatted_message.to_messages()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can chain all components together using LCEL (LangChain Expression Language)\n",
    "# This involves chaining the prompt template, model, and output parser\n",
    "\n",
    "# Create a processing chain for the OpenAI model\n",
    "# The chain_openai_2 will first format the input using the prompt_template,\n",
    "# then pass the formatted input to model_openai, and finally parse the model's output using simple_parser\n",
    "chain_openai_2 = prompt_template | model_openai | simple_parser\n",
    "\n",
    "# Same thing for the LLaMA model\n",
    "chain_llama_2 = prompt_template | model_llama | simple_parser\n",
    "\n",
    "# These chains allow for streamlined processing of inputs through the entire pipeline, from prompt formatting to model inference and output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos de Linguagem de Grande Escala estão revolucionando a maneira como interagimos com os computadores.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the processing chain for the OpenAI model with specific inputs\n",
    "# The invoke method will execute the entire chain:\n",
    "# 1. Format the input using the prompt_template\n",
    "# 2. Pass the formatted input to model_openai for translation\n",
    "# 3. Parse the model's output using simple_parser\n",
    "\n",
    "# The input dictionary specifies the target language and the text to be translated\n",
    "# \"language\" is set to \"Portuguese\" to indicate the target language for translation\n",
    "# \"text\" is set to the sentence that needs to be translated\n",
    "\n",
    "translated_text = chain_openai_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Portuguese\",  # Target language for translation\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\",  # Text to be translated\n",
    "    }\n",
    ")\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os Grandes Modelos de Língua estão revolucionando a forma como interagimos com computadores.\n"
     ]
    }
   ],
   "source": [
    "# Same thing for the LLaMA model\n",
    "\n",
    "translated_text = chain_llama_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Portuguese\",\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大型语言模型正在彻底改变我们与计算机的互动方式。\n"
     ]
    }
   ],
   "source": [
    "# Changing the target language to Chinese\n",
    "\n",
    "translated_text = chain_openai_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Chinese\",\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\",\n",
    "    }\n",
    ")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大型语言模型正在改变我们与计算机的交互方式。\n"
     ]
    }
   ],
   "source": [
    "translated_text = chain_llama_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Chinese\",\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\",\n",
    "    }\n",
    ")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From now on, I'll use just the OpenAI model except when the difference between GPT-4o and Llama 3.1 becomes relevant `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the Conversation Flowing\n",
    "\n",
    "In a conversational setting, maintaining a coherent and engaging dialogue is essential for a positive user experience. To achieve this, it's crucial to guide the conversation effectively, ensuring that each turn builds upon the previous one and leads to a meaningful exchange of information. Because LLMs are stateless, developers must manage the conversation flow explicitly to maintain context and coherence throughout the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import message classes from the langchain_core.messages module\n",
    "# HumanMessage: Represents a message from a human user\n",
    "# SystemMessage: Represents a message from the system (e.g., instructions or prompts)\n",
    "# AIMessage: Represents a message from an AI model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "def get_user_message(message_text):\n",
    "    # A function to simulate getting the user input during a conversation\n",
    "    # This function takes a string input (message_text) and returns a HumanMessage object\n",
    "    # The HumanMessage object encapsulates the content of the user's message\n",
    "    return HumanMessage(content=message_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store messages\n",
    "# This list will hold different types of messages (e.g., system, human, AI) in a conversation\n",
    "list_of_messages = []\n",
    "\n",
    "# Append a SystemMessage to the list_of_messages\n",
    "# SystemMessage is used to provide instructions or context to the AI model\n",
    "# Here, the content of the SystemMessage is 'You are a helpful assistant'\n",
    "# This message sets the context for the AI model, indicating that it should behave as a helpful assistant\n",
    "list_of_messages.append(SystemMessage(content=\"You are a helpful assistant\"))\n",
    "\n",
    "# Display the list of messages\n",
    "# At this point, the list contains only one message, the SystemMessage added above\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is Elias', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append a HumanMessage to the list_of_messages\n",
    "# The get_user_message function creates a HumanMessage object from the provided text\n",
    "# Here, the content of the HumanMessage is 'My name is Elias'\n",
    "# This simulates a user inputting their name into the conversation\n",
    "list_of_messages.append(get_user_message(\"My name is Elias\"))\n",
    "\n",
    "# Display the updated list of messages\n",
    "# The list now contains the initial SystemMessage and the new HumanMessage\n",
    "# This helps in tracking the flow of the conversation\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is Elias', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice to meet you, Elias! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 20, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-0a1cf380-6398-4725-8216-b1d6e93acfa0-0')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the OpenAI model with the current list of messages\n",
    "# The model_openai.invoke method processes the list_of_messages and generates a response\n",
    "# The response is stored in the ai_response variable\n",
    "ai_response = model_openai.invoke(list_of_messages)\n",
    "\n",
    "# Append the AI's response to the list_of_messages\n",
    "# The AIMessage object encapsulates the content of the AI's response\n",
    "# This step adds the AI's response to the conversation history, maintaining the sequence of messages\n",
    "list_of_messages.append(ai_response)\n",
    "\n",
    "# Display the updated list of messages\n",
    "# The list now includes the initial SystemMessage, the HumanMessage, and the AIMessage\n",
    "# This helps in tracking the entire conversation flow\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is Elias', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice to meet you, Elias! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 20, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-0a1cf380-6398-4725-8216-b1d6e93acfa0-0'),\n",
       " HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append another HumanMessage to the list_of_messages\n",
    "# This simulates the user asking a follow-up question in the conversation\n",
    "# The get_user_message function creates a HumanMessage object with the content 'Do you remember my name?'\n",
    "# This message is added to the conversation history\n",
    "list_of_messages.append(get_user_message(\"Do you remember my name?\"))\n",
    "\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is Elias', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice to meet you, Elias! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 20, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-0a1cf380-6398-4725-8216-b1d6e93acfa0-0'),\n",
       " HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes, your name is Elias. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 48, 'total_tokens': 63, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-585c590e-27a9-4f01-98f2-bb423135be6f-0')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the OpenAI model with the current list of messages\n",
    "# The model_openai.invoke method processes the list_of_messages and generates a response\n",
    "# The response is stored in the ai_response variable\n",
    "ai_response = model_openai.invoke(list_of_messages)\n",
    "\n",
    "# Append the AI's response to the list_of_messages\n",
    "list_of_messages.append(ai_response)\n",
    "\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don’t have the ability to remember personal information or previous interactions. Each conversation is treated independently. How can I assist you today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to invoke the OpenAI model without passing a list of messages\n",
    "# Here, we directly pass a string \"Do you remember my name?\" to the model_openai.invoke method\n",
    "# This is different from the previous approach where we passed a list of messages\n",
    "\n",
    "response_content = model_openai.invoke(\"Do you remember my name?\").content\n",
    "response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This message history serves several important purposes and has effects for token usage, context length, and pricing.\n",
    "\n",
    "- **Token Count**: Every message in the history contributes to the overall token count. Tokens are the fundamental units used to measure the length of text in AI systems. The more messages included in the history, the higher the token count will be.\n",
    "\n",
    "- **Context Length**: The message history directly affects the context length. AI models have a limited context window, typically measured in tokens. If the message history becomes too long, it may exceed the maximum context length supported by the model. This can lead to truncation or loss of earlier messages in the conversation.\n",
    "\n",
    "- **Token-based Pricing**: Many AI services charge based on the number of tokens processed. The more tokens included in the message history, the higher the cost of each interaction. It's important to consider the trade-off between providing sufficient context and minimizing token usage to manage costs effectively.\n",
    "\n",
    "- **Efficiency**: To perfect pricing, it's crucial to keep the message history concise and relevant. Removing unnecessary or redundant messages can help reduce token usage while still maintaining adequate context for the AI to generate appropriate responses.\n",
    "\n",
    "`But enough with python lists.... Let's see how to manage history properly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message History: Storing and Consuming Conversation Context\n",
    "\n",
    "The Message History class is a powerful tool for creating the illusion of stateful models that maintain conversation context across interactions. By wrapping our model with the Message History class, we can keep track of inputs and outputs, storing them in a datastore for future reference. This allows the model to load and incorporate previous messages as part of the input for each new interaction, enabling more coherent and contextually relevant responses.\n",
    "\n",
    "To use the Message History functionality, we need to set up a chain that wraps the model and incorporates the message history. A crucial component of this setup is the `get_session_history` function, which is passed into the chain. This function takes a `session_id` parameter and is expected to return a Message History object. It should be passed as part of the configuration when calling the new chain. By using unique session IDs, the model can maintain distinct conversation contexts for different users or sessions.\n",
    "\n",
    "> The `session_id` acts as a key to retrieve the appropriate message history for each conversation, ensuring that the model responds based on the correct context.\n",
    "\n",
    "##### Storing and Retrieving Message History\n",
    "\n",
    "When a new interaction occurs, the message history chain:\n",
    "\n",
    "1. Retrieves the message history associated with the provided `session_id`\n",
    "2. Incorporates the previous messages as part of the input to the model\n",
    "3. Generates a response based on the current input and the conversation context\n",
    "4. Stores the new input and output in the datastore, updating the message history for the specific `session_id`\n",
    "\n",
    "This process allows the model to maintain a coherent conversation flow, as it has access to the previous messages and can generate responses that build upon the established context.\n",
    "\n",
    "<br>\n",
    "\n",
    ">\n",
    "> Incorporating message history into our models offers several advantages:\n",
    ">\n",
    "> - **Improved Coherence**: By considering previous messages, the model can generate responses that are more coherent and contextually relevant to the ongoing conversation.\n",
    "> - **Personalization**: Message history enables the model to tailor its responses based on the specific user or session, creating a more personalized experience.\n",
    "> - **Memory Retention**: The model can retain important information mentioned earlier in the conversation, allowing for more natural and informed interactions.\n",
    "> - **Conversation Continuity**: Users can pick up where they left off in a conversation, as the model has access to the full history of the interaction.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets simulate two people, João and Maria, interacting with our LLM\n",
    "import random\n",
    "\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "        # Generate a unique 10-character ID for each person\n",
    "        # This could be used to track individual interactions with the LLM\n",
    "        self.uid = \"\".join(\n",
    "            random.choices(\"abcdefghijklmnopqrstuvwxyz01234567890\", k=10)\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Custom string representation of the Person object\n",
    "        # This will be used when printing the object or using str() function\n",
    "        return f\"Name: {self.name}\\nID: {self.uid}\"\n",
    "\n",
    "\n",
    "# Create instances of the Person class for Maria and João\n",
    "maria = Person(\"Maria\")\n",
    "joao = Person(\"João\")\n",
    "\n",
    "# At this point, maria and joao are objects with unique names and IDs\n",
    "# We can use these objects to simulate different users interacting with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Maria\n",
       "ID: cfvp0xqiz2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: João\n",
       "ID: 6m99vcia60"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Dictionary to store chat histories for different sessions\n",
    "database = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retrieves or creates a chat history for a given session ID.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Unique identifier for the chat session.\n",
    "\n",
    "    Returns:\n",
    "        BaseChatMessageHistory: Chat history object for the session.\n",
    "    \"\"\"\n",
    "    # If the session doesn't exist, create a new InMemoryChatMessageHistory\n",
    "    if session_id not in database:\n",
    "        database[session_id] = InMemoryChatMessageHistory()\n",
    "\n",
    "    # Return the chat history for the session\n",
    "    return database[session_id]\n",
    "\n",
    "\n",
    "# Create a RunnableWithMessageHistory object\n",
    "# This combines the OpenAI model with the ability to manage chat history\n",
    "runnable_with_history = RunnableWithMessageHistory(model_openai, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate João and Maria are talking to the LLM\n",
    "\n",
    "# Configuration for João's session\n",
    "config_joao = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": joao.uid  # Using João's unique identifier for session tracking\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration for Maria's session\n",
    "config_maria = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": maria.uid  # Using Maria's unique identifier for session tracking\n",
    "    }\n",
    "}\n",
    "\n",
    "# These configurations are used to maintain separate conversation contexts\n",
    "# for João and Maria when interacting with the LLM. This allows the LLM\n",
    "# to provide personalized responses based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá, João! Como posso ajudá-lo hoje?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the language model with a specific configuration for João\n",
    "response_joao = runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"Olá, eu sou o João\")], config=config_joao\n",
    ")\n",
    "\n",
    "# Extract and return the content of the response\n",
    "# This is useful for accessing just the generated text without metadata\n",
    "response_joao.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6m99vcia60': InMemoryChatMessageHistory(messages=[HumanMessage(content='Olá, eu sou o João', additional_kwargs={}, response_metadata={}), AIMessage(content='Olá, João! Como posso ajudá-lo hoje?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-ae175201-70b6-44be-a048-881910447b89-0')])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, database is automatically keeping track of the conversation as long as we use the runnable history\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sim, você se apresentou como João. Como posso ajudar você hoje, João?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the runnable_with_history object to invoke a conversation\n",
    "response_joao2 = runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"Você lembra meu nome?\")], config=config_joao\n",
    ")\n",
    "\n",
    "# Note: We only need to pass the current message. The runnable_with_history object automatically includes previous messages.\n",
    "response_joao2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, mas eu não tenho a capacidade de lembrar informações pessoais de interações anteriores. Como posso ajudar você hoje?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the runnable_with_history chain for Maria\n",
    "response_maria = runnable_with_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Você lembra meu nome?\")\n",
    "    ],  # Ask if the AI remembers Maria's name\n",
    "    config=config_maria,  # Use Maria's specific configuration\n",
    ")\n",
    "\n",
    "# The AI won't remember Maria's name since this is the first interaction with her ID\n",
    "\n",
    "# Extract and return the content of the response\n",
    "response_maria.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6m99vcia60': InMemoryChatMessageHistory(messages=[HumanMessage(content='Olá, eu sou o João', additional_kwargs={}, response_metadata={}), AIMessage(content='Olá, João! Como posso ajudá-lo hoje?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-ae175201-70b6-44be-a048-881910447b89-0'), HumanMessage(content='Você lembra meu nome?', additional_kwargs={}, response_metadata={}), AIMessage(content='Sim, você se apresentou como João. Como posso ajudar você hoje, João?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 36, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-fb69a03d-952a-4dc6-9ee6-0cc4d5671279-0')]),\n",
       " 'cfvp0xqiz2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Você lembra meu nome?', additional_kwargs={}, response_metadata={}), AIMessage(content='Desculpe, mas eu não tenho a capacidade de lembrar informações pessoais de interações anteriores. Como posso ajudar você hoje?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 12, 'total_tokens': 38, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-7cd97f25-2c22-4889-923b-50ad5f6de03c-0')])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our `database` to see the conversation history. It should have two keys, one for João and one for Maria, with their UIDs as keys and the history as values.\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components from langchain_core.prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create a ChatPromptTemplate with a system message and a placeholder for chat history\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message to set the behavior of the AI\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You always answer in English, even if the user wrote in a different language\",\n",
    "        ),\n",
    "        # Placeholder for the chat history. This allows for including previous messages in the conversation\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain by combining the prompt template with an OpenAI model\n",
    "# The '|' operator is used to chain these components together\n",
    "chain = prompt | model_openai\n",
    "\n",
    "# The 'chain' variable now represents a complete conversation flow:\n",
    "# 1. It starts with the defined prompt (including system message and history)\n",
    "# 2. Then passes the formatted prompt to the OpenAI model for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history_and_prompt = RunnableWithMessageHistory(\n",
    "    chain, get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your last message was asking if I remembered your name. Would you like to discuss something specific?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the runnable_with_history_and_prompt object to invoke a response\n",
    "response_joao3 = runnable_with_history_and_prompt.invoke(\n",
    "    # Pass a list containing a single HumanMessage object\n",
    "    # This represents the user's input/question\n",
    "    [HumanMessage(content=\"Qual foi a minha última mensagem?\")],\n",
    "    # Use the config_joao configuration\n",
    "    config=config_joao,\n",
    ")\n",
    "\n",
    "# Extract and return the content of the response\n",
    "response_joao3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more complex prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message to set the language behavior\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You always answer in {language}, even if the user wrote in a different language\",\n",
    "        ),\n",
    "        # Placeholder for user messages\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain by combining the prompt with the OpenAI model\n",
    "# The '|' operator is used to connect the prompt and model in a pipeline\n",
    "chain = prompt | model_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RunnableWithMessageHistory\n",
    "# This class is used to manage the execution of a chain with message history and variable prompts\n",
    "\n",
    "# Parameters:\n",
    "# - chain: The processing chain that will be executed. This chain typically includes the prompt template, model, and output parser.\n",
    "# - get_session_history: A function that retrieves the session history (i.e., the list of previous messages in the conversation).\n",
    "# - input_messages_key: The key used to access the input messages in the session history. Here, it is set to \"messages\".\n",
    "\n",
    "# This setup allows the chain to be executed with the context of previous messages, ensuring that the AI model has the necessary context to generate accurate responses.\n",
    "runnable_with_history_and_prompt_with_variables = RunnableWithMessageHistory(\n",
    "    chain,  # The processing chain to be executed\n",
    "    get_session_history,  # Function to retrieve the session history\n",
    "    input_messages_key=\"messages\",  # Key to access input messages in the session history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vous avez demandé si je me souvenais de votre nom. Voudriez-vous discuter de quelque chose de précis ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with message history and variable prompts\n",
    "# This method call will execute the chain with the provided inputs and configuration\n",
    "\n",
    "# Parameters:\n",
    "# - The first argument is a dictionary containing:\n",
    "#   - \"language\": The target language for the response, set to \"French\" in this case.\n",
    "#   - \"messages\": A list of messages to be processed. Here, it includes a HumanMessage asking \"Pode repetir sua última mensagem?\".\n",
    "# - The second argument is the configuration for the invocation, specified by config_joao.\n",
    "\n",
    "# The chain will use the provided messages and configuration to generate a response, taking into account the session history and the specified language.\n",
    "\n",
    "response_joao4 = runnable_with_history_and_prompt_with_variables.invoke(\n",
    "    {\n",
    "        \"language\": \"French\",  # Target language for the response\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Pode repetir sua última mensagem?\")\n",
    "        ],  # List of messages to be processed\n",
    "    },\n",
    "    config=config_joao,  # Configuration with ID\n",
    ")\n",
    "\n",
    "# Access and display the content of the response\n",
    "# The response_joao4 object contains the AI's reply, and the content attribute holds the actual text of the response\n",
    "response_joao4.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certo! La tua ultima domanda è stata se potevo ripetere la mia ultima risposta. Come posso aiutarti ulteriormente?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing for Italian\n",
    "\n",
    "response_joao5 = runnable_with_history_and_prompt_with_variables.invoke(\n",
    "    {\n",
    "        \"language\": \"Italian\",  # Target language for the response\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Pode repetir sua última mensagem?\")\n",
    "        ],  # List of messages to be processed\n",
    "    },\n",
    "    config=config_joao,  # Configuration with ID\n",
    ")\n",
    "\n",
    "response_joao5.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6m99vcia60': InMemoryChatMessageHistory(messages=[HumanMessage(content='Olá, eu sou o João', additional_kwargs={}, response_metadata={}), AIMessage(content='Olá, João! Como posso ajudá-lo hoje?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-ae175201-70b6-44be-a048-881910447b89-0'), HumanMessage(content='Você lembra meu nome?', additional_kwargs={}, response_metadata={}), AIMessage(content='Sim, você se apresentou como João. Como posso ajudar você hoje, João?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 36, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-fb69a03d-952a-4dc6-9ee6-0cc4d5671279-0'), HumanMessage(content='Qual foi a minha última mensagem?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your last message was asking if I remembered your name. Would you like to discuss something specific?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 86, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-a66c6426-cf9e-4930-b037-3f4b99fa07fa-0'), HumanMessage(content='Pode repetir sua última mensagem?', additional_kwargs={}, response_metadata={}), AIMessage(content='Vous avez demandé si je me souvenais de votre nom. Voudriez-vous discuter de quelque chose de précis ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 119, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-3b8670f8-10c5-4d01-8d22-bfb4f26b6bde-0'), HumanMessage(content='Pode repetir sua última mensagem?', additional_kwargs={}, response_metadata={}), AIMessage(content='Certo! La tua ultima domanda è stata se potevo ripetere la mia ultima risposta. Come posso aiutarti ulteriormente?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 156, 'total_tokens': 184, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-2a860dd1-665a-45bf-94d2-4ee15281cb70-0')]),\n",
       " 'cfvp0xqiz2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Você lembra meu nome?', additional_kwargs={}, response_metadata={}), AIMessage(content='Desculpe, mas eu não tenho a capacidade de lembrar informações pessoais de interações anteriores. Como posso ajudar você hoje?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 12, 'total_tokens': 38, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-7cd97f25-2c22-4889-923b-50ad5f6de03c-0')])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 6m99vcia60\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Olá, eu sou o João\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Olá, João! Como posso ajudá-lo hoje?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Você lembra meu nome?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Sim, você se apresentou como João. Como posso ajudar você hoje, João?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Qual foi a minha última mensagem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your last message was asking if I remembered your name. Would you like to discuss something specific?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Pode repetir sua última mensagem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Vous avez demandé si je me souvenais de votre nom. Voudriez-vous discuter de quelque chose de précis ?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Pode repetir sua última mensagem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Certo! La tua ultima domanda è stata se potevo ripetere la mia ultima risposta. Come posso aiutarti ulteriormente?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "User: cfvp0xqiz2\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Você lembra meu nome?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Desculpe, mas eu não tenho a capacidade de lembrar informações pessoais de interações anteriores. Como posso ajudar você hoje?\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in database.items():\n",
    "    print(f\"User: {k}\")\n",
    "    for message in v.messages:\n",
    "        message.pretty_print()\n",
    "\n",
    "    print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that, even if I am using a simple python dictionary as database, LangChain has [built-in support for several databases](https://python.langchain.com/v0.2/docs/integrations/memory/), including Redis, MongoDB, and PostgreSQL. This allows you to store and retrieve message history data efficiently and securely, ensuring that the conversation context is preserved across interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- LLMs represent a significant advancement in AI and NLP, with the potential to transform various industries by enabling more natural and efficient human-machine interactions.\n",
    "\n",
    "- Understanding the architecture, training data, and evolution of LLMs is essential for utilizing their capabilities effectively and keeping pace with the rapidly advancing field.\n",
    "\n",
    "- Prompt engineering plays a vital role in improving LLM performance, requiring careful design and refinement of text inputs to elicit accurate and relevant responses.\n",
    "\n",
    "- Platforms like LangChain and LlamaIndex simplify the process of integrating LLMs into applications, offering a range of features and tools to suit different use cases and requirements.\n",
    "\n",
    "- Managing the stateless nature of LLMs is a key challenge for developers, necessitating the use of techniques like prompt engineering, external memory, and stateful wrappers to maintain conversation context and coherence.\n",
    "\n",
    "- Continuously exploring and applying techniques to enhance LLM performance, such as few-shot learning, chain-of-thought prompting, and interactive feedback loops, is crucial for extending the limits of what LLMs can achieve in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What are Large Language Models (LLMs) and why are they significant in AI and NLP?\n",
    "\n",
    "2. How do LLMs understand and generate human-like text?\n",
    "\n",
    "3. What are some key milestones in the evolution of LLMs?\n",
    "\n",
    "4. What is the Transformer architecture, and why is it important in the development of LLMs?\n",
    "\n",
    "5. How do foundation models differ from traditional AI models?\n",
    "\n",
    "6. What is prompt engineering, and why is it important when working with LLMs?\n",
    "\n",
    "7. What are LangChain and LlamaIndex, and how do they ease interaction with LLMs?\n",
    "\n",
    "8. Why are LLMs considered stateless, and what are the effects of this characteristic?\n",
    "\n",
    "9. How can developers manage conversation flow and message history when working with stateless LLMs?\n",
    "\n",
    "10. What are some techniques to enhance the performance and relevance of LLM-generated responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "\n",
    "<!--\n",
    "\n",
    "1. Large Language Models are advanced AI systems trained on extensive text datasets to understand and generate human-like text. They are significant because they can perform various language-related tasks with high proficiency, transforming human-machine interactions and offering new opportunities in fields like customer service, healthcare, education, and creative writing.\n",
    "\n",
    "2. LLMs are trained on enormous datasets containing billions of words, which allows them to learn language complexities such as grammar, syntax, and semantics. They use deep learning techniques and vast numbers of parameters to generate text that closely resembles human language.\n",
    "\n",
    "3. Key milestones include:\n",
    "- **Transformer Architecture (2017):** Revolutionized NLP by allowing models to process input sequences in parallel.\n",
    "- **GPT (Generative Pre-trained Transformer) (2018):** Demonstrated the potential of unsupervised pre-training.\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers) (2018):** Introduced bidirectional training for better context understanding.\n",
    "- **GPT-3 (2020):** Showcased the power of scaling up language models with 175 billion parameters.\n",
    "\n",
    "4. The Transformer architecture, introduced in 2017 by Vaswani et al., uses attention mechanisms to process input sequences in parallel, improving training speed and performance. It is important because it laid the foundation for many subsequent LLMs, enabling them to handle large-scale data and complex language tasks more efficiently.\n",
    "\n",
    "5. Foundation models are trained on large, diverse datasets and acquire general knowledge through self-supervised learning. Unlike traditional AI models, which are trained on specific tasks with labeled data, foundation models can perform a wide array of tasks with minimal fine-tuning, making them more versatile and powerful.\n",
    "\n",
    "6. Prompt engineering involves designing and refining text inputs to guide LLMs to produce accurate and contextually appropriate responses. It is important because the quality of the prompt directly impacts the quality of the AI's output, making it a critical skill for enhancing the performance of LLMs.\n",
    "\n",
    "7. LangChain and LlamaIndex are platforms that simplify the process of interacting with LLMs. LangChain offers continuous assimilation, an instinctive API, and task-specific modules for various language tasks. LlamaIndex focuses on indexing and querying large document collections, using LLMs for semantic search and query optimization.\n",
    "\n",
    "8. LLMs are stateless because they do not maintain an internal state or memory across different interactions. Each input is processed independently, which means they cannot retain context from previous exchanges. This characteristic requires developers to explicitly manage conversation flow and message history to maintain coherence and context in interactions.\n",
    "\n",
    "9. Developers can manage conversation flow and message history by using techniques such as:\n",
    "- **Prompt Engineering:** Crafting prompts that include relevant context.\n",
    "- **External Memory Systems:** Storing and retrieving relevant information.\n",
    "- **Stateful Wrappers:** Managing conversation flow and maintaining context.\n",
    "- **Message History Classes:** Using tools like LangChain's Message History class to store and incorporate previous messages into each new interaction.\n",
    "\n",
    "10. Techniques to enhance LLM performance include:\n",
    "- **Using Prompt Templates:** Ensuring consistent and structured input.\n",
    "- **Few-Shot and Zero-Shot Learning:** Providing examples or task descriptions.\n",
    "- **Chain-of-Thought Prompting:** Guiding the model through a step-by-step reasoning process.\n",
    "- **Role Assignment:** Assigning specific roles to the model for context.\n",
    "- **Interactive Prompts and Feedback Loops:** Engaging in iterative refinement of prompts and responses. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imd1107-nlp-1oxVnwDa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
