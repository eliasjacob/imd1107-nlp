{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "## IMD1107 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](htttps://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Keypoints\n",
    "\n",
    "- Large Language Models (LLMs) are advanced AI systems trained on vast text datasets to understand and generate human-like text, revolutionizing human-machine interactions across various industries.\n",
    "\n",
    "- LLMs learn language nuances like grammar, syntax, and semantics through deep learning techniques and billions of parameters, enabling them to generate text that closely resembles human language.\n",
    "\n",
    "- The evolution of LLMs includes milestones such as the Transformer architecture, GPT, BERT, and GPT-3, each advancing the field in terms of performance, efficiency, and scale.\n",
    "\n",
    "- Foundation models are trained on large, diverse datasets and acquire general knowledge through self-supervised learning, making them versatile and capable of performing a wide array of tasks with minimal fine-tuning.\n",
    "\n",
    "- Prompt engineering is crucial for guiding LLMs to produce accurate and contextually appropriate responses by designing and refining text inputs.\n",
    "\n",
    "- LangChain and LlamaIndex are platforms that simplify interaction with LLMs, offering features like seamless integration, intuitive APIs, task-specific modules, and indexing capabilities for large document collections.\n",
    "\n",
    "- LLMs are stateless, meaning they do not maintain context across interactions, requiring developers to explicitly manage conversation flow and message history to ensure coherence.\n",
    "\n",
    "- Techniques like prompt engineering, external memory systems, stateful wrappers, and message history classes help mitigate the challenges posed by the stateless nature of LLMs.\n",
    "\n",
    "- Enhancing LLM performance involves strategies such as using prompt templates, few-shot and zero-shot learning, chain-of-thought prompting, role assignment, and interactive prompts with feedback loops.\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "- LLMs represent a significant advancement in AI and NLP, with the potential to transform various industries by enabling more natural and efficient human-machine interactions.\n",
    "\n",
    "- Understanding the architecture, training data, and evolution of LLMs is essential for leveraging their capabilities effectively and keeping pace with the rapidly advancing field.\n",
    "\n",
    "- Prompt engineering plays a vital role in optimizing LLM performance, requiring careful design and refinement of text inputs to elicit accurate and relevant responses.\n",
    "\n",
    "- Platforms like LangChain and LlamaIndex streamline the process of integrating LLMs into applications, offering a range of features and tools to suit different use cases and requirements.\n",
    "\n",
    "- Managing the stateless nature of LLMs is a key challenge for developers, necessitating the use of techniques like prompt engineering, external memory, and stateful wrappers to maintain conversation context and coherence.\n",
    "\n",
    "- Continuously exploring and applying techniques to enhance LLM performance, such as few-shot learning, chain-of-thought prompting, and interactive feedback loops, is crucial for pushing the boundaries of what LLMs can achieve in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models\n",
    "\n",
    "Large Language Models (LLMs) represent a significant advancement in artificial intelligence (AI) and natural language processing (NLP). By being trained on extensive text datasets, these models can generate human-like text, understand context, and perform various language-related tasks with exceptional proficiency. The emergence of LLMs has introduced new opportunities in AI, potentially transforming human-machine interactions.\n",
    "\n",
    "\n",
    "## Why Large Language Models Matter\n",
    "\n",
    "LLMs have the potential to revolutionize various industries and transform the way we interact with machines. Here are some key reasons why LLMs matter:\n",
    "\n",
    "1. **Human-Like Language Understanding**: LLMs can comprehend and generate human-like text, enabling more natural and intuitive communication between humans and machines. This opens up new possibilities for conversational AI, chatbots, and virtual assistants.\n",
    "\n",
    "2. **Versatility**: LLMs can perform a wide range of language tasks, such as translation, summarization, question answering, and content generation, with minimal fine-tuning. This versatility makes them valuable tools for businesses and researchers across different domains.\n",
    "\n",
    "3. **Knowledge Acquisition**: LLMs are trained on vast amounts of diverse data, allowing them to acquire a broad range of knowledge and understand complex concepts. This knowledge can be leveraged to provide accurate and informative responses to user queries.\n",
    "\n",
    "4. **Efficiency and Scalability**: LLMs can process and generate text much faster than humans, making them suitable for handling large-scale language tasks. They can be deployed at scale to serve millions of users simultaneously, enabling efficient and cost-effective language processing solutions.\n",
    "\n",
    "5. **Innovation and Creativity**: LLMs have the potential to spark innovation and creativity by assisting humans in generating ideas, writing content, and solving complex problems. They can increase human intelligence and help unlock new possibilities in fields like research, education, and the arts.\n",
    "\n",
    "6. **Industry Applications**: LLMs are already being used in various industries, including customer service, healthcare, finance, and marketing. They can automate repetitive tasks, improve decision-making processes, and enhance user experiences, leading to increased efficiency and productivity.\n",
    "\n",
    "\n",
    "## Definition and Importance\n",
    "\n",
    "Large Language Models are AI systems that utilize deep learning to process and generate human language. They are trained on enormous datasets containing billions of words, enabling them to grasp language nuances like grammar, syntax, and semantics. The \"large\" in LLMs refers to their vast number of parameters, often numbering in the billions.\n",
    "\n",
    "**Importance:**\n",
    "- **Understanding and Generating Text:** LLMs can understand and produce text that closely resembles human writing.\n",
    "- **Versatility:** They are useful in various applications, such as language translation, text summarization, question answering, and content generation.\n",
    "- **Industry Impact:** LLMs can significantly impact sectors like customer service, healthcare, education, and creative writing.\n",
    "\n",
    "## History and Evolution\n",
    "\n",
    "The development of LLMs has been a progressive journey, with each iteration improving upon previous models. Key milestones include:\n",
    "\n",
    "- **Transformer Architecture (2017):** Introduced by Vaswani et al., this architecture revolutionized NLP by allowing models to process input sequences in parallel, improving training speed and performance.\n",
    "- **GPT (Generative Pre-trained Transformer) (2018):** Developed by OpenAI, GPT was one of the first large-scale models to demonstrate the potential of unsupervised pre-training on diverse text data.\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers) (2018):** Google's BERT model introduced bidirectional training, enabling the model to learn from both left and right contexts simultaneously, enhancing language understanding.\n",
    "- **GPT-3 (2020):** OpenAI's GPT-3, with 175 billion parameters, showcased the power of scaling up language models, demonstrating remarkable language generation capabilities and the ability to perform tasks with minimal fine-tuning.\n",
    "\n",
    "The evolution continues with even larger and more powerful models being developed, and anything I try to write here will become obsolete way too fast for me to keep up with the pace of the field in a jupyter notebook.\n",
    "\n",
    "## The Impact of Large Language Models\n",
    "Large Language Models have significantly impacted various industries and applications. Some key areas where LLMs are making a difference include:\n",
    "\n",
    "1. **Natural Language Processing**: LLMs have revolutionized tasks like machine translation, text summarization, and sentiment analysis.\n",
    "2. **Content Creation**: Automated content generation for marketing, journalism, and creative writing.\n",
    "3. **Customer Service**: Powering chatbots and virtual assistants for improved customer interactions.\n",
    "4. **Healthcare**: Assisting in medical research, diagnosis, and patient communication.\n",
    "5. **Education**: Personalized tutoring and adaptive learning systems.\n",
    "\n",
    "## Challenges and Limitations of Large Language Models\n",
    "While LLMs have shown remarkable capabilities, they also face several challenges and limitations:\n",
    "\n",
    "1. **Bias and Fairness**: LLMs can perpetuate or enhance biases present in their training data.\n",
    "2. **Hallucination**: Generation of plausible-sounding but factually incorrect information.\n",
    "3. **Lack of Common Sense Reasoning**: Difficulty in understanding context and making logical inferences.\n",
    "4. **Computational Resources**: Training and running large models require significant computational power.\n",
    "5. **Privacy Concerns**: Potential misuse of personal information in training data.\n",
    "\n",
    "> **Note:** The emergence of Large Language Models is a significant milestone in AI and NLP. Their ability to understand and generate human-like text has the potential to transform human-machine interactions and revolutionize various industries. As research progresses, LLMs are expected to become even more powerful and versatile, pushing the boundaries of AI and language capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation Models\n",
    "\n",
    "Foundation models represent a class of AI models trained on extensive and diverse datasets, enabling them to perform a wide array of tasks with minimal fine-tuning. Unlike traditional models, which are trained on specific tasks using labeled data, foundation models acquire general knowledge and patterns from unlabeled data through self-supervised learning.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/fms.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The architecture of foundation models is grounded in the transformer model, which has significantly impacted natural language processing (NLP) and other domains. Key components of the transformer architecture include:\n",
    "\n",
    "- **Attention Mechanisms:** These mechanisms allow the model to focus on relevant parts of the input when processing each element, enabling it to capture long-range dependencies and context.\n",
    "- **Encoders:** The encoder layers process the input sequence and generate hidden representations that capture the meaning and context of each element.\n",
    "- **Decoders:** The decoder layers take the encoded representations and generate the output sequence, attending to relevant parts of the input.\n",
    "\n",
    "The transformer architecture enables foundation models to process and generate sequential data efficiently, making them suitable for a wide range of tasks.\n",
    " \n",
    "## Training Data\n",
    "\n",
    "A critical factor in the success of foundation models is the utilization of large-scale datasets. These models are trained on massive amounts of diverse data, often spanning multiple domains and modalities. The training data can include:\n",
    "\n",
    "- Unstructured text from web pages, books, and articles\n",
    "- Structured data from databases and knowledge bases\n",
    "- Images, videos, and audio data\n",
    "- Code snippets and programming language data\n",
    "\n",
    "By training on such diverse data, foundation models can capture a broad range of knowledge and develop a deep understanding of language, concepts, and relationships.\n",
    "\n",
    "## Pre-training\n",
    "\n",
    "Foundation models undergo an unsupervised pre-training phase, where they learn general knowledge and patterns from the training data without explicit supervision. During pre-training, the models are typically trained on tasks such as:\n",
    "\n",
    "- **Language Modeling:** Predicting the next word or token in a sequence.\n",
    "- **Masked Language Modeling:** Predicting masked or hidden tokens in a sequence.\n",
    "- **Contrastive Learning:** Learning to distinguish positive examples from negative ones.\n",
    "\n",
    "Through pre-training, foundation models develop a rich understanding of language and can capture complex relationships and patterns in the data.\n",
    "\n",
    "## Reinforcement Learning in ChatGPT and Similar Models\n",
    "\n",
    "In addition to pre-training, models like ChatGPT often undergo a fine-tuning phase that includes reinforcement learning to enhance their performance and alignment with user expectations. This phase typically involves the following steps:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/rlhf.jpg\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "1. **Supervised Fine-Tuning:** The model is first fine-tuned on a dataset curated by human annotators. This dataset includes examples of desired behavior, guiding the model towards generating more appropriate and relevant responses.\n",
    "\n",
    "2. **Reinforcement Learning from Human Feedback (RLHF):** This further refines the model's behavior by incorporating feedback from human users. The process involves:\n",
    "    - **Collecting Feedback:** Human users interact with the model and provide feedback on its responses, indicating preferences or highlighting issues.\n",
    "    - **Training a Reward Model:** A separate model is trained to predict the quality of responses based on the collected feedback. This reward model assigns scores to the generated responses.\n",
    "    - **Optimizing the Policy:** The main model is then fine-tuned using reinforcement learning techniques, optimizing its responses to maximize the reward scores predicted by the reward model.\n",
    "\n",
    "Through RLHF, models like ChatGPT can better align with human preferences, generating more useful, accurate, and contextually appropriate responses.\n",
    "\n",
    "## Versatility and Adaptation\n",
    "\n",
    "These foundation models have significantly advanced the field of AI. They have been adapted and fine-tuned for numerous downstream tasks, demonstrating their versatility and effectiveness. The ability to generalize from vast amounts of data and adapt to specific tasks with minimal additional training is what makes foundation models particularly powerful.\n",
    "\n",
    "> **Note:** The versatility of foundation models stems from their ability to generalize across different types of data and tasks, making them a cornerstone of modern AI research and application.\n",
    "> By adhering to these principles, foundation models continue to push the boundaries of what is possible in AI, offering robust solutions across various domains and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Foundation Models\n",
    "\n",
    "## Pretrained Language Models (PLMs)\n",
    "\n",
    "Pretrained Language Models (PLMs) are neural networks that undergo extensive pretraining on vast amounts of publicly available text data, such as web pages, books, and articles. The primary goal of this pretraining process is to enable the model to learn and understand the elaborate semantics and nuances of natural language. Some notable early examples of PLMs include **ELMo**, **BERT**, and **RoBERTA**. These models are trained using objectives like predicting masked words within a given text, allowing them to develop a deep understanding of language patterns and relationships. PLMs typically consist of hundreds of millions of parameters, which gives them the capacity to capture complex linguistic knowledge.\n",
    "\n",
    "### Adapting PLMs to Downstream Tasks\n",
    "Once pretrained, PLMs can be adapted to perform specific downstream tasks through the following methods:\n",
    "\n",
    "- **Task-Specific Prediction Layer**: A task-specific prediction layer, such as a classification or regression layer, is added on top of the pretrained model. This allows the model to utilize its learned language understanding to make predictions tailored to the specific task at hand.\n",
    "\n",
    "- **Task-Specific Fine-Tuning**: During the fine-tuning process, all the weights of the pretrained model are updated using task-specific training data. This fine-tuning step helps the model to further specialize its knowledge and skills for the particular downstream application, optimizing its performance on that task.\n",
    "\n",
    "By adapting PLMs to downstream tasks, we can utilize the power of their extensive pretraining and apply it to a wide range of natural language processing applications, such as sentiment analysis, named entity recognition, and question answering.\n",
    "\n",
    "## The Rise of Large Autoregressive Language Models\n",
    "\n",
    "In 2020, the introduction of **GPT-3** marked a significant milestone in the field of machine learning, ushering in a new class of large-scale language models known as autoregressive language models. Unlike earlier PLMs that focused on predicting masked words, autoregressive models are pretrained to predict the next word in a sequence based on the preceding context. This pretraining objective makes them particularly well-suited for language generation tasks, such as question answering, text completion, and summarization.\n",
    "\n",
    "One of the key characteristics of GPT-3 and its successors is their massive scale, with models boasting billions of parameters. This substantial increase in model capacity allows them to capture and generate language with unprecedented fluency and coherence, pushing the boundaries of what is possible with language models.\n",
    "\n",
    "More recently, **GPT-4**, the successor to GPT-3, has further pushed the limits of language modeling. With an even larger parameter count and advanced training techniques, GPT-4 demonstrates remarkable capabilities across a wide range of tasks, including few-shot learning, multi-modal understanding, and reasoning. Its performance on benchmarks and real-world applications has set new standards in the field of natural language processing.\n",
    "\n",
    "## Emergent Behaviors and Few-Shot Learning\n",
    "\n",
    "One of the most fascinating aspects of large autoregressive models like GPT-3 and GPT-4 is their ability to solve a wide range of natural language tasks with minimal examples or even just a task description. This capability is known as **few-shot learning** or **in-context learning**.\n",
    "\n",
    "- **Few-Shot Prompting**: In this approach, the model is provided with a small number of examples (typically fewer than 10) demonstrating the desired task. The model then leverages its extensive pretraining to \"locate\" the relevant knowledge and skills needed to perform the task, without requiring any updates to its parameters.\n",
    "\n",
    "- **Zero-Shot Prompting**: In some cases, these models have shown the ability to perform tasks with just a task description, without any examples at all. For instance, the model can translate between languages or perform arithmetic operations simply by being prompted with a natural language instruction.\n",
    "\n",
    "The emergence of few-shot and zero-shot learning capabilities in large autoregressive models has opened up new possibilities for natural language processing. These models can adapt to a wide range of tasks that differ significantly from their pretraining objectives, showcasing a level of generalization and versatility that was previously unattainable.\n",
    "\n",
    "## The Role of Smaller Models and Fine-Tuning\n",
    "\n",
    "While large autoregressive models like GPT-3 and GPT-4 have demonstrated impressive few-shot learning capabilities, smaller models with fewer parameters often require more specialized adaptation to achieve optimal performance on downstream tasks. For these models, task-specific fine-tuning remains an important technique for aligning their capabilities with the specific requirements of the target application.\n",
    "\n",
    "Fine-tuning allows smaller models to capitalize on their pretraining while also benefiting from exposure to task-specific data, enabling them to develop a more targeted understanding of the problem at hand. This process can help bridge the performance gap between smaller models and their larger counterparts, making them viable options for a wide range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of Foundation Models\n",
    "\n",
    "Foundation models are inherently versatile and multi-purpose, making them powerful tools for a wide range of applications. However, to effectively utilize these models for specific use cases, some form of adaptation is necessary. This process of adaptation can range from simple task specification to more extensive domain specialization, depending on the requirements and available resources.\n",
    "\n",
    "## Methods of Adaptation\n",
    "\n",
    "There are several methods to adapt foundation models, each offering different trade-offs between the costs of adaptation and the extent of model specialization:\n",
    "\n",
    "1. **Prompting**\n",
    "    - Prompting involves providing specific instructions or examples to guide the model's responses.\n",
    "    - It is a cost-effective and straightforward method, making it accessible to a wider range of users.\n",
    "    - However, prompting may not yield the highest performance compared to more resource-intensive adaptation methods.\n",
    "\n",
    "2. **In-Context Learning**\n",
    "    - In this method, the model is given a few examples of the task during inference.\n",
    "    - The goal is to improve the model's performance on that specific task without changing the basic model parameters.\n",
    "    - In-context learning is particularly useful when task-specific data is scarce, as it allows the model to learn from a limited number of examples.\n",
    "\n",
    "3. **Fine-Tuning**\n",
    "    - Fine-tuning involves adjusting the model parameters by training on a specific dataset.\n",
    "    - This method can significantly improve model performance, as it allows the model to specialize in the domain of interest.\n",
    "    - However, fine-tuning is more resource-intensive compared to prompting or in-context learning, as it requires retraining the model on a new dataset.\n",
    "\n",
    "4. **Low-Rank Adaptation (LoRA)**\n",
    "    - LoRA is a technique that involves learning low-rank updates to the model's weights.\n",
    "    - By focusing on low-rank updates, LoRA can be more computationally efficient compared to full fine-tuning.\n",
    "    - This method strikes a balance between the performance gains of fine-tuning and the efficiency of less resource-intensive adaptation methods.\n",
    "\n",
    "## Key Considerations for Adaptation\n",
    "\n",
    "When adapting a foundation model, several key considerations must be taken into account to ensure effective and efficient specialization:\n",
    "\n",
    "1. **Compute Budget**\n",
    "    - Foundation models can be incredibly large, often containing up to trillions of parameters.\n",
    "    - Adapting the entire model can be computationally expensive, requiring significant time and resources.\n",
    "    - To save time and computational resources, developers may opt to adapt only the last neural layer or the bias vectors.\n",
    "    - This strategy allows for more efficient adaptation while still achieving a degree of specialization.\n",
    "\n",
    "> **Note:** Adapting only the bias vectors or the last neural layer is a strategy to reduce computational costs. However, it's important to keep in mind that this approach may limit the extent of specialization achievable compared to adapting the entire model.\n",
    "\n",
    "2. **Data Availability**\n",
    "    - For niche applications or specific domains, the availability of relevant data may be limited.\n",
    "    - In such cases, adapting the foundation model sufficiently can be challenging, as it requires a sufficient amount of task-specific data.\n",
    "    - When specific data is not readily available, it may be necessary to manually label data or seek out domain-specific datasets.\n",
    "    - This process can be costly and may require expert knowledge to ensure the quality and relevance of the labeled data.\n",
    "\n",
    "## Addressing Potential Questions and Misconceptions\n",
    "\n",
    "1. **Why not just use the foundation model as-is?**\n",
    "    - While foundation models are powerful and versatile, they are designed to be generic and applicable to a wide range of tasks.\n",
    "    - Without adaptation, their performance on specific tasks may not be optimal, as they lack the specialization required for the domain of interest.\n",
    "    - Adapting the foundation model allows for tailoring its capabilities to the specific requirements of the task at hand, leading to improved performance and usability.\n",
    "\n",
    "2. **Is fine-tuning always better than prompting?**\n",
    "    - Not necessarily. The choice between fine-tuning and prompting depends on various factors, such as the available resources, the complexity of the task, and the desired level of specialization.\n",
    "    - Fine-tuning can provide better performance, as it allows the model to specialize in the domain of interest. However, it comes at a higher computational cost and requires a sufficient amount of task-specific data.\n",
    "    - Prompting, on the other hand, is less resource-intensive and can be sufficient for certain tasks, especially when computational resources are limited, or task-specific data is scarce.\n",
    "\n",
    "3. **What if the task requires very specific knowledge?**\n",
    "    - In cases where the task requires highly specific or domain-specific knowledge, adapting the foundation model may be more challenging.\n",
    "    - To effectively fine-tune the model, you may need to manually label data or seek out domain-specific datasets that capture the nuances and details of the task at hand.\n",
    "    - This process can be time-consuming and may require collaboration with domain experts to ensure the quality and relevance of the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Explanations: Prompt Engineering\n",
    "\n",
    "Prompt engineering is a critical aspect of working with AI language models, as it directly influences the quality and relevance of the generated responses. By carefully designing and refining prompts, users can guide the AI to produce more accurate, useful, and contextually appropriate outputs. This practice involves considering various factors such as clarity, specificity, and context to create prompts that effectively communicate the desired task to the AI.\n",
    "\n",
    "## Designing Effective Prompts\n",
    "To create prompts that elicit the best possible responses from AI language models, several key considerations should be kept in mind:\n",
    "\n",
    "1. **Clarity**\n",
    "    - Ensure that the prompt is clear and unambiguous, using straightforward language and avoiding complex sentence structures.\n",
    "    - Ambiguous or confusing prompts can lead to misinterpretation by the AI, resulting in irrelevant or inaccurate responses.\n",
    "\n",
    "2. **Specificity**\n",
    "    - Be specific about the task you want the AI to perform, providing detailed instructions and requirements.\n",
    "    - Vague or open-ended prompts may result in incomplete or off-topic responses, as the AI lacks clear guidance on what is expected.\n",
    "\n",
    "3. **Context**\n",
    "    - Provide sufficient context within the prompt to help the AI understand the background and scope of the task.\n",
    "    - Include relevant details, constraints, or examples that can guide the AI's response and ensure it stays within the desired boundaries.\n",
    "\n",
    "*Example Analogy:* Designing a prompt is similar to giving instructions to a colleague. Just as you would provide clear, detailed, and context-rich instructions to ensure your colleague understands and completes the task effectively, the same approach should be applied when crafting prompts for AI language models.\n",
    "\n",
    "## Types of Prompts\n",
    "Depending on the desired outcome and the nature of the task, different types of prompts can be employed:\n",
    "\n",
    "1. **Zero-Shot Prompts**\n",
    "    - Zero-shot prompts do not provide any examples to the AI, requiring it to generate a response based solely on the instructions given in the prompt.\n",
    "    - These prompts test the AI's ability to generalize and apply its knowledge to new situations without explicit guidance.\n",
    "\n",
    "> *Example:* \"Translate the following sentence into Portuguese: 'Hello, how are you?'\"\n",
    "\n",
    "2. **Few-Shot Prompts**\n",
    "    - Few-shot prompts include a small number of examples within the prompt itself to demonstrate the desired pattern or structure of the response.\n",
    "    - By providing a few relevant examples, users can help the AI understand the specific requirements and expectations for the task at hand.\n",
    "\n",
    "> *Example:* \"Translate the following sentences into Portuguese. 'Hello, how are you?' -> 'Olá, como vai você?' 'Good morning.' -> 'Bom dia' Now, translate: 'Good night.'\"\n",
    "\n",
    "3. **Multi-Step Prompts**\n",
    "    - Multi-step prompts involve breaking down a complex task into smaller, more manageable steps.\n",
    "    - By guiding the AI through a series of sequential sub-tasks, users can help the AI tackle detailed problems more effectively.\n",
    "    - This approach allows for a more structured and controlled interaction with the AI, ensuring that each step is completed satisfactorily before moving on to the next.\n",
    "\n",
    "> *Example:* \"First, summarize the following paragraph. Then, provide a critical analysis. Finally, suggest improvements.\"\n",
    "\n",
    "## Evaluating Prompt Effectiveness\n",
    "To ensure that prompts are achieving the desired objectives and eliciting high-quality responses from the AI, it is crucial to evaluate their effectiveness:\n",
    "\n",
    "1. **Response Quality**\n",
    "    - Assess the relevance, accuracy, and completeness of the AI's responses to the given prompts.\n",
    "    - Determine whether the generated outputs align with the intended purpose and provide meaningful and useful information.\n",
    "\n",
    "2. **Consistency**\n",
    "    - Evaluate the AI's ability to provide consistent responses to similar prompts or variations of the same prompt.\n",
    "    - Inconsistent responses may indicate a lack of robustness or reliability in the AI's understanding of the task.\n",
    "\n",
    "3. **Adaptability**\n",
    "    - Test the AI's ability to handle variations or modifications of the prompt while still producing appropriate and relevant responses.\n",
    "    - A well-designed prompt should allow for some flexibility and adaptability to accommodate minor changes or variations in the input.\n",
    "\n",
    "## Practical Applications\n",
    "Prompt engineering has found numerous practical applications across various domains, demonstrating its value in enhancing the performance and utility of AI language models:\n",
    "\n",
    "1. **Customer Support**\n",
    "    - By crafting prompts that guide AI to provide accurate, helpful, and context-specific responses, businesses can improve their customer support capabilities.\n",
    "    - Well-designed prompts can enable AI to handle a wide range of customer inquiries, reducing response times and increasing customer satisfaction.\n",
    "\n",
    "2. **Content Generation**\n",
    "    - Prompt engineering can be leveraged to generate high-quality content, such as creative writing, marketing copy, or technical documentation.\n",
    "    - By providing clear instructions, relevant context, and examples within the prompts, users can guide AI to produce engaging and coherent content tailored to specific requirements.\n",
    "\n",
    "3. **Data Analysis**\n",
    "    - Prompts can be designed to assist AI in various data analysis tasks, such as summarizing large datasets, generating insights, or performing complex calculations.\n",
    "    - By breaking down the analysis process into smaller steps and providing specific instructions, users can exploit AI to extract meaningful information from data more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Strategies and Examples\n",
    "\n",
    "Prompt engineering strategies are techniques used to design and structure prompts in ways that optimize the performance of AI language models. By employing various strategies, users can guide the AI to generate more accurate, coherent, and contextually relevant responses. This section will explore several prompt engineering strategies and provide examples to illustrate their application.\n",
    "\n",
    "## Prompt Engineering Strategies\n",
    "\n",
    "1. **Zero-Shot Learning**\n",
    "    - Description: The model is given a task without any prior examples or demonstrations.\n",
    "    - Example Prompt: \"Translate the following sentence from English to Portuguese: 'Good morning, everyone.'\"\n",
    "    - Zero-shot learning tests the model's ability to perform tasks without explicit training or examples. It relies on the model's pre-existing knowledge and understanding of language to generate appropriate responses.\n",
    "\n",
    "2. **Few-Shot Learning**\n",
    "    - Description: The model is given a few examples or demonstrations before being asked to perform the task.\n",
    "    - Example Prompt: \"Translate the following sentences from English to Portuguese: 'Good morning, everyone.' - 'Bom dia a todos' 'How are you?' - 'Como vai você?' Now translate: 'What is your name?'\"\n",
    "    - Few-shot learning provides the model with a small number of examples to guide its understanding of the task. By observing the patterns and relationships in the examples, the model can adapt its responses to similar tasks.\n",
    "\n",
    "3. **Chain-of-Thought Prompting**\n",
    "    - Description: The model is guided through a step-by-step reasoning process to arrive at the final answer.\n",
    "    - Example Prompt: \"To solve the math problem 12 + 5, first add 10 + 5 to get 15, then add 2 to get 17. So, 12 + 5 equals 17.\"\n",
    "    - Chain-of-thought prompting breaks down complex tasks into smaller, sequential steps. By providing a clear reasoning process, the model can generate more accurate and coherent responses, especially for tasks that require multi-step problem-solving.\n",
    "\n",
    "4. **React (Reasoning and Acting)**\n",
    "    - Description: The model is prompted to reason about the task and then act based on that reasoning.\n",
    "    - Example Prompt: \"To write a summary, first identify the main points of the article, then condense those points into a few sentences. Here's the article: [Article Text]\"\n",
    "    - The React strategy involves two stages: reasoning and acting. The model first analyzes the task and formulates a plan of action. It then executes the plan to generate the final response. This strategy helps the model produce more structured and purposeful outputs.\n",
    "\n",
    "5. **Plan-and-Solve**\n",
    "    - Description: The model is first asked to create a plan and then execute it.\n",
    "    - Example Prompt: \"First, outline the steps needed to bake a cake. Then, write a recipe following those steps.\"\n",
    "    - Similar to the React strategy, Plan-and-Solve involves two distinct phases. The model first generates a plan or outline for the task and then uses that plan to guide its response. This strategy is particularly effective for tasks that require a systematic approach or have multiple components.\n",
    "\n",
    "6. **Contextual Prompting**\n",
    "    - Description: The model is given a rich context to understand the task better.\n",
    "    - Example Prompt: \"Given the context of a restaurant review, summarize the following review: [Review Text]\"\n",
    "    - Contextual prompting provides the model with additional information or background to help it better understand the task at hand. By supplying relevant context, the model can generate more accurate and nuanced responses that align with the specific domain or scenario.\n",
    "\n",
    "7. **Task-Specific Templates**\n",
    "    - Description: Using templates tailored to specific tasks to guide the model.\n",
    "    - Example Prompt: \"Email template: 'Dear [Name], I am writing to inform you about [Event/Topic]. Please let me know if you have any questions. Best regards, [Your Name]'\"\n",
    "    - Task-specific templates provide a structured format for the model to follow when generating responses. By incorporating placeholders or variables, templates can be easily customized for different instances of the same task, ensuring consistency and efficiency.\n",
    "\n",
    "8. **Mixed-Task Prompting**\n",
    "    - Description: Combining multiple tasks within a single prompt to improve model performance.\n",
    "    - Example Prompt: \"Translate the following sentence to Spanish and then summarize it: 'The weather is nice today.'\"\n",
    "    - Mixed-task prompting involves presenting the model with multiple related tasks in a single prompt. By combining tasks, the model can capitalize on the shared context and generate more coherent and thorough responses. This strategy can also help improve efficiency by reducing the number of separate prompts needed.\n",
    "\n",
    "9. **Feedback Loop**\n",
    "    - Description: Incorporating feedback to refine and improve the prompts iteratively.\n",
    "    - Example Prompt: \"Generate a story based on the following prompt, and then refine the story based on feedback: [Initial Story]\"\n",
    "    - The feedback loop strategy involves generating an initial response, receiving feedback or critique, and then iteratively refining the prompt and response based on that feedback. This iterative process allows for continuous improvement and adaptation of the model's outputs.\n",
    "\n",
    "10. **Role Assignment**\n",
    "    - Description: Assigning specific roles to the model to guide its response.\n",
    "    - Example Prompt: \"You are a customer service representative. Respond to the following customer query: 'I have an issue with my order.'\"\n",
    "    - Role assignment prompts the model to adopt a specific persona or role when generating responses. By providing a clear context and expectations for the model's behavior, role assignment can lead to more consistent and appropriate responses within a given domain.\n",
    "\n",
    "11. **Data Augmentation**\n",
    "    - Description: Using various augmented data examples to enhance the prompt's effectiveness.\n",
    "    - Example Prompt: \"Given these examples of email subjects and their corresponding responses, generate a response for the new email subject: [Examples and New Subject]\"\n",
    "    - Data augmentation involves providing the model with additional examples or variations of the input data to improve its understanding and generalization capabilities. By exposing the model to a diverse range of examples, data augmentation can help the model generate more robust and accurate responses.\n",
    "\n",
    "12. **Interactive Prompts**\n",
    "    - Description: Engaging the model in a back-and-forth interaction to complete a task.\n",
    "    - Example Prompt: \"Let's brainstorm ideas for a new project. What do you think about focusing on environmental sustainability?\"\n",
    "    - Interactive prompts simulate a conversation or dialogue between the user and the model. By engaging in a series of exchanges, the model can progressively refine its understanding of the task and generate more relevant and coherent responses. Interactive prompts are particularly useful for tasks that require collaboration or iterative refinement.\n",
    "\n",
    "13. **Clarifying Questions**\n",
    "    - Description: Encouraging the model to ask clarifying questions before providing the answer.\n",
    "    - Example Prompt: \"If you're unsure about the task, ask questions to clarify. Task: Write a report on climate change.\"\n",
    "    - Clarifying questions prompt the model to seek additional information or clarification when the task or input is ambiguous or incomplete. By asking relevant questions, the model can gather the necessary details to provide a more accurate and thorough response. This strategy helps improve the model's understanding and reduces the likelihood of generating irrelevant or incorrect outputs.\n",
    "\n",
    "> Each of these strategies can be adapted and combined based on the specific requirements of the task at hand, leveraging the strengths of large language models to achieve the desired outcomes. By carefully designing and structuring prompts using these strategies, users can guide the model to generate more accurate, coherent, and contextually relevant responses.\n",
    "> It's important to note that the effectiveness of each strategy may vary depending on the specific model and task. Experimentation and iteration are key to finding the most suitable combination of strategies for a given scenario. Additionally, as language models continue to grow and improve, new prompt engineering strategies may emerge, further expanding the possibilities for optimizing model performance.\n",
    ">\n",
    "> To learn more, [this site](https://www.promptingguide.ai/techniques) is a good starting point for exploring additional prompt engineering techniques and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Side note: Is it \"Engineering\"?\n",
    ">\n",
    "> The use of the term \"engineering\" in the context of \"prompt engineering\" is a subject of debate. While prompt engineering involves crafting and refining text inputs to effectively guide AI behavior, it differs from traditional engineering disciplines in several key aspects.\n",
    ">\n",
    "> **Traditional Engineering:**\n",
    "> - Applies rigorous scientific principles and quantitative methods\n",
    "> - Focuses on designing and building physical structures and systems\n",
    "> - Relies heavily on mathematical modeling, testing, and validation\n",
    ">\n",
    "> **Prompt Engineering:**\n",
    "> - Requires creativity, strategy, and understanding of the AI model\n",
    "> - Involves crafting and refining text inputs to guide AI behavior\n",
    "> - Lacks the same level of rigorous, quantitative methods as traditional engineering\n",
    ">\n",
    "> Some argue that referring to prompt creation as \"engineering\" may overstate the technical rigor involved in the process. Prompt engineering is more akin to a form of art or creative writing, where the goal is to effectively communicate ideas and elicit desired responses from the AI model.\n",
    ">\n",
    "> However, others contend that the term \"engineering\" is appropriate, as prompt engineering does involve:\n",
    "> - Systematic design and refinement of prompts\n",
    "> - Consideration of the AI model's architecture and capabilities\n",
    "> - Iterative testing and optimization to achieve desired outcomes\n",
    ">\n",
    "> Ultimately, while prompt engineering may not adhere to the strict definition of traditional engineering, it does require a structured, iterative approach to designing effective prompts. The use of the term \"engineering\" highlights the strategic and methodical aspects of the process, even if it does not involve the same level of mathematical rigor as other engineering disciplines.\n",
    ">\n",
    "> Regardless of the terminology used, the importance of crafting clear, well-structured, and effective prompts cannot be overstated. The quality of the prompt directly impacts the quality of the AI's output, making prompt engineering a critical skill in the field of AI application and development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs with [LangChain](https://www.langchain.com/)\n",
    "\n",
    "LangChain is a powerful platform that simplifies the process of interacting with large language models (LLMs), enabling users to use their capabilities for various language-related tasks. By providing a user-friendly interface and a range of features, LangChain makes it easier to utilize LLMs for text generation, question answering, content summarization, and more.\n",
    "\n",
    "## Key Features of LangChain\n",
    "\n",
    "1. **Seamless Incorporation with LLMs**\n",
    "    - LangChain offers seamless combination with popular LLMs, allowing users to access their capabilities without the need for complex setup or configuration.\n",
    "    - It supports a wide range of LLMs, including OpenAI's GPT models, Google's BERT, and others, ensuring compatibility with state-of-the-art language models.\n",
    "\n",
    "2. **Intuitive API and Documentation**\n",
    "    - LangChain provides a well-documented and intuitive API, making it easy for developers to incorporate LLMs into their applications.\n",
    "    - The platform offers complete documentation, including code examples and tutorials, to guide users through the process of working with LLMs.\n",
    "\n",
    "3. **Flexible Input and Output Handling**\n",
    "    - LangChain supports various input formats, such as text, documents, and even structured data, allowing users to process and analyze diverse types of content.\n",
    "    - It provides flexibility in handling output, enabling users to customize the generated text, control the length and style of the output, and integrate it seamlessly into their applications.\n",
    "\n",
    "4. **Task-Specific Modules**\n",
    "    - LangChain offers task-specific modules that are optimized for common language-related tasks, such as text summarization, question answering, and sentiment analysis.\n",
    "    - These modules encapsulate best practices and provide pre-configured settings, making it easier for users to achieve high-quality results without extensive fine-tuning.\n",
    "\n",
    "5. **Memory and Context Management**\n",
    "    - LangChain includes features for managing memory and context, allowing LLMs to maintain a coherent understanding of the conversation or document being processed.\n",
    "    - This enables more contextually relevant and consistent outputs, especially in scenarios involving multi-turn conversations or long-form text generation.\n",
    "\n",
    "## Interacting with LLMs using LangChain\n",
    "\n",
    "To interact with LLMs using LangChain, follow these general steps:\n",
    "\n",
    "1. **Installation and Setup**\n",
    "    - Install the LangChain library and its dependencies in your Python environment.\n",
    "    - Configure the necessary API keys and credentials for the LLMs you intend to use.\n",
    "\n",
    "2. **Importing and Initializing Models**\n",
    "    - Import the required LangChain modules and classes for the specific LLMs you want to work with.\n",
    "    - Initialize instances of the LLMs, specifying the desired configuration options.\n",
    "\n",
    "3. **Preparing Input Data**\n",
    "    - Preprocess and format your input data, such as text documents or user queries, to ensure compatibility with the LLMs.\n",
    "    - Utilize LangChain's input handling features to convert and structure the data as needed.\n",
    "\n",
    "4. **Generating Output**\n",
    "    - Use LangChain's API to pass the prepared input data to the LLMs and generate the desired output.\n",
    "    - Customize the generation parameters, such as the output length, temperature, or top-k sampling, to control the quality and diversity of the generated text.\n",
    "\n",
    "5. **Post-processing and Integration**\n",
    "    - Process the generated output to extract relevant information, perform additional analysis, or integrate it into your application's workflow.\n",
    "    - Capitalize On LangChain's output handling capabilities to format and structure the results as required.\n",
    "\n",
    "## [LlamaIndex](https://www.llamaindex.ai/): An Alternative to LangChain\n",
    "\n",
    "LlamaIndex is another platform that enables interaction with LLMs, particularly focusing on indexing and querying large document collections. Here are some key features of LlamaIndex:\n",
    "\n",
    "1. **Document Indexing**: LlamaIndex provides tools to efficiently index large collections of documents, making them searchable and accessible for querying using LLMs.\n",
    "\n",
    "2. **Semantic Search**: With LlamaIndex, users can perform semantic searches on the indexed documents, leveraging the power of LLMs to understand the context and meaning of the queries.\n",
    "\n",
    "3. **Customizable Indexing**: LlamaIndex allows users to customize the indexing process, including document preprocessing, tokenization, and embedding generation, to optimize the indexing performance for specific use cases.\n",
    "\n",
    "4. **Query Optimization**: LlamaIndex employs various techniques to optimize the querying process, such as query expansion, relevance ranking, and context-aware retrieval, to ensure accurate and efficient results.\n",
    "\n",
    "While LlamaIndex excels in indexing and querying large document collections, it may have a steeper learning curve compared to LangChain and may require more setup and configuration for custom use cases.\n",
    "\n",
    "## Why Choose LangChain?\n",
    "\n",
    "LangChain is a compelling choice for several reasons:\n",
    "\n",
    "1. **Versatility**: LangChain's support for a wide range of LLMs and its flexibility in handling various language-related tasks make it a versatile platform for diverse use cases.\n",
    "\n",
    "2. **Ease of Use**: With its intuitive API, complete documentation, and task-specific modules, LangChain simplifies the process of integrating LLMs into applications, even for developers with limited experience.\n",
    "\n",
    "3. **Extensibility**: LangChain's modular architecture allows users to extend and customize its functionality to suit their specific requirements, enabling the development of tailored language-based solutions.\n",
    "\n",
    "4. **Active Development**: LangChain has an active development community and regular updates, ensuring that users have access to the latest features, improvements, and bug fixes.\n",
    "\n",
    "5. **Popularity**: As I am writing this notebook, LlamaIndex has 33.9k stars and 4.8k forks on GitHub. LangChain, on the other hand, has 14.2k forks and 89.8k stars. This indicates that LangChain is more popular and widely used among developers.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "<h4>Google Trends Comparison Between LangChain and LlamaIndex</h4>\n",
    "</div>\n",
    "<p align=\"center\">\n",
    "<img src=\"images/langchain_llamaindex.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "> **Note:** The choice between LangChain and LlamaIndex ultimately depends on the specific requirements and goals of your project. If your primary focus is on indexing and querying large document collections, LlamaIndex might be a suitable choice. However, if you require a more versatile and user-friendly platform for interacting with LLMs across a wide range of tasks, LangChain is often the preferred option. It's recommended to evaluate both platforms based on your needs and explore their respective documentation and examples to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "## Ollama: A Versatile Framework for Running Large Language Models Locally\n",
    "\n",
    "Ollama is a powerful and flexible framework designed to run large language models (LLMs) locally on a user's machine. It supports a wide range of popular models, including Llama, Phi, Mistral, and Gemma, among others. With Ollama, users can create, manage, and deploy custom models easily, making it an ideal tool for developers and researchers looking to build innovative AI applications. For more detailed information and to get started, visit the [Ollama website](https://ollama.com).\n",
    "\n",
    "### Key Features of Ollama\n",
    "\n",
    "1. **Extensive Model Support**: Ollama supports several state-of-the-art models, such as Llama. These models are optimized for various tasks, including text generation and dialogue systems, allowing users to choose the most suitable model for their specific needs.\n",
    "\n",
    "2. **Customization through Modelfiles**: Users can create and customize their models using a `Modelfile`. This feature enables setting parameters like temperature, context length, and system messages to tailor the model's behavior. For example, a `Modelfile` can be configured to make the model respond as a specific character or follow certain dialogue styles, providing great flexibility in model customization.\n",
    "\n",
    "3. **Local Deployment for Privacy and Security**: One of the key advantages of Ollama is its ability to run models locally. This is particularly useful for applications requiring high privacy and security standards, as users can process data on their own hardware without relying on cloud services. Local deployment gives users full control over their data and reduces the risk of unauthorized access.\n",
    "\n",
    "4. **Multimodal Capabilities for Enhanced Functionality**: Some models within the Ollama framework, such as the LLaVA (Large Language-and-Vision Assistant), support multimodal inputs. This means they can process and generate text based on image inputs, opening up new possibilities for creative and interactive applications.\n",
    "\n",
    "5. **Optimized Performance and Efficiency**: Models like Phi-3 are designed for efficient processing, featuring Rotary Position Embeddings and long context lengths. These optimizations make them suitable for complex tasks like code completion and dialogue generation, ensuring smooth performance even with resource-intensive applications.\n",
    "\n",
    "### Practical Usage of Ollama\n",
    "\n",
    "1. **Running Models with Simple Commands**: To run a model like Llama 3.1, users can execute commands such as `ollama run llama3.1` for the 8B or 70B parameter versions. Ollama supports popular tooling integrations, including LangChain and LlamaIndex, allowing seamless implementation into existing workflows.\n",
    "\n",
    "2. **Creating Custom Models with Modelfiles**: Ollama makes it straightforward to create custom models. Users can define their model configurations in a `Modelfile` and use commands like `ollama create` to generate the model. Once created, the model can be deployed using the `ollama run` command.\n",
    "\n",
    "3. **Integrating Models with REST API**: Ollama provides a thorough REST API for developers to integrate models into their applications. This API supports various functionalities, such as generating responses and engaging in interactive dialogues with the models, making it easy to build AI-powered features into existing software.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here's what you'll get after installing ollama.\n",
    "\n",
    "```bash\n",
    "(base) jacob@schrodinger ~ % ollama run llama3.1\n",
    ">>> Oi! Tudo bem?\n",
    "Tudo bem, obrigado! Eu sou um modelo de linguagem treinado para ajudar a\n",
    "responder perguntas e discutir tópicos. Não tenho sentimentos como o ser\n",
    "humano, mas estou aqui para ajudá-lo a qualquer momento. Como posso\n",
    "ajudá-lo hoje?\n",
    "\n",
    ">>> /bye\n",
    "(base) jacob@schrodinger ~ %\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Llama 3.1: Advanced Natural Language Processing from Meta\n",
    "\n",
    "Llama 3.1, developed by Meta, is the latest version of the Llama language model series. This model offers advanced natural language processing capabilities and is accessible to individuals, researchers, and businesses for experimentation and innovation.\n",
    "\n",
    "### Key Features of Llama 3.1\n",
    "\n",
    "1. **Multiple Model Sizes**: Llama 3.1 is available in various sizes, including 8 billion (8B), 70 billion (70B), and the new 405 billion (405B) parameter models. These models come in both pre-trained and instruction-tuned versions, serving to different applications and levels of customization.\n",
    "\n",
    "2. **Superior Performance**: Llama 3.1 has shown impressive performance compared to its predecessors and competitors. It outperforms OpenAI's GPT-4 on the HumanEval benchmark, scoring 81.7 compared to GPT-4's 67. However, it slightly trails behind GPT-4 in the MMLU knowledge assessment.\n",
    "\n",
    "3. **Serverless API Deployment**: The models can be deployed via serverless APIs on platforms like Azure AI, providing developers with tools to easily integrate these models into their applications. This assimilation includes enhanced security and compliance features, such as Azure AI Content Safety.\n",
    "\n",
    "4. **Safety Features and Guardrails**: Meta has implemented new safety features, including Code Shield, to catch insecure code that the model might produce. This ensures that applications built using Llama 3.1 adhere to ethical and security standards.\n",
    "\n",
    "5. **Versatility and Compatibility**: Llama 3.1 models are versatile, supporting various applications from text generation to chatbots. They are compatible with popular machine learning frameworks like Hugging Face's Transformers, enabling easy fine-tuning and deployment.\n",
    "\n",
    "### Practical Usage of Llama 3.1\n",
    "\n",
    "Llama 3.1 can be used for a wide range of tasks, such as:\n",
    "- **Text Generation**: Generating coherent and contextually relevant text based on prompts.\n",
    "- **Chatbots**: Creating conversational agents that can handle complex interactions.\n",
    "- **Coding Assistance**: Assisting in code generation and debugging with high accuracy.\n",
    "- **Content Creation**: Generating creative content for marketing, storytelling, and more.\n",
    "\n",
    "## Requirements and Considerations\n",
    "\n",
    "Running large language models like Llama 3.1 requires significant computational resources, especially for the larger model sizes. The table below shows the memory requirements for each model size:\n",
    "\n",
    "| Model Size | FP16 | FP8 | INT4 |\n",
    "|------------|-------|-------|-------|\n",
    "| 8B | 16 GB | 8 GB | 4 GB |\n",
    "| 70B | 140 GB| 70 GB | 35 GB |\n",
    "| 405B | 810 GB| 405 GB| 203 GB|\n",
    "\n",
    "<br>\n",
    "\n",
    "> Note: The numbers above indicate the GPU VRAM required just to load the model checkpoint. They don't include torch reserved space for kernels or CUDA graphs.\n",
    "> Note: FP16, FP8 and INT4 are the precision types used to store the model weights. FP16 is half precision, FP8 is 8-bit floating point, and INT4 is 4-bit integer. Lower precision types require less memory but may impact model performance.\n",
    "\n",
    "Depending on your computer configuration, you may be able to run the 8B or 70B model. For optimal performance, it is recommended to use a GPU with sufficient VRAM. If you don't have access to a GPU, the models can still run on CPU and RAM, but the processing speed will be slower. Apple devices with Mx series chips can capitalize on the Apple Silicon GPU to accelerate the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder: Stateless Nature of Language Models\n",
    "\n",
    "Language models, including Large Language Models (LLMs), are inherently **stateless**. This means they do not maintain an internal state or memory across different interactions or requests. Each input is processed independently, without retaining any context or information from previous exchanges.\n",
    "\n",
    "## Reasons for Statelessness\n",
    "\n",
    "The stateless design of language models is intentional and serves several purposes:\n",
    "\n",
    "1. **Efficient Scaling**: By treating each request as an independent interaction, language models can handle a large number of concurrent users without needing to manage and store individual conversation states. This allows for efficient resource allocation.\n",
    "\n",
    "2. **Design and Maintenance**: Stateless models are simpler to design, implement, and maintain compared to stateful models. They do not require complex mechanisms for tracking and updating conversation states, making the overall architecture more straightforward.\n",
    "\n",
    "3. **Versatility**: Stateless models can handle a wide range of tasks and topics without being constrained by previous interactions. They can switch between different contexts and respond to each input based on the current information provided.\n",
    "\n",
    "## Consequences of Statelessness\n",
    "\n",
    "The stateless nature of language models has significant consequences:\n",
    "\n",
    "1. **No Memory Retention**: Language models do not maintain an internal state, so they cannot remember or refer back to information from previous interactions. Each input is treated as a standalone request, and the model generates a response based solely on the current input and its pre-trained knowledge.\n",
    "\n",
    "2. **Coherence Challenges**: Without the ability to retain context across interactions, language models may struggle to maintain coherence and consistency in long-form conversations or multi-turn dialogues. They cannot build upon previous exchanges or understand the broader context of the conversation.\n",
    "\n",
    "3. **Response Variability**: Due to the lack of memory, language models may repeat information or provide inconsistent responses if asked similar questions multiple times. They cannot learn from or adapt to the specific user's preferences or previous interactions.\n",
    "\n",
    "4. **Impersonal Responses**: Stateless models cannot personalize their responses based on individual user profiles or preferences. Each interaction is treated independently, making it challenging to tailor the model's behavior to specific users.\n",
    "\n",
    "## Mitigating Statelessness\n",
    "\n",
    "To address the limitations of statelessness, developers and researchers employ various techniques:\n",
    "\n",
    "- **Contextual Prompts**: Carefully crafting prompts that provide sufficient context and information can help guide the language model's response and mitigate the lack of long-term memory. By including relevant details in the prompt, the model can generate more coherent and contextually appropriate responses.\n",
    "\n",
    "- **Information Retrieval**: Some approaches involve integrating external memory systems or databases with language models. By storing and retrieving relevant information from external sources, the model can simulate a form of memory and provide more consistent and contextually aware responses.\n",
    "\n",
    "- **Conversation Management**: Developers can build stateful wrappers around language models to maintain conversation states and enable more personalized interactions. These wrappers manage the conversation flow, store relevant information, and provide the necessary context to the language model for each interaction.\n",
    "\n",
    "> **Note**: Understanding the stateless nature of language models is crucial for setting appropriate expectations and designing effective strategies for utilizing them in various applications. By recognizing their limitations and employing suitable techniques, developers can use the power of language models while mitigating the challenges posed by statelessness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "`Enough of the marketing talk! Let's get to the real stuff!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 characters of API key: sk-zYm0VXQ\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# This is useful for keeping sensitive information like API keys out of your code\n",
    "load_dotenv()  # You are expected to have a .env file with the OpenAI API KEY `OPENAI_API_KEY`\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "# We're only displaying the first 10 characters for security reasons\n",
    "# This is a good practice to verify the key is loaded without exposing it entirely\n",
    "api_key_preview = os.getenv('OPENAI_API_KEY')[:10]\n",
    "print(f\"First 10 characters of API key: {api_key_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using the model directly. ChatModels are examples of LangChain \"Runnables,\" meaning they provide a uniform interface for interaction. To call the model straightforwardly, we can provide a list of messages to the .invoke method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/imd0187/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?\" response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 13, 'total_tokens': 37}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-449a1143-d3cc-458e-82f3-94fbd28f2384-0'\n"
     ]
    }
   ],
   "source": [
    "# Import the ChatOpenAI class from the appropriate module (not shown in the original code)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # Specify the model to use (GPT-4 mini variant)\n",
    "    temperature=0.7  # Set the temperature for response generation (higher values increase randomness)\n",
    ")\n",
    "\n",
    "# Note on pricing:\n",
    "# This model costs approximately 0.15 USD per million tokens for input\n",
    "# and half of that for output. For more details on models and pricing,\n",
    "# visit: https://openai.com/api/pricing/\n",
    "\n",
    "# Invoke the model with a simple greeting\n",
    "response = model_openai.invoke(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. I exist to assist and provide information, so I'm functioning properly if I can help you with something!\\n\\nHow about you? How's your day going?\" response_metadata={'model': 'llama3.1', 'created_at': '2024-08-23T21:18:37.500869577Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 5644436919, 'load_duration': 4574576531, 'prompt_eval_count': 16, 'prompt_eval_duration': 15924000, 'eval_count': 51, 'eval_duration': 1012624000} id='run-bcf778e6-9dd6-400d-98f0-bfe6c9151cf2-0'\n"
     ]
    }
   ],
   "source": [
    "# Import the ChatOllama class from the langchain library\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# Initialize the ChatOllama model\n",
    "model_llama = ChatOllama(\n",
    "    model='llama3.1',  # Specify the model version\n",
    "    base_url='http://localhost:11434',  # URL where Ollama is running locally\n",
    "    temperature=0.7  # Control the randomness of the output (0.0 to 1.0)\n",
    ")\n",
    "\n",
    "# Note: Ensure Ollama is running on your computer before executing this code\n",
    "\n",
    "# If you encounter an OllamaEndpointNotFoundError, you may need to pull the model\n",
    "# Run the following command in your terminal:\n",
    "# ollama pull llama3.1\n",
    "\n",
    "# Generate a response from the model\n",
    "response = model_llama.invoke(\"Hello, how are you?\")\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `temperature` in LLMs?\n",
    "\n",
    "In the context of Large Language Models (LLMs), **temperature** is a crucial parameter that influences the randomness and creativity of the model's output. It plays a significant role in determining how the model generates text, impacting the balance between deterministic and stochastic behavior.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "- **Definition**: Temperature is a hyperparameter that controls the probability distribution of the next word in a sequence. It essentially modifies the logits (raw predictions) before they are converted into probabilities.\n",
    "\n",
    "- **Effect on Output**:\n",
    "    - **Low Temperature**: When the temperature is set to a low value (close to 0), the model's output becomes more deterministic and conservative. It tends to choose the highest probability word more consistently, resulting in repetitive or predictable text.\n",
    "    - **High Temperature**: A high temperature value makes the model's output more random and creative. It increases the likelihood of selecting less probable words, which can lead to more diverse and imaginative text, but also increases the risk of generating incoherent or irrelevant content.\n",
    "\n",
    "#### Detailed Explanation\n",
    "\n",
    "- **Mathematical Perspective**:\n",
    "    - The temperature parameter $ T $ adjusts the logits $ z $ before applying the softmax function to obtain probabilities $ P $.\n",
    "    - The formula used is: $ P(i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $\n",
    "    - When $ T $ is low, the differences between logits are amplified, making the highest probability word much more likely to be chosen.\n",
    "    - When $ T $ is high, the differences between logits are diminished, leading to a more uniform probability distribution.\n",
    "\n",
    "- **Practical Implications**:\n",
    "    - **Creative Writing**: A higher temperature setting can be useful for tasks requiring creativity, such as story generation or poetry.\n",
    "    - **Technical Writing**: For tasks that require precision and accuracy, a lower temperature is often preferred to avoid introducing errors or irrelevant information.\n",
    "\n",
    "#### Examples and Analogies\n",
    "\n",
    "- **Analogy**: Think of temperature as a dial that controls the \"creativity\" of the model. Turning the dial down makes the model more conservative and focused, while turning it up makes the model more adventurous and willing to take risks.\n",
    "\n",
    "- **Example Scenario**:\n",
    "    - **Low Temperature**: In a technical document summarization task, a low temperature setting ensures that the summary remains accurate and closely aligned with the source material.\n",
    "    - **High Temperature**: In a creative writing prompt, a high temperature setting can help generate novel and interesting ideas, even if they are less predictable.\n",
    "\n",
    "#### Addressing Potential Questions\n",
    "\n",
    "- **Why not always use a high temperature for creativity?**\n",
    "    - While high temperature can enhance creativity, it can also lead to incoherent or off-topic responses. Balancing temperature is crucial depending on the desired outcome.\n",
    "\n",
    "- **Can temperature be dynamically adjusted?**\n",
    "    - Yes, in some advanced applications, temperature can be adjusted dynamically based on the context or phase of the task to optimize output quality.\n",
    "\n",
    "#### Important Notes\n",
    "\n",
    "> **Tip**: Experimenting with different temperature settings can help you find the optimal balance for your specific application. Start with a moderate value and adjust as needed based on the output quality.\n",
    ">\n",
    "> You can significantly influence the behavior of LLMs to better suit your needs, whether for creative endeavors or precise tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Eu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para o aprendizado de máquina.' response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 35, 'total_tokens': 58}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-6721728f-0b4d-4afd-8238-7d0231ba8d9b-0'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary message types from langchain_core.messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Create a list of messages to establish the context for the AI model\n",
    "messages = [\n",
    "    # Set the system message to define the task for the AI\n",
    "    SystemMessage(content=\"Translate the following from English into Portuguese\"),\n",
    "    \n",
    "    # Add a human message with the content to be translated\n",
    "    HumanMessage(content=\"I'd love yo learn more about Euler's number and why it is so important for ML\"),\n",
    "]\n",
    "# Note: This list structure allows for maintaining conversation history\n",
    "# and providing context for each interaction with the model\n",
    "\n",
    "# Invoke the OpenAI model with the prepared messages\n",
    "# The model will then generate a response based on the given context\n",
    "response = model_openai.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Que prazer!\\n\\nHere\\'s the translation:\\n\\nGostaria de saber mais sobre o número de Euler e por que ele é tão importante para a IA.\\n\\n(Note: I assume you meant \"Machine Learning\" instead of \"ML\", which is an abbreviation commonly used in English-speaking countries)\\n\\nIf you\\'d like, I can also provide some additional information on Euler\\'s number and its significance in machine learning. Just let me know!' response_metadata={'model': 'llama3.1', 'created_at': '2024-08-23T21:18:40.260700236Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1898454776, 'load_duration': 19755155, 'prompt_eval_count': 40, 'prompt_eval_duration': 46960000, 'eval_count': 86, 'eval_duration': 1701460000} id='run-8ae36717-7d2b-4898-9e55-94382232dc43-0'\n"
     ]
    }
   ],
   "source": [
    "response = model_llama.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is the translation:\\n\\nEu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para a Inteligência Artificial (ML).' response_metadata={'model': 'llama3.1', 'created_at': '2024-08-23T21:18:41.010099186Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 742220639, 'load_duration': 21132115, 'prompt_eval_count': 36, 'prompt_eval_duration': 50376000, 'eval_count': 33, 'eval_duration': 531379000} id='run-59ea7491-2c59-4b49-b4f0-b87b8a2d6a83-0'\n"
     ]
    }
   ],
   "source": [
    "# Define a list of messages for the language model\n",
    "messages_no_system = [\n",
    "    # First message: Instruction for translation\n",
    "    HumanMessage(content=\"Translate the following from English into Portuguese\"),\n",
    "    # Note: Some models may have issues with System messages, so we pass the instruction as a Human Message.\n",
    "    \n",
    "    # Second message: The actual content to be translated\n",
    "    HumanMessage(content=\"I'd love yo learn more about Euler's number and why it is so important for ML\"),\n",
    "    # This message contains the text we want to translate into Portuguese\n",
    "]\n",
    "\n",
    "# Invoke the language model (llama) with the defined messages\n",
    "response = model_llama.invoke(messages_no_system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputParsers: Extracting Relevant Information from Model Responses\n",
    "\n",
    "When working with language models, the response received is often in the form of an `AIMessage`. This message contains not only the string response but also additional metadata about the generated output. However, in many cases, we are primarily interested in extracting and using just the string response itself.\n",
    "\n",
    "#### Using the Simple Output Parser\n",
    "\n",
    "To parse out the desired string response, we can employ a simple output parser. Here's how you can use it:\n",
    "\n",
    "1. **Standalone Usage**:\n",
    "    - Import the output parser.\n",
    "    - Save the result of the language model call.\n",
    "    - Pass the saved result to the parser to extract the string response.\n",
    "\n",
    "2. **Chaining with the Language Model**:\n",
    "    - A more common and convenient approach is to \"chain\" the output parser with the language model.\n",
    "    - By chaining, the output parser is automatically invoked every time the language model is called within the chain.\n",
    "    - The chain takes the input type of the language model (string or list of messages) and returns the output type of the output parser (string).\n",
    "\n",
    "In LangChain, the `|` operator is used to combine two elements together, making it easy to create a chain. This way, we create a chain that automatically parses the model's output and returns the extracted string response.\n",
    "\n",
    "#### The Power of Chaining in LangChain\n",
    "\n",
    "Chaining elements is a fundamental pattern in LangChain that enables the creation of powerful processing pipelines. It allows you to combine different components seamlessly, passing the output of one element as the input to the next.\n",
    "\n",
    "This concept is known as **[LCEL - LangChain Expression Language](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel)**. LCEL provides a concise and expressive way to define and manipulate chains of elements in LangChain.\n",
    "\n",
    "By leveraging chaining, you can create complex workflows that involve multiple stages of processing, such as:\n",
    "- Preprocessing input data\n",
    "- Calling language models\n",
    "- Parsing and transforming model outputs\n",
    "- Performing additional computations or integrations\n",
    "\n",
    "Chaining simplifies the process of building and managing these pipelines, making it easier to create sophisticated applications with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StrOutputParser class from the langchain_core.output_parsers module\n",
    "# This class is used to parse string outputs from language models\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create an instance of the StrOutputParser\n",
    "# This instance will be used to parse string outputs in subsequent code\n",
    "simple_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the OpenAI model with the provided messages\n",
    "# The model_openai.invoke method sends the messages_no_system to the OpenAI model\n",
    "# and returns the model's response, which is stored in output_openai_1\n",
    "output_openai_1 = model_openai.invoke(messages_no_system)\n",
    "\n",
    "# Invoke the LLaMA model with the same set of messages\n",
    "# The model_llama.invoke method sends the messages_no_system to the LLaMA model\n",
    "# and returns the model's response, which is stored in output_llama_1\n",
    "output_llama_1 = model_llama.invoke(messages_no_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the simple_parser instance to parse the output from the OpenAI model\n",
    "\n",
    "parsed_output_openai = simple_parser.invoke(output_openai_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aqui está a tradução:\\n\\n\"Eu gostaria muito de aprender mais sobre o número de Euler e por que ele é tão importante para a Inteligência Artificial (ML)\". \\n\\nNota: Você pode querer especificar \"Máquina Lógica\" em vez de apenas \"ML\", pois essa abreviatura pode se referir a diferentes coisas. No entanto, na grande maioria dos casos, ML se refere à Inteligência Artificial ou à Máquina Aprendizagem (Machine Learning).'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the simple_parser instance to parse the output from the OpenAI model\n",
    "simple_parser.invoke(output_llama_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a processing chain for the OpenAI model\n",
    "# The '|' operator is used to chain together the model and the parser\n",
    "# This means that the output of model_openai will be automatically passed to simple_parser\n",
    "# The resulting chain_openai can be used to process inputs through the model and then parse the outputs\n",
    "chain_openai = model_openai | simple_parser\n",
    "\n",
    "# Create a processing chain for the LLaMA model\n",
    "# Similar to chain_openai, this chain will process inputs through model_llama and then parse the outputs using simple_parser\n",
    "chain_llama = model_llama | simple_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eu adoraria aprender mais sobre o número de Euler e por que ele é tão importante para o aprendizado de máquina (ML).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can invoke the whole chain to process inputs and parse outputs in a single step\n",
    "chain_openai.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claro, posso ajudar!\\n\\nAqui está a tradução:\\n\\nEu gostaria muito de aprender mais sobre o número de Euler e por que ele é tão importante para a Máquina de Aprendizado (ML).\\n\\nEuler\\'s Number (ou Número de Euler) é uma constante matemática chamada \"e\" (do nome do matemático suíço Leonhard Euler), que representa a taxa de crescimento instantâneo dos números naturais. Ela é aproximadamente igual a 2,71828.\\n\\nO número de Euler é fundamental em várias áreas da matemática e física, incluindo:\\n\\n* Cálculo diferencial: o número de Euler aparece na fórmula da função exponencial.\\n* Probabilidade: ele é usado em teoria dos jogos e estatística.\\n* Física: ele aparece na teoria do calor e na mecânica quântica.\\n\\nNa área da Máquina de Aprendizado (ML), o número de Euler tem implicações importantes:\\n\\n* Algoritmos de aprendizado profundo: o número de Euler é usado em algoritmos como a Rede Neuronal Recorrente (RNN) e a Diferenciação Retrograde (BDT).\\n* Modelagem de dados: ele é utilizado em modelos estatísticos para prever a taxa de crescimento de séries temporais.\\n* Teoria da complexidade computacional: o número de Euler aparece na teoria dos algoritmos e na análise do tempo de execução.\\n\\nEm resumo, o número de Euler é uma constante matemática fundamental que tem implicações importantes em várias áreas da ciência, incluindo a Máquina de Aprendizado (ML).'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_llama.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Prompt templates are essential in constructing the list of messages passed to a language model. They act as a bridge between raw user input and the structured data that the language model can process effectively. The primary role of prompt templates is to transform user input into a format that aligns with the application logic and the language model's requirements.\n",
    "\n",
    "Prompt templates take raw user input and apply a series of transformations to create a list of messages suitable for the language model. These transformations can include:\n",
    "\n",
    "1. **Adding a System Message**\n",
    "    - Prompt templates can prepend a system message to the list of messages. This system message provides context, instructions, or guidelines for the language model to follow when generating a response.\n",
    "\n",
    "2. **Formatting a Template**\n",
    "    - User input can be inserted into a predefined template using prompt templates. This template can include placeholders for the user input, along with additional text or instructions that provide structure and context for the language model.\n",
    "\n",
    "#### Benefits of Prompt Templates\n",
    "\n",
    "Using prompt templates offers several advantages:\n",
    "\n",
    "1. **Consistency**\n",
    "    - Prompt templates ensure that the input to the language model follows a consistent format and structure. This consistency helps the language model understand the context and generate more accurate and relevant responses.\n",
    "\n",
    "2. **Reusability**\n",
    "    - Prompt templates can be reused across different user inputs, reducing the need to manually format and structure the input each time. This reusability saves time and effort in constructing the list of messages.\n",
    "\n",
    "3. **Flexibility**\n",
    "    - Prompt templates allow for easy customization and modification of the input format. By adjusting the template, you can experiment with different structures and instructions to optimize the language model's performance for specific tasks or domains.\n",
    "\n",
    "#### Using Prompt Templates in LangChain\n",
    "\n",
    "With LangChain's PromptTemplates, you can:\n",
    "\n",
    "1. **Define a Template**\n",
    "    - Create a template with placeholders for user input.\n",
    "\n",
    "2. **Specify the System Message**\n",
    "    - Include any system message or additional instructions.\n",
    "\n",
    "3. **Combine User Input with the Template**\n",
    "    - Easily integrate user input into the template to create a formatted prompt.\n",
    "\n",
    "4. **Pass the Formatted Prompt to the Language Model**\n",
    "    - Send the formatted prompt directly to the language model for processing.\n",
    "\n",
    "With PromptTemplates, you can focus on the application logic and user experience while ensuring that the input to the language model is well-structured and optimized for generating accurate and relevant responses. Incorporating prompt templates into your language model workflow can greatly enhance the quality and efficiency of your application. They provide a flexible and reusable approach to transforming user input, allowing you to focus on building robust and user-friendly applications powered by language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatPromptTemplate class from the langchain_core.prompts module\n",
    "# This class is used to create templates for chat-based prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a template string for the system message\n",
    "# This template instructs the model to translate the given text into a specified language\n",
    "# The {language} placeholder will be replaced with the target language during execution\n",
    "system_template_text = \"Translate the following into {language} and output only the translated text: \"\n",
    "\n",
    "# Create a ChatPromptTemplate instance using the from_messages method\n",
    "# This method takes a list of tuples, where each tuple represents a message in the chat\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template_text),  # System message template for translation instruction\n",
    "        (\"human\", \"{text}\")  # Human message template with a placeholder for the input text\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Translate the following into Portuguese and output only the translated text: '), HumanMessage(content='Large Language Models are revolutionizing the way we interact with computers.')]\n"
     ]
    }
   ],
   "source": [
    "# Invoke the prompt template with specific inputs\n",
    "# The invoke method replaces the placeholders in the template with the provided values\n",
    "# This will format the message according to the template defined earlier\n",
    "formatted_message = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"Portuguese\",  # Target language for translation\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\"  # Text to be translated\n",
    "    }\n",
    ")\n",
    "\n",
    "# The formatted_message now contains the system and human messages with the placeholders replaced by the provided values\n",
    "# This formatted message can be sent to a language model for translation\n",
    "print(formatted_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Portuguese and output only the translated text: '),\n",
       " HumanMessage(content='Large Language Models are revolutionizing the way we interact with computers.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the formatted message into a list of messages\n",
    "# The to_messages method converts the formatted message into a format suitable for sending to a language model\n",
    "messages = formatted_message.to_messages()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can chain all components together using LCEL (LangChain Expression Language)\n",
    "# This involves chaining the prompt template, model, and output parser\n",
    "\n",
    "# Create a processing chain for the OpenAI model\n",
    "# The chain_openai_2 will first format the input using the prompt_template,\n",
    "# then pass the formatted input to model_openai, and finally parse the model's output using simple_parser\n",
    "chain_openai_2 = prompt_template | model_openai | simple_parser\n",
    "\n",
    "# Same thing for the LLaMA model\n",
    "chain_llama_2 = prompt_template | model_llama | simple_parser\n",
    "\n",
    "# These chains allow for streamlined processing of inputs through the entire pipeline, from prompt formatting to model inference and output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos de Linguagem de Grande Escala estão revolucionando a maneira como interagimos com os computadores.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the processing chain for the OpenAI model with specific inputs\n",
    "# The invoke method will execute the entire chain: \n",
    "# 1. Format the input using the prompt_template\n",
    "# 2. Pass the formatted input to model_openai for translation\n",
    "# 3. Parse the model's output using simple_parser\n",
    "\n",
    "# The input dictionary specifies the target language and the text to be translated\n",
    "# \"language\" is set to \"Portuguese\" to indicate the target language for translation\n",
    "# \"text\" is set to the sentence that needs to be translated\n",
    "\n",
    "translated_text = chain_openai_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Portuguese\",  # Target language for translation\n",
    "        \"text\": \"Large Language Models are revolutionizing the way we interact with computers.\"  # Text to be translated\n",
    "    }\n",
    ")\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grandes Modelos de Língua estão revolucionando a maneira como interagimos com computadores.\n"
     ]
    }
   ],
   "source": [
    "# Same thing for the LLaMA model\n",
    "\n",
    "translated_text = chain_llama_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Portuguese\",\n",
    "        \"text\" : \"Large Language Models are revolutionizing the way we interact with computers.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大型语言模型正在彻底改变我们与计算机的互动方式。\n"
     ]
    }
   ],
   "source": [
    "# Changing the target language to Chinese\n",
    "\n",
    "translated_text = chain_openai_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Chinese\",\n",
    "        \"text\" : \"Large Language Models are revolutionizing the way we interact with computers.\"\n",
    "    }\n",
    ")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大型语言模型正在改变我们与计算机的互动方式。\n"
     ]
    }
   ],
   "source": [
    "translated_text = chain_llama_2.invoke(\n",
    "    {\n",
    "        \"language\": \"Chinese\",\n",
    "        \"text\" : \"Large Language Models are revolutionizing the way we interact with computers.\"\n",
    "    }\n",
    ")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`From now on, I'll use just the OpenAI model except when the difference between GPT-4o and Llama 3.1 becomes relevant `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the Conversation Flowing\n",
    "\n",
    "In a conversational setting, maintaining a coherent and engaging dialogue is essential for a positive user experience. To achieve this, it's crucial to guide the conversation effectively, ensuring that each turn builds upon the previous one and leads to a meaningful exchange of information. Because LLMs are stateless, developers must manage the conversation flow explicitly to maintain context and coherence throughout the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import message classes from the langchain_core.messages module\n",
    "# HumanMessage: Represents a message from a human user\n",
    "# SystemMessage: Represents a message from the system (e.g., instructions or prompts)\n",
    "# AIMessage: Represents a message from an AI model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "def get_user_message(message_text):\n",
    "    # A function to simulate getting the user input during a conversation\n",
    "    # This function takes a string input (message_text) and returns a HumanMessage object\n",
    "    # The HumanMessage object encapsulates the content of the user's message\n",
    "    return HumanMessage(content=message_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store messages\n",
    "# This list will hold different types of messages (e.g., system, human, AI) in a conversation\n",
    "list_of_messages = []\n",
    "\n",
    "# Append a SystemMessage to the list_of_messages\n",
    "# SystemMessage is used to provide instructions or context to the AI model\n",
    "# Here, the content of the SystemMessage is 'You are a helpful assistant'\n",
    "# This message sets the context for the AI model, indicating that it should behave as a helpful assistant\n",
    "list_of_messages.append(\n",
    "    SystemMessage(content='You are a helpful assistant')\n",
    ")\n",
    "\n",
    "# Display the list of messages\n",
    "# At this point, the list contains only one message, the SystemMessage added above\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant'),\n",
       " HumanMessage(content='My name is Elias')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append a HumanMessage to the list_of_messages\n",
    "# The get_user_message function creates a HumanMessage object from the provided text\n",
    "# Here, the content of the HumanMessage is 'My name is Elias'\n",
    "# This simulates a user inputting their name into the conversation\n",
    "list_of_messages.append(get_user_message(\"My name is Elias\"))\n",
    "\n",
    "# Display the updated list of messages\n",
    "# The list now contains the initial SystemMessage and the new HumanMessage\n",
    "# This helps in tracking the flow of the conversation\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant'),\n",
       " HumanMessage(content='My name is Elias'),\n",
       " AIMessage(content='Nice to meet you, Elias! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 20, 'total_tokens': 34}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-677342ca-78c0-4173-9580-a29289018b37-0')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the OpenAI model with the current list of messages\n",
    "# The model_openai.invoke method processes the list_of_messages and generates a response\n",
    "# The response is stored in the ai_response variable\n",
    "ai_response = model_openai.invoke(list_of_messages)\n",
    "\n",
    "# Append the AI's response to the list_of_messages\n",
    "# The AIMessage object encapsulates the content of the AI's response\n",
    "# This step adds the AI's response to the conversation history, maintaining the sequence of messages\n",
    "list_of_messages.append(ai_response)\n",
    "\n",
    "# Display the updated list of messages\n",
    "# The list now includes the initial SystemMessage, the HumanMessage, and the AIMessage\n",
    "# This helps in tracking the entire conversation flow\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant'),\n",
       " HumanMessage(content='My name is Elias'),\n",
       " AIMessage(content='Nice to meet you, Elias! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 20, 'total_tokens': 34}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-677342ca-78c0-4173-9580-a29289018b37-0'),\n",
       " HumanMessage(content='Do you remember my name?')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append another HumanMessage to the list_of_messages\n",
    "# This simulates the user asking a follow-up question in the conversation\n",
    "# The get_user_message function creates a HumanMessage object with the content 'Do you remember my name?'\n",
    "# This message is added to the conversation history\n",
    "list_of_messages.append(get_user_message(\"Do you remember my name?\"))\n",
    "\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant'),\n",
       " HumanMessage(content='My name is Elias'),\n",
       " AIMessage(content='Nice to meet you, Elias! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 20, 'total_tokens': 34}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-677342ca-78c0-4173-9580-a29289018b37-0'),\n",
       " HumanMessage(content='Do you remember my name?'),\n",
       " AIMessage(content='Yes, your name is Elias! How can I help you today?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 48, 'total_tokens': 62}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-8dbb932f-83ad-4109-ade1-08cf0c29c074-0')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the OpenAI model with the current list of messages\n",
    "# The model_openai.invoke method processes the list_of_messages and generates a response\n",
    "# The response is stored in the ai_response variable\n",
    "ai_response = model_openai.invoke(list_of_messages)\n",
    "\n",
    "# Append the AI's response to the list_of_messages\n",
    "list_of_messages.append(ai_response)\n",
    "\n",
    "list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have the ability to remember past interactions or personal details. How can I assist you today?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to invoke the OpenAI model without passing a list of messages\n",
    "# Here, we directly pass a string \"Do you remember my name?\" to the model_openai.invoke method\n",
    "# This is different from the previous approach where we passed a list of messages\n",
    "\n",
    "response_content = model_openai.invoke(\"Do you remember my name?\").content\n",
    "response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This message history serves several important purposes and has effects for token usage, context length, and pricing.\n",
    "\n",
    "- **Token Count**: Every message in the history contributes to the overall token count. Tokens are the fundamental units used to measure the length of text in AI systems. The more messages included in the history, the higher the token count will be.\n",
    "\n",
    "- **Context Length**: The message history directly affects the context length. AI models have a limited context window, typically measured in tokens. If the message history becomes too long, it may exceed the maximum context length supported by the model. This can lead to truncation or loss of earlier messages in the conversation.\n",
    "\n",
    "- **Token-based Pricing**: Many AI services charge based on the number of tokens processed. The more tokens included in the message history, the higher the cost of each interaction. It's important to consider the trade-off between providing sufficient context and minimizing token usage to manage costs effectively.\n",
    "\n",
    "- **Efficiency**: To optimize pricing, it's crucial to keep the message history concise and relevant. Removing unnecessary or redundant messages can help reduce token usage while still maintaining adequate context for the AI to generate appropriate responses.\n",
    "\n",
    "`But enough with python lists.... Let's see how to manage history properly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message History: Storing and Consuming Conversation Context\n",
    "\n",
    "The Message History class is a powerful tool for creating the illusion of stateful models that maintain conversation context across interactions. By wrapping our model with the Message History class, we can keep track of inputs and outputs, storing them in a datastore for future reference. This allows the model to load and incorporate previous messages as part of the input for each new interaction, enabling more coherent and contextually relevant responses.\n",
    "\n",
    "To use the Message History functionality, we need to set up a chain that wraps the model and incorporates the message history. A crucial component of this setup is the `get_session_history` function, which is passed into the chain. This function takes a `session_id` parameter and is expected to return a Message History object. It should be passed as part of the configuration when calling the new chain. By using unique session IDs, the model can maintain distinct conversation contexts for different users or sessions.\n",
    "\n",
    "> The `session_id` acts as a key to retrieve the appropriate message history for each conversation, ensuring that the model responds based on the correct context.\n",
    "\n",
    "##### Storing and Retrieving Message History\n",
    "\n",
    "When a new interaction occurs, the message history chain:\n",
    "\n",
    "1. Retrieves the message history associated with the provided `session_id`\n",
    "2. Incorporates the previous messages as part of the input to the model\n",
    "3. Generates a response based on the current input and the conversation context\n",
    "4. Stores the new input and output in the datastore, updating the message history for the specific `session_id`\n",
    "\n",
    "This process allows the model to maintain a coherent conversation flow, as it has access to the previous messages and can generate responses that build upon the established context.\n",
    "\n",
    "<br>\n",
    "\n",
    ">\n",
    "> Incorporating message history into our models offers several advantages:\n",
    ">\n",
    "> - **Improved Coherence**: By considering previous messages, the model can generate responses that are more coherent and contextually relevant to the ongoing conversation.\n",
    "> - **Personalization**: Message history enables the model to tailor its responses based on the specific user or session, creating a more personalized experience.\n",
    "> - **Memory Retention**: The model can retain important information mentioned earlier in the conversation, allowing for more natural and informed interactions.\n",
    "> - **Conversation Continuity**: Users can pick up where they left off in a conversation, as the model has access to the full history of the interaction.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets simulate two people, João and Maria, interacting with our LLM\n",
    "import random\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "        # Generate a unique 10-character ID for each person\n",
    "        # This could be used to track individual interactions with the LLM\n",
    "        self.uid = \"\".join(random.choices(\"abcdefghijklmnopqrstuvwxyz01234567890\", k=10))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Custom string representation of the Person object\n",
    "        # This will be used when printing the object or using str() function\n",
    "        return f\"Name: {self.name}\\nID: {self.uid}\"\n",
    "\n",
    "# Create instances of the Person class for Maria and João\n",
    "maria = Person(\"Maria\") \n",
    "joao = Person(\"João\") \n",
    "\n",
    "# At this point, maria and joao are objects with unique names and IDs\n",
    "# We can use these objects to simulate different users interacting with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Maria\n",
       "ID: 3v36tp50pa"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: João\n",
       "ID: o0ql6oeo1m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Dictionary to store chat histories for different sessions\n",
    "database = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retrieves or creates a chat history for a given session ID.\n",
    "    \n",
    "    Args:\n",
    "        session_id (str): Unique identifier for the chat session.\n",
    "    \n",
    "    Returns:\n",
    "        BaseChatMessageHistory: Chat history object for the session.\n",
    "    \"\"\"\n",
    "    # If the session doesn't exist, create a new InMemoryChatMessageHistory\n",
    "    if session_id not in database:\n",
    "        database[session_id] = InMemoryChatMessageHistory()\n",
    "    \n",
    "    # Return the chat history for the session\n",
    "    return database[session_id]\n",
    "\n",
    "# Create a RunnableWithMessageHistory object\n",
    "# This combines the OpenAI model with the ability to manage chat history\n",
    "runnable_with_history = RunnableWithMessageHistory(model_openai, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate João and Maria are talking to the LLM\n",
    "\n",
    "# Configuration for João's session\n",
    "config_joao = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": joao.uid  # Using João's unique identifier for session tracking\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration for Maria's session\n",
    "config_maria = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": maria.uid  # Using Maria's unique identifier for session tracking\n",
    "    }\n",
    "}\n",
    "\n",
    "# These configurations are used to maintain separate conversation contexts\n",
    "# for João and Maria when interacting with the LLM. This allows the LLM\n",
    "# to provide personalized responses based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá, João! Como posso ajudá-lo hoje?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the language model with a specific configuration for João\n",
    "response_joao = runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"Olá, eu sou o João\")],\n",
    "    config = config_joao\n",
    ")\n",
    "\n",
    "# Extract and return the content of the response\n",
    "# This is useful for accessing just the generated text without metadata\n",
    "response_joao.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o0ql6oeo1m': InMemoryChatMessageHistory(messages=[HumanMessage(content='Olá, eu sou o João'), AIMessage(content='Olá, João! Como posso ajudá-lo hoje?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 13, 'total_tokens': 23}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-00abab8e-70cf-4e7d-846e-06e2cf553f47-0')])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, database is automatically keeping track of the conversation as long as we use the runnable history \n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sim, você se apresentou como João. Como posso ajudar você, João?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the runnable_with_history object to invoke a conversation\n",
    "response_joao2 = runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"Você lembra meu nome?\")],\n",
    "    config = config_joao\n",
    ")\n",
    "\n",
    "# Note: We only need to pass the current message. The runnable_with_history object automatically includes previous messages.\n",
    "response_joao2.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, mas não tenho a capacidade de lembrar informações pessoais ou interações passadas. Como posso ajudá-lo hoje?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the runnable_with_history chain for Maria\n",
    "response_maria = runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"Você lembra meu nome?\")],  # Ask if the AI remembers Maria's name\n",
    "    config = config_maria  # Use Maria's specific configuration\n",
    ")\n",
    "\n",
    "# The AI won't remember Maria's name since this is the first interaction with her ID\n",
    "\n",
    "# Extract and return the content of the response\n",
    "response_maria.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o0ql6oeo1m': InMemoryChatMessageHistory(messages=[HumanMessage(content='Olá, eu sou o João'), AIMessage(content='Olá, João! Como posso ajudá-lo hoje?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 13, 'total_tokens': 23}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-00abab8e-70cf-4e7d-846e-06e2cf553f47-0'), HumanMessage(content='Você lembra meu nome?'), AIMessage(content='Sim, você se apresentou como João. Como posso ajudar você, João?', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 36, 'total_tokens': 51}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-e9d652c1-98d4-4360-9a4b-ff6ceb9b6779-0')]),\n",
       " '3v36tp50pa': InMemoryChatMessageHistory(messages=[HumanMessage(content='Você lembra meu nome?'), AIMessage(content='Desculpe, mas não tenho a capacidade de lembrar informações pessoais ou interações passadas. Como posso ajudá-lo hoje?', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 12, 'total_tokens': 37}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-7503ed7a-4284-4ba5-ad4e-d93731e15b45-0')])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check our `database` to see the conversation history. It should have two keys, one for João and one for Maria, with their UIDs as keys and the history as values.\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components from langchain_core.prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create a ChatPromptTemplate with a system message and a placeholder for chat history\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message to set the behavior of the AI\n",
    "        (\"system\", \"You always answer in English, even if the user wrote in a different language\"),\n",
    " \n",
    "        # Placeholder for the chat history. This allows for including previous messages in the conversation\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain by combining the prompt template with an OpenAI model\n",
    "# The '|' operator is used to chain these components together\n",
    "chain = prompt | model_openai\n",
    "\n",
    "# The 'chain' variable now represents a complete conversation flow:\n",
    "# 1. It starts with the defined prompt (including system message and history)\n",
    "# 2. Then passes the formatted prompt to the OpenAI model for processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history_and_prompt = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your last message was asking if I remember your name. How can I assist you further?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the runnable_with_history_and_prompt object to invoke a response\n",
    "response_joao3 = runnable_with_history_and_prompt.invoke(\n",
    "    # Pass a list containing a single HumanMessage object\n",
    "    # This represents the user's input/question\n",
    "    [HumanMessage(content=\"Qual foi a minha última mensagem?\")],\n",
    "    \n",
    "    # Use the config_joao configuration\n",
    "    config = config_joao\n",
    ")\n",
    "\n",
    "# Extract and return the content of the response\n",
    "response_joao3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more complex prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message to set the language behavior\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You always answer in {language}, even if the user wrote in a different language\",\n",
    "        ),\n",
    "        # Placeholder for user messages\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain by combining the prompt with the OpenAI model\n",
    "# The '|' operator is used to connect the prompt and model in a pipeline\n",
    "chain = prompt | model_openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RunnableWithMessageHistory\n",
    "# This class is used to manage the execution of a chain with message history and variable prompts\n",
    "\n",
    "# Parameters:\n",
    "# - chain: The processing chain that will be executed. This chain typically includes the prompt template, model, and output parser.\n",
    "# - get_session_history: A function that retrieves the session history (i.e., the list of previous messages in the conversation).\n",
    "# - input_messages_key: The key used to access the input messages in the session history. Here, it is set to \"messages\".\n",
    "\n",
    "# This setup allows the chain to be executed with the context of previous messages, ensuring that the AI model has the necessary context to generate accurate responses.\n",
    "runnable_with_history_and_prompt_with_variables = RunnableWithMessageHistory(\n",
    "    chain,  # The processing chain to be executed\n",
    "    get_session_history,  # Function to retrieve the session history\n",
    "    input_messages_key=\"messages\"  # Key to access input messages in the session history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Votre dernier message était de demander si je me souvenais de votre nom. Comment puis-je vous aider davantage ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with message history and variable prompts\n",
    "# This method call will execute the chain with the provided inputs and configuration\n",
    "\n",
    "# Parameters:\n",
    "# - The first argument is a dictionary containing:\n",
    "#   - \"language\": The target language for the response, set to \"French\" in this case.\n",
    "#   - \"messages\": A list of messages to be processed. Here, it includes a HumanMessage asking \"Pode repetir sua última mensagem?\".\n",
    "# - The second argument is the configuration for the invocation, specified by config_joao.\n",
    "\n",
    "# The chain will use the provided messages and configuration to generate a response, taking into account the session history and the specified language.\n",
    "\n",
    "response_joao4 = runnable_with_history_and_prompt_with_variables.invoke(\n",
    "    {\n",
    "        \"language\": \"French\",  # Target language for the response\n",
    "        \"messages\": [HumanMessage(content=\"Pode repetir sua última mensagem?\")]  # List of messages to be processed\n",
    "    },\n",
    "    config=config_joao  # Configuration with ID\n",
    ")\n",
    "\n",
    "# Access and display the content of the response\n",
    "# The response_joao4 object contains the AI's reply, and the content attribute holds the actual text of the response\n",
    "response_joao4.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La tua ultima messaggio era di chiedere se potevo ripetere la mia ultima risposta. Come posso aiutarti ulteriormente?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing for Italian\n",
    "\n",
    "response_joao5 = runnable_with_history_and_prompt_with_variables.invoke(\n",
    "    {\n",
    "        \"language\": \"Italian\",  # Target language for the response\n",
    "        \"messages\": [HumanMessage(content=\"Pode repetir sua última mensagem?\")]  # List of messages to be processed\n",
    "    },\n",
    "    config=config_joao  # Configuration with ID\n",
    ")\n",
    "\n",
    "response_joao5.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o0ql6oeo1m': InMemoryChatMessageHistory(messages=[HumanMessage(content='Olá, eu sou o João'), AIMessage(content='Olá, João! Como posso ajudá-lo hoje?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 13, 'total_tokens': 23}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-00abab8e-70cf-4e7d-846e-06e2cf553f47-0'), HumanMessage(content='Você lembra meu nome?'), AIMessage(content='Sim, você se apresentou como João. Como posso ajudar você, João?', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 36, 'total_tokens': 51}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-e9d652c1-98d4-4360-9a4b-ff6ceb9b6779-0'), HumanMessage(content='Qual foi a minha última mensagem?'), AIMessage(content='Your last message was asking if I remember your name. How can I assist you further?', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 85, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-04b495f7-e10f-4bd5-b3fa-cfbe05f23198-0'), HumanMessage(content='Pode repetir sua última mensagem?'), AIMessage(content='Votre dernier message était de demander si je me souvenais de votre nom. Comment puis-je vous aider davantage ?', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 117, 'total_tokens': 139}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8ecbd26-9197-4f8c-9d69-ff87bbb00545-0'), HumanMessage(content='Pode repetir sua última mensagem?'), AIMessage(content='La tua ultima messaggio era di chiedere se potevo ripetere la mia ultima risposta. Come posso aiutarti ulteriormente?', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 153, 'total_tokens': 180}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-a7456e2d-ddbb-4382-b9e0-be3e892e34a1-0')]),\n",
       " '3v36tp50pa': InMemoryChatMessageHistory(messages=[HumanMessage(content='Você lembra meu nome?'), AIMessage(content='Desculpe, mas não tenho a capacidade de lembrar informações pessoais ou interações passadas. Como posso ajudá-lo hoje?', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 12, 'total_tokens': 37}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-7503ed7a-4284-4ba5-ad4e-d93731e15b45-0')])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: o0ql6oeo1m\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Olá, eu sou o João\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Olá, João! Como posso ajudá-lo hoje?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Você lembra meu nome?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Sim, você se apresentou como João. Como posso ajudar você, João?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Qual foi a minha última mensagem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your last message was asking if I remember your name. How can I assist you further?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Pode repetir sua última mensagem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Votre dernier message était de demander si je me souvenais de votre nom. Comment puis-je vous aider davantage ?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Pode repetir sua última mensagem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "La tua ultima messaggio era di chiedere se potevo ripetere la mia ultima risposta. Come posso aiutarti ulteriormente?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "User: 3v36tp50pa\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Você lembra meu nome?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Desculpe, mas não tenho a capacidade de lembrar informações pessoais ou interações passadas. Como posso ajudá-lo hoje?\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in database.items():\n",
    "    print(f\"User: {k}\")\n",
    "    for message in v.messages:\n",
    "        message.pretty_print()\n",
    "\n",
    "    print(\"\\n\"*3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that, even if I am using a simple python dictionary as database, LangChain has [built-in support for several databases](https://python.langchain.com/v0.2/docs/integrations/memory/), including Redis, MongoDB, and PostgreSQL. This allows you to store and retrieve message history data efficiently and securely, ensuring that the conversation context is preserved across interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What are Large Language Models (LLMs) and why are they significant in AI and NLP?\n",
    "\n",
    "2. How do LLMs understand and generate human-like text?\n",
    "\n",
    "3. What are some key milestones in the evolution of LLMs?\n",
    "\n",
    "4. What is the Transformer architecture, and why is it important in the development of LLMs?\n",
    "\n",
    "5. How do foundation models differ from traditional AI models?\n",
    "\n",
    "6. What is prompt engineering, and why is it important when working with LLMs?\n",
    "\n",
    "7. What are LangChain and LlamaIndex, and how do they ease interaction with LLMs?\n",
    "\n",
    "8. Why are LLMs considered stateless, and what are the effects of this characteristic?\n",
    "\n",
    "9. How can developers manage conversation flow and message history when working with stateless LLMs?\n",
    "\n",
    "10. What are some techniques to enhance the performance and relevance of LLM-generated responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "\n",
    "<!--\n",
    "\n",
    "1. Large Language Models are advanced AI systems trained on extensive text datasets to understand and generate human-like text. They are significant because they can perform various language-related tasks with high proficiency, transforming human-machine interactions and offering new opportunities in fields like customer service, healthcare, education, and creative writing.\n",
    "\n",
    "2. LLMs are trained on enormous datasets containing billions of words, which allows them to learn language complexities such as grammar, syntax, and semantics. They use deep learning techniques and vast numbers of parameters to generate text that closely resembles human language.\n",
    "\n",
    "3. Key milestones include:\n",
    "- **Transformer Architecture (2017):** Revolutionized NLP by allowing models to process input sequences in parallel.\n",
    "- **GPT (Generative Pre-trained Transformer) (2018):** Demonstrated the potential of unsupervised pre-training.\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers) (2018):** Introduced bidirectional training for better context understanding.\n",
    "- **GPT-3 (2020):** Showcased the power of scaling up language models with 175 billion parameters.\n",
    "\n",
    "4. The Transformer architecture, introduced in 2017 by Vaswani et al., uses attention mechanisms to process input sequences in parallel, improving training speed and performance. It is important because it laid the foundation for many subsequent LLMs, enabling them to handle large-scale data and complex language tasks more efficiently.\n",
    "\n",
    "5. Foundation models are trained on large, diverse datasets and acquire general knowledge through self-supervised learning. Unlike traditional AI models, which are trained on specific tasks with labeled data, foundation models can perform a wide array of tasks with minimal fine-tuning, making them more versatile and powerful.\n",
    "\n",
    "6. Prompt engineering involves designing and refining text inputs to guide LLMs to produce accurate and contextually appropriate responses. It is important because the quality of the prompt directly impacts the quality of the AI's output, making it a critical skill for optimizing the performance of LLMs.\n",
    "\n",
    "7. LangChain and LlamaIndex are platforms that simplify the process of interacting with LLMs. LangChain offers seamless integration, an intuitive API, and task-specific modules for various language tasks. LlamaIndex focuses on indexing and querying large document collections, using LLMs for semantic search and query optimization.\n",
    "\n",
    "8. LLMs are stateless because they do not maintain an internal state or memory across different interactions. Each input is processed independently, which means they cannot retain context from previous exchanges. This characteristic requires developers to explicitly manage conversation flow and message history to maintain coherence and context in interactions.\n",
    "\n",
    "9. Developers can manage conversation flow and message history by using techniques such as:\n",
    "- **Prompt Engineering:** Crafting prompts that include relevant context.\n",
    "- **External Memory Systems:** Storing and retrieving relevant information.\n",
    "- **Stateful Wrappers:** Managing conversation flow and maintaining context.\n",
    "- **Message History Classes:** Using tools like LangChain's Message History class to store and incorporate previous messages into each new interaction.\n",
    "\n",
    "10. Techniques to enhance LLM performance include:\n",
    "- **Using Prompt Templates:** Ensuring consistent and structured input.\n",
    "- **Few-Shot and Zero-Shot Learning:** Providing examples or task descriptions.\n",
    "- **Chain-of-Thought Prompting:** Guiding the model through a step-by-step reasoning process.\n",
    "- **Role Assignment:** Assigning specific roles to the model for context.\n",
    "- **Interactive Prompts and Feedback Loops:** Engaging in iterative refinement of prompts and responses. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
