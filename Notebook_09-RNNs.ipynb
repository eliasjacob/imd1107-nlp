{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## IMD1107 - Natural Language Processing\n",
    "### [Dr. Elias Jacob de Menezes Neto](htttps://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "\n",
    "- Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states across time steps, allowing them to capture dependencies and relationships between words in a sentence.\n",
    "\n",
    "- RNNs face challenges with vanishing and exploding gradients when dealing with long sequences. The vanishing gradient problem occurs when gradients become extremely small during backpropagation, preventing effective learning. The exploding gradient problem happens when gradients become very large, leading to unstable training.\n",
    "\n",
    "- Long Short-Term Memory (LSTM) networks address the limitations of RNNs by introducing a memory cell and gates (input, output, forget) that regulate information flow. This allows LSTMs to effectively capture long-term dependencies.\n",
    "\n",
    "- The LSTM architecture consists of an input gate that controls what new information is added to the cell state, a forget gate that determines what information to discard, an output gate that decides what information from the cell state is used to compute the hidden state, and a memory cell that maintains long-term information.\n",
    "\n",
    "- Bidirectional LSTM networks process sequences in both forward and backward directions, capturing both past and future contexts. This additional context helps improve the performance of sequence processing tasks.\n",
    "\n",
    "- Gated Recurrent Units (GRUs) simplify the LSTM architecture by combining the hidden and cell states into a single hidden state and using only two gates (update and reset). GRUs are computationally more efficient than LSTMs while still effectively handling long-term dependencies.\n",
    "\n",
    "- The case study demonstrated the application of RNN, LSTM, and GRU architectures on a name gender classification task using a dataset from the 2010 Brazilian Census. The Bidirectional LSTM achieved the highest accuracy, followed by the Bidirectional GRU, LSTM, and Vanilla RNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this class, you will be able to:\n",
    "\n",
    "1) **Explain the fundamental principles of Recurrent Neural Networks (RNNs) and how they process sequential data by maintaining hidden states across time steps.**\n",
    "\n",
    "2) **Compare and contrast the architectures of Vanilla RNNs, Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs), highlighting their mechanisms for addressing the vanishing gradient problem and capturing long-term dependencies.**\n",
    "\n",
    "3) **Implement and train Recurrent Neural Network models, including Vanilla RNN, LSTM, and Bidirectional LSTM/GRU, using PyTorch for a sequence classification task, such as name gender prediction.**\n",
    "\n",
    "4) **Evaluate the performance of different recurrent network architectures using appropriate metrics like accuracy, and interpret classification reports and confusion matrices to understand model strengths and weaknesses.**\n",
    "\n",
    "5) **Discuss the tradeoffs between different recurrent network architectures in terms of model complexity, number of parameters, computational efficiency, and accuracy, and justify the selection of a particular architecture for specific NLP tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When processing textual data, it's crucial to consider the dependencies and relationships between words in a sentence. The semantics of a sentence can change profoundly based on the order and selection of words.\n",
    "\n",
    "Consider these two similar sentences:\n",
    "> \"A bomba explodiu no jornal.\"\n",
    ">\n",
    "> \"A notícia do jornal explodiu como uma bomba\"\n",
    "\n",
    "Despite having analogous structure, interchanging just one adjective leads to a completely different meaning and emotional impact on the reader. Context plays a vital role, especially when a sentence's overall meaning can be greatly influenced by what has been said or happened previously.\n",
    "\n",
    "*Recurrent Neural Networks* (RNNs) provide neural networks with the capability to memorize previous words within a statement, enabling them to better capture and understand patterns that appear when certain tokens appear sequentially relative to other tokens. This is the fundamental premise of RNNs.\n",
    "\n",
    "## How RNNs Maintain State Across Time\n",
    "\n",
    "RNNs operate on the principle of maintaining state across time. While initially, it might seem complicated, it's essentially about giving the network a context for its current operation based on historical data.\n",
    "\n",
    "For each input fed into a standard feed-forward network, the output from one time step 't' is provided as an additional input for the next step 't+1', along with the fresh data being supplied at 't+1'. In simpler terms, you inform the network about what happened earlier alongside what is happening \"now\".\n",
    "\n",
    "This concept forms the basis of RNNs—which learn and remember over time, enabling them to better capture patterns within sequences. Understanding this is key to exploiting the power of RNNs for text analysis and other sequential data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a RNN\n",
    "\n",
    "You can visualize a recurrent net as shown in figure below:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/rnn_unrolled.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "Look at the left side. The circles are entire feedforward network layers composed of one or more neurons. The output of the hidden layer emerges from the network as normal, but it’s also set aside to be passed back in as an input to itself along with the normal input from the next time step. This feedback is represented with an arc from the output of a layer back into its own input.\n",
    "\n",
    "An easier way to see this process—and it’s more commonly shown this way—is by unrolling the net. The right side of the image above shows the network stood on its head with two unfoldings of the time variable (t), showing layers for t+1 and t+2.\n",
    "\n",
    "Each time step is represented by a column of neurons in the unrolled version of the very same neural network. It’s like looking at a screenplay or video frame of the neural net for each sample in time. The network to the right is the future version of the network on the left. The output of a hidden layer at one time step (t) is fed back into the hidden layer along with input data for the next time step (t+1) to the right. Repeat. This diagram shows two iterations of this unfolding, so three columns of neurons for t=0, t=1, and t=2.\n",
    "\n",
    "All the vertical paths in this visualization are clones, or views of the same neurons. They are the single network represented on a timeline. This visualization is helpful when talking about how information flows through the network forward and backward during backpropagation. But when looking at the three unfolded networks, remember that they’re all different snapshots of the same network with a single set of weights.\n",
    "\n",
    "### Structure of RNN: Feedforward Network Layers\n",
    "\n",
    "Viewing the left side of the image above, you'll notice circles that represent layers in a feedforward network, with each layer comprising one or more neurons. The output of the hidden layer not only moves forward through the network but also feeds back into the input of its originating layer.\n",
    "\n",
    "This recurrent feedback is illustrated by an arc looping from the layer's output back to its own input.\n",
    "\n",
    "### Unfolding Time Variable for Better Visualization\n",
    "\n",
    "To better visualize this process, we can 'unroll' the network over time. This technique, represented on the right side of the image, essentially flips the network on its head, revealing the progress of the network over two stages of the time variable (t), namely t+1 and t+2.\n",
    "\n",
    "Each time step 't' is denoted as a column of neurons in the unrolled version of our network. It can be thought of as watching successive frames of a movie, where each frame represents the state of the network at a given moment in time.\n",
    "\n",
    "### Cloned Views of Same Neurons\n",
    "\n",
    "In this representation, all vertical paths are clones or different views of the same set of neurons; they depict the same neural network captured at various points along a timeline.\n",
    "\n",
    "While this kind of representation simplifies comprehension of information flow (both forward and backward during backpropagation), it's essential to remember when looking at these multiple 'unfolded' networks: they are merely simultaneous snapshots of the same single network maintaining a consistent set of weights.\n",
    "\n",
    "\n",
    "> Recognizing an unrolled RNN as sequential instances of the same network operating over time is crucial for understanding how RNNs capture and utilize temporal information from sequences. This comprehension forms the basis of effectively utilizing the power of RNNs for sequence data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our first RNN\n",
    "\n",
    "### Our case study\n",
    "\n",
    "Let's discuss an interesting scenario. Assume you are employed at the Ombudsman Office of our University. A major part of your role involves addressing students' complaints and correspondingly communicating with the related departments. As a measure to enhance the quality of your communication, you have decided to use gender-appropriate pronouns based on the person's first name.\n",
    "\n",
    "One important goal is to avoid incorrectly gendering specific roles or offices within the university (e.g., naming the President's office as \"Gabinete do Reitor\" even when the President is a female, which happened between 2011 and 2019 at UFRN).\n",
    "\n",
    "To achieve this objective, we will be using a [dataset collected by IBGE during 2010 Census](https://brasil.io/dataset/genero-nomes/nomes/). This dataset contains a total of 90,104 names, out of which 49,274 are female and 40,830 are male. To ensure accuracy, any names that could be associated with both genders, such as \"Elias\", \"Ivani\" or \"Alison\", have been excluded from our analysis.\n",
    "\n",
    "> Interestingly, during my data exploration, I found out that there were 189,315 men and 1,387 women with the same name as mine. I'd never imagine women could be named Elias!\n",
    "\n",
    "Our aim here is to develop a Recurrent Neural Network (RNN) that can read a name letter by letter, and predict the probability of the name being either masculine or feminine. For the purpose of this project:\n",
    "\n",
    "- Each lowercase letter will be considered a token\n",
    "- The vocabulary will comprise the 26 alphabet letters\n",
    "- Any accented letters will be converted to their non-accented versions\n",
    "\n",
    "We will divide our dataset into two parts: 80% for training and 20% for validation.\n",
    "\n",
    "> Please note that while we recognize and respect the existence of non-binary gender identities, for the purposes of this exercise, we will be employing a binary classification model due to dataset limitations. Our dataset solely contains names which are classified as either masculine or feminine, hence we are confined to two classes. Maybe in the future we can work on a more inclusive model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that said, let's start by importing the necessary libraries and loading our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the unicodedata library for Unicode character database\n",
    "import unicodedata\n",
    "\n",
    "# Import PyTorch library for deep learning\n",
    "import torch\n",
    "\n",
    "# Import neural network module from PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import functional interface for neural networks from PyTorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import optimization algorithms from PyTorch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import random module for generating random numbers\n",
    "import random\n",
    "\n",
    "# Import accuracy_score, classification_report, and confusion_matrix from sklearn for evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import datetime and timedelta from datetime module for handling date and time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silmari</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jovanilde</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yorrana</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nakita</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiarle</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90098</th>\n",
       "      <td>edsmar</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90099</th>\n",
       "      <td>altenice</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90100</th>\n",
       "      <td>arthemis</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90101</th>\n",
       "      <td>mielly</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90102</th>\n",
       "      <td>geasi</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90103 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label label_str\n",
       "0        silmari      1         F\n",
       "1      jovanilde      1         F\n",
       "2        yorrana      1         F\n",
       "3         nakita      1         F\n",
       "4         tiarle      0         M\n",
       "...          ...    ...       ...\n",
       "90098     edsmar      0         M\n",
       "90099   altenice      1         F\n",
       "90100   arthemis      1         F\n",
       "90101     mielly      1         F\n",
       "90102      geasi      0         M\n",
       "\n",
       "[90103 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/names_gender.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jose\n",
      "cafe\n",
      "uber\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Step 1: Normalize the Unicode string\n",
    "    # NFKD stands for Normalization Form Compatibility Decomposition\n",
    "    # This step separates combined characters into base character and diacritical marks\n",
    "    normalized = unicodedata.normalize(\"NFKD\", name)\n",
    "\n",
    "    # Step 2: Encode to ASCII and ignore non-ASCII characters\n",
    "    # This effectively removes accents and other diacritical marks\n",
    "    ascii_encoded = normalized.encode(\"ASCII\", \"ignore\")\n",
    "\n",
    "    # Step 3: Decode back to UTF-8\n",
    "    # This converts the bytes object back into a string\n",
    "    utf8_decoded = ascii_encoded.decode(\"utf-8\")\n",
    "\n",
    "    # Step 4: Convert to lowercase\n",
    "    # This ensures consistent casing across all names\n",
    "    lowercased = utf8_decoded.lower()\n",
    "\n",
    "    return lowercased\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# This will output 'jose', removing the accent from the 'é'\n",
    "print(normalize_name(\"José\"))  # Output: jose\n",
    "print(normalize_name(\"Café\"))  # Output: cafe\n",
    "print(normalize_name(\"Über\"))  # Output: uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silmari</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jovanilde</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yorrana</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nakita</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiarle</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90098</th>\n",
       "      <td>edsmar</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90099</th>\n",
       "      <td>altenice</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90100</th>\n",
       "      <td>arthemis</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90101</th>\n",
       "      <td>mielly</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90102</th>\n",
       "      <td>geasi</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90103 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label label_str\n",
       "0        silmari      1         F\n",
       "1      jovanilde      1         F\n",
       "2        yorrana      1         F\n",
       "3         nakita      1         F\n",
       "4         tiarle      0         M\n",
       "...          ...    ...       ...\n",
       "90098     edsmar      0         M\n",
       "90099   altenice      1         F\n",
       "90100   arthemis      1         F\n",
       "90101     mielly      1         F\n",
       "90102      geasi      0         M\n",
       "\n",
       "[90103 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90103, 3)\n",
      "(90103, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df[\"name\"] = df[\"name\"].apply(normalize_name)\n",
    "df.drop_duplicates(inplace=True, subset=[\"name\"], keep=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58546</th>\n",
       "      <td>jose</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  label label_str\n",
       "58546  jose      0         M"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('name == \"jose\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30451</th>\n",
       "      <td>maria</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  label label_str\n",
       "30451  maria      1         F"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('name == \"maria\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j', 'o', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "# This will be our simple tokenizer. It will split the names into letters\n",
    "\n",
    "\n",
    "def custom_tokenizer_letters(text):\n",
    "    text = normalize_name(text)\n",
    "\n",
    "    # Convert the normalized text into a list of individual characters\n",
    "    # This approach treats each letter as a separate token\n",
    "    return list(text)\n",
    "\n",
    "\n",
    "# Example usage of the tokenizer\n",
    "print(custom_tokenizer_letters(\"José\"))\n",
    "\n",
    "# Expected output: ['j', 'o', 's', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silmari</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jovanilde</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yorrana</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nakita</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiarle</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90098</th>\n",
       "      <td>edsmar</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90099</th>\n",
       "      <td>altenice</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90100</th>\n",
       "      <td>arthemis</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90101</th>\n",
       "      <td>mielly</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90102</th>\n",
       "      <td>geasi</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90103 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label label_str\n",
       "0        silmari      1         F\n",
       "1      jovanilde      1         F\n",
       "2        yorrana      1         F\n",
       "3         nakita      1         F\n",
       "4         tiarle      0         M\n",
       "...          ...    ...       ...\n",
       "90098     edsmar      0         M\n",
       "90099   altenice      1         F\n",
       "90100   arthemis      1         F\n",
       "90101     mielly      1         F\n",
       "90102      geasi      0         M\n",
       "\n",
       "[90103 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reserve two tokens: <pad> for padding and <unk> for any unknown letter.\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# First, split the data into training and validation sets (80/20)\n",
    "# Here we use a random shuffle and split based on indices.\n",
    "df = df.sample(frac=1, random_state=271828).reset_index(drop=True)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "df_train = df.iloc[:split_idx].reset_index(drop=True)\n",
    "df_valid = df.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Build the vocabulary from training names. We loop over each name, tokenize it,\n",
    "# and collect all unique letters.\n",
    "letters = set()\n",
    "for name in df_train[\"name\"]:\n",
    "    tokens = custom_tokenizer_letters(name)\n",
    "    letters.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sorted list of letters.\n",
    "letters = sorted(list(letters))\n",
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'a': 2,\n",
       " 'b': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'g': 8,\n",
       " 'h': 9,\n",
       " 'i': 10,\n",
       " 'j': 11,\n",
       " 'k': 12,\n",
       " 'l': 13,\n",
       " 'm': 14,\n",
       " 'n': 15,\n",
       " 'o': 16,\n",
       " 'p': 17,\n",
       " 'q': 18,\n",
       " 'r': 19,\n",
       " 's': 20,\n",
       " 't': 21,\n",
       " 'u': 22,\n",
       " 'v': 23,\n",
       " 'w': 24,\n",
       " 'x': 25,\n",
       " 'y': 26,\n",
       " 'z': 27}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary dictionary: index mapping for each letter.\n",
    "# Reserve index 0 for PAD and index 1 for UNK.\n",
    "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, letter in enumerate(letters, start=2):\n",
    "    vocab[letter] = i\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'<pad>': 0, '<unk>': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
      "Vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "# Create an inverse mapping (optional)\n",
    "itos = {i: s for s, i in vocab.items()}\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions for Conversion\n",
    "\n",
    "\n",
    "def tokenize_and_convert(name, vocab):\n",
    "    \"\"\"\n",
    "    Given a name string, tokenize into letters and convert each letter into its corresponding index.\n",
    "    Unknown letters are replaced with the index for <unk>.\n",
    "    \"\"\"\n",
    "    tokens = custom_tokenizer_letters(name)\n",
    "    indices = [vocab.get(token, vocab[UNK_TOKEN]) for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "def label_to_int(label_str):\n",
    "    \"\"\"\n",
    "    Convert label string to integer.\n",
    "    We'll assume that in the CSV the label is given as \"F\" for female and \"M\" for male.\n",
    "    (If your CSV uses other encodings, adjust this function accordingly.)\n",
    "    Here we map 'M' -> 0 and 'F' -> 1.\n",
    "    \"\"\"\n",
    "    return 0 if label_str.strip().upper() == \"M\" else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          df: DataFrame with columns 'name' and 'label'\n",
    "          vocab: letter to index dictionary\n",
    "        \"\"\"\n",
    "        self.names = df[\"name\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        # Convert name to list of token indices\n",
    "        name_tensor = tokenize_and_convert(name, self.vocab)\n",
    "        # Also get the label as an integer.\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        # Return the tensor, its length, and the label.\n",
    "        return name_tensor, len(name_tensor), label_tensor\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NamesDataset(df_train, vocab)\n",
    "valid_dataset = NamesDataset(df_valid, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>francileda</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biaka</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artemira</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oberico</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eliziene</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72077</th>\n",
       "      <td>tindaro</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72078</th>\n",
       "      <td>diuliene</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72079</th>\n",
       "      <td>damaris</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72080</th>\n",
       "      <td>divonzir</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72081</th>\n",
       "      <td>nerlandia</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72082 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  label label_str\n",
       "0      francileda      1         F\n",
       "1           biaka      1         F\n",
       "2        artemira      1         F\n",
       "3         oberico      0         M\n",
       "4        eliziene      1         F\n",
       "...           ...    ...       ...\n",
       "72077     tindaro      0         M\n",
       "72078    diuliene      1         F\n",
       "72079     damaris      1         F\n",
       "72080    divonzir      0         M\n",
       "72081   nerlandia      1         F\n",
       "\n",
       "[72082 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a list of (name, label) tuples in string/int format for later evaluation.\n",
    "names_train = [\n",
    "    (name, str(label)) for name, label in zip(df_train[\"name\"], df_train[\"label\"])\n",
    "]\n",
    "names_valid = [\n",
    "    (name, str(label)) for name, label in zip(df_valid[\"name\"], df_valid[\"label\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size 72082\n",
      "Valid dataset size 18021\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size\", len(train_dataset))\n",
    "print(\"Valid dataset size\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7, 19,  2, 15,  4, 10, 13,  6,  5,  2]), 10, tensor(1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Custom Collate Function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Expects a list of tuples: (name_tensor, name_length, label_tensor)\n",
    "    This function pads the name tensors to the length of the longest name and returns:\n",
    "      names_padded: tensor [max_seq_length, batch_size]\n",
    "      lengths: tensor of sequence lengths\n",
    "      labels: tensor of labels\n",
    "    \"\"\"\n",
    "    # Separate out the fields.\n",
    "    name_tensors, lengths, labels = zip(*batch)\n",
    "    # Pad the sequences. (The output will be of shape: [batch_size, max_seq_len])\n",
    "    names_padded = pad_sequence(\n",
    "        name_tensors, batch_first=False, padding_value=vocab[PAD_TOKEN]\n",
    "    )\n",
    "    # Convert lengths and labels to tensors.\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    labels = torch.stack(labels)\n",
    "    return names_padded, lengths, labels\n",
    "\n",
    "\n",
    "# Create DataLoader objects.\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amarantino', '0'),\n",
       " ('sebila', '1'),\n",
       " ('anaili', '1'),\n",
       " ('mauricius', '0'),\n",
       " ('estefesson', '0'),\n",
       " ('deonize', '1'),\n",
       " ('antonios', '0'),\n",
       " ('eronisa', '1'),\n",
       " ('vicencio', '0'),\n",
       " ('melquias', '0')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(names_valid, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atalirio', '0'),\n",
       " ('dionato', '0'),\n",
       " ('arioneide', '1'),\n",
       " ('leicia', '1'),\n",
       " ('claonice', '1'),\n",
       " ('jilmario', '0'),\n",
       " ('tharcila', '1'),\n",
       " ('meize', '1'),\n",
       " ('marsol', '0'),\n",
       " ('nesmar', '0')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(names_train, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2, 19, 21,  6, 14, 10, 19,  2]), 8, tensor(1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 39345, 0: 32737})\n",
      "Counter({1: 9929, 0: 8092})\n"
     ]
    }
   ],
   "source": [
    "# Import the Counter class from the collections module\n",
    "from collections import Counter\n",
    "\n",
    "# Use a list comprehension to extract the gender labels from the training and validation datasets\n",
    "# The Counter class is then used to count the frequency of each label (0 for male, 1 for female)\n",
    "# The resulting counts are printed to the console\n",
    "print(Counter([label.item() for _, _, label in train_loader.dataset]))\n",
    "print(Counter([label.item() for _, _, label in valid_loader.dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72082, 18021)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a GPU is available for PyTorch\n",
    "# torch.cuda.is_available() returns True if a GPU is available, otherwise False\n",
    "# If a GPU is available, set the device to 'cuda' to utilize the GPU for computations\n",
    "# If a GPU is not available, set the device to 'cpu' to use the CPU for computations\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Define a class for our RNN model\n",
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Define an RNN layer to encode our sequence of letters\n",
    "        self.encoder = nn.RNN(\n",
    "            input_size=embedding_dim,  # Size of each input vector\n",
    "            hidden_size=hidden_size,  # Number of features in the hidden state\n",
    "            num_layers=2,  # Number of recurrent layers\n",
    "            dropout=0.3,  # Dropout probability for the RNN layers\n",
    "            bidirectional=False,  # Whether the RNN is bidirectional\n",
    "        )\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),  # First linear layer\n",
    "            self.dropout,  # Dropout layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Linear(\n",
    "                hidden_size // 2, 2\n",
    "            ),  # Second linear layer, output size is 2 for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Encode the embedded sequence using the RNN layer\n",
    "        packed_output, hidden = self.encoder(embedded)\n",
    "\n",
    "        # Pass the final hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden[-1])\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = vocab[PAD_TOKEN]\n",
    "pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = NameRNN(\n",
    "    hidden_size=50,  # Hidden state size of the RNN\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we've selected an arbitrary hidden size of 50 and an embedding dimension of 25. These choices were made largely to minimize computation time; however, adjusting these parameters can yield different results in both computational speed and model accuracy. Hence, it's encouraged that you experiment with varying these numbers based on your specific use-cases.\n",
    "\n",
    "#### Rule of Thumb for Model Complexity\n",
    "\n",
    "A helpful guiding principle when designing models is to ensure that the complexity of your model aligns appropriately with your data's innate structure. The objective is to achieve a balance where:\n",
    "\n",
    "1. Your model isn't too complex for your data (Overfitting), and\n",
    "2. It's not too simple relative to your data (Underfitting).\n",
    "\n",
    "Let's understand what this means:\n",
    "\n",
    "##### Overfitting - High Variance and Low Bias\n",
    "\n",
    "When a model is overly complex, it tends to \"memorize\" the training data rather than \"learning\" from it, causing poor generalization when faced with new, unseen data. This scenario is referred to as **overfitting** the data and results in a model with high variance and low bias.\n",
    "\n",
    "##### Underfitting - Low Variance and High Bias\n",
    "\n",
    "Conversely, if a model is too uncomplicated, it will fail even in capturing the fundamental patterns of the training data. Such underutilizing models are prone to consistently generate inaccurate predictions across all data types, both seen and unseen. This sub-optimal situation is known as **underfitting** and leads to a model with low variance and high bias.\n",
    "\n",
    "#### Balancing Between Bias and Variance\n",
    "\n",
    "Balancing between overfitting and underfitting is often described as managing the trade-off between bias and variance. The key is to find a sweet spot where the model is just complex enough to learn useful patterns from the training data but also retains the ability to generalize effectively to unseen data.\n",
    "\n",
    "While building your model, it's essential to keep this concept in mind: experiment with different parameters, monitor how they affect the performance of your model, and fine-tune them for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,977 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    # Sum the number of elements (numel) for each parameter in the model\n",
    "    # Only include parameters that require gradients (i.e., are trainable)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Print the total number of trainable parameters in a human-readable format\n",
    "    print(f\"The model has {n_parameters:,} trainable parameters\")\n",
    "\n",
    "    # Return the total number of trainable parameters\n",
    "    return n_parameters\n",
    "\n",
    "\n",
    "# Count the number of trainable parameters in the model_rnn instance\n",
    "n_parameters = count_parameters(model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device,\n",
    "    checkpoint_fname,\n",
    "    verbose=True,\n",
    "):\n",
    "    if verbose:\n",
    "        count_parameters(model)\n",
    "    start_time = datetime.now()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            names, name_len, labels = batch\n",
    "            # Move to device\n",
    "            names, name_len, labels = (\n",
    "                names.to(device),\n",
    "                name_len.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            outputs = model(names, name_len)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Multiply by the number of examples in the batch (i.e. batch size)\n",
    "            training_loss += loss.item() * names.size(1)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                names, name_len, labels = batch\n",
    "                names, name_len, labels = (\n",
    "                    names.to(device),\n",
    "                    name_len.to(device),\n",
    "                    labels.to(device),\n",
    "                )\n",
    "                outputs = model(names, name_len)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item() * names.size(1)\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_fname)\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Epoch {epoch} - Training Loss: {training_loss:.4f} - Valid Loss: {valid_loss:.4f} - New Best\"\n",
    "                )\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Epoch {epoch} - Training Loss: {training_loss:.4f} - Valid Loss: {valid_loss:.4f}\"\n",
    "                )\n",
    "    elapsed_time = datetime.now() - start_time\n",
    "    if verbose:\n",
    "        print(f\"Time elapsed: {elapsed_time}\")\n",
    "        print(f\"Mean time per epoch: {elapsed_time / epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a function to predict the gender label for a given name using a trained model\n",
    "# The function takes three arguments:\n",
    "# - name: the name to predict the label for\n",
    "# - model: the trained model to use for prediction\n",
    "# - device: the device to use for computation (default is 'cpu')\n",
    "\n",
    "\n",
    "def predict(name, model, vocab, device=\"cpu\"):\n",
    "    # Convert the input name into token indices and then a tensor.\n",
    "    name_tensor = tokenize_and_convert(name, vocab)\n",
    "    name_len = torch.tensor([len(name_tensor)])\n",
    "    # Add a batch dimension (tensor shape [seq_len] becomes [seq_len, 1]).\n",
    "    name_tensor = name_tensor.unsqueeze(1)\n",
    "    name_tensor = name_tensor.to(device)\n",
    "    name_len = name_len.to(device)\n",
    "    logits = model(name_tensor, name_len)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    # We use a small dictionary that maps {0: 'M', 1: 'F'}\n",
    "    result_dict = {0: \"M\", 1: \"F\"}\n",
    "    label = result_dict[probabilities.argmax().item()]\n",
    "    return [label, probabilities.max().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Path class from the pathlib module to handle file system paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path where the model checkpoints will be saved\n",
    "checkpoint_path = Path(\"./outputs/rnns/\")\n",
    "\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,977 trainable parameters\n",
      "Epoch 1 - Training Loss: 0.3567 - Valid Loss: 0.2522 - New Best\n",
      "Epoch 2 - Training Loss: 0.2523 - Valid Loss: 0.2093 - New Best\n",
      "Epoch 3 - Training Loss: 0.2263 - Valid Loss: 0.1956 - New Best\n",
      "Epoch 4 - Training Loss: 0.2144 - Valid Loss: 0.1963\n",
      "Epoch 5 - Training Loss: 0.2045 - Valid Loss: 0.1760 - New Best\n",
      "Epoch 6 - Training Loss: 0.1954 - Valid Loss: 0.1735 - New Best\n",
      "Epoch 7 - Training Loss: 0.1904 - Valid Loss: 0.1629 - New Best\n",
      "Epoch 8 - Training Loss: 0.1838 - Valid Loss: 0.1624 - New Best\n",
      "Epoch 9 - Training Loss: 0.1787 - Valid Loss: 0.1548 - New Best\n",
      "Epoch 10 - Training Loss: 0.1747 - Valid Loss: 0.1551\n",
      "Epoch 11 - Training Loss: 0.1713 - Valid Loss: 0.1488 - New Best\n",
      "Epoch 12 - Training Loss: 0.1684 - Valid Loss: 0.1476 - New Best\n",
      "Epoch 13 - Training Loss: 0.1656 - Valid Loss: 0.1452 - New Best\n",
      "Epoch 14 - Training Loss: 0.1623 - Valid Loss: 0.1502\n",
      "Epoch 15 - Training Loss: 0.1604 - Valid Loss: 0.1468\n",
      "Epoch 16 - Training Loss: 0.1602 - Valid Loss: 0.1362 - New Best\n",
      "Epoch 17 - Training Loss: 0.1562 - Valid Loss: 0.1412\n",
      "Epoch 18 - Training Loss: 0.1548 - Valid Loss: 0.1341 - New Best\n",
      "Epoch 19 - Training Loss: 0.1541 - Valid Loss: 0.1332 - New Best\n",
      "Epoch 20 - Training Loss: 0.1517 - Valid Loss: 0.1341\n",
      "Epoch 21 - Training Loss: 0.1487 - Valid Loss: 0.1309 - New Best\n",
      "Epoch 22 - Training Loss: 0.1478 - Valid Loss: 0.1289 - New Best\n",
      "Epoch 23 - Training Loss: 0.1463 - Valid Loss: 0.1272 - New Best\n",
      "Epoch 24 - Training Loss: 0.1468 - Valid Loss: 0.1248 - New Best\n",
      "Epoch 25 - Training Loss: 0.1446 - Valid Loss: 0.1263\n",
      "Epoch 26 - Training Loss: 0.1440 - Valid Loss: 0.1229 - New Best\n",
      "Epoch 27 - Training Loss: 0.1413 - Valid Loss: 0.1258\n",
      "Epoch 28 - Training Loss: 0.1416 - Valid Loss: 0.1209 - New Best\n",
      "Epoch 29 - Training Loss: 0.1403 - Valid Loss: 0.1211\n",
      "Epoch 30 - Training Loss: 0.1388 - Valid Loss: 0.1219\n",
      "Time elapsed: 0:03:13.066951\n",
      "Mean time per epoch: 0:00:06.435565\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best model checkpoint\n",
    "# The checkpoint will be saved in the previously defined checkpoint_path directory\n",
    "checkpoint_fname = checkpoint_path / \"bestRNN.pt\"\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_rnn)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_loader: The iterator for the training dataset\n",
    "# - valid_loader: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(\n",
    "    30,\n",
    "    model_rnn,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device,\n",
    "    checkpoint_fname,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the NameRNN model with the same hyperparameters as the trained model\n",
    "# This ensures that the model architecture matches the one used during training\n",
    "model_rnn_inference = NameRNN(\n",
    "    hidden_size=50,  # Hidden state size of the RNN\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_rnn_inference.load_state_dict(torch.load(checkpoint_fname, weights_only=False))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_rnn_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_rnn_inference = model_rnn_inference.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 0.9914980530738831]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"joana\", model_rnn_inference, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 0.995205283164978]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"joão\", model_rnn_inference, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 0.9976400136947632]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"maria\", model_rnn_inference, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 0.9918715953826904]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"marcos\", model_rnn_inference, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('francileda', '1') ['F', 0.9985161423683167]\n",
      "('biaka', '1') ['F', 0.9904242157936096]\n",
      "('artemira', '1') ['F', 0.9970412850379944]\n",
      "('oberico', '0') ['M', 0.9997696280479431]\n",
      "('eliziene', '1') ['F', 0.9995582699775696]\n",
      "('hannha', '1') ['F', 0.9961289167404175]\n",
      "('jocela', '1') ['F', 0.9954225420951843]\n",
      "('dejaina', '1') ['F', 0.9992408752441406]\n",
      "('avimar', '0') ['M', 0.8921133875846863]\n",
      "('kathen', '1') ['F', 0.548227846622467]\n",
      "('maua', '0') ['F', 0.979082465171814]\n",
      "('udo', '0') ['M', 0.9996289014816284]\n",
      "('thalila', '1') ['F', 0.999546468257904]\n",
      "('joseri', '0') ['M', 0.7859906554222107]\n",
      "('lurivaldo', '0') ['M', 0.9991143345832825]\n",
      "('zaiara', '1') ['F', 0.9980607628822327]\n",
      "('periclis', '0') ['M', 0.8826977014541626]\n",
      "('jaquiline', '1') ['F', 0.9952273368835449]\n",
      "('izenaide', '1') ['F', 0.9973949193954468]\n",
      "('luzinette', '1') ['F', 0.998856782913208]\n",
      "('uerlan', '0') ['M', 0.9329925775527954]\n",
      "('dharlan', '0') ['M', 0.9563585519790649]\n",
      "('armelindo', '0') ['M', 0.9952217936515808]\n",
      "('hersilio', '0') ['M', 0.9993630051612854]\n",
      "('dejanir', '1') ['M', 0.8366883397102356]\n",
      "('martides', '1') ['F', 0.5004174113273621]\n",
      "('katllyn', '1') ['F', 0.7809947729110718]\n",
      "('eudezio', '0') ['M', 0.999142050743103]\n",
      "('valbio', '0') ['M', 0.9997732043266296]\n",
      "('firmiana', '1') ['F', 0.9989032745361328]\n",
      "('nilcilei', '1') ['F', 0.9775654077529907]\n",
      "('erlam', '0') ['M', 0.9251142144203186]\n",
      "('geverson', '0') ['M', 0.9998162388801575]\n",
      "('robernaldo', '0') ['M', 0.9998149275779724]\n",
      "('edineuda', '1') ['F', 0.9951103329658508]\n",
      "('handel', '0') ['M', 0.9531697630882263]\n",
      "('raiel', '0') ['M', 0.726701557636261]\n",
      "('wochiton', '0') ['M', 0.9996716976165771]\n",
      "('loreca', '1') ['F', 0.9983181953430176]\n",
      "('criselidia', '1') ['F', 0.9990835189819336]\n",
      "('manildo', '0') ['M', 0.996656060218811]\n",
      "('valciana', '1') ['F', 0.9986394047737122]\n",
      "('eudemar', '0') ['M', 0.6157037019729614]\n",
      "('carusa', '1') ['F', 0.9923105239868164]\n",
      "('alcionor', '0') ['M', 0.9997945427894592]\n",
      "('eloysa', '1') ['F', 0.9995354413986206]\n",
      "('zoelia', '1') ['F', 0.9997394680976868]\n",
      "('zelza', '1') ['F', 0.9993687272071838]\n",
      "('deonizia', '1') ['F', 0.9982563853263855]\n",
      "('julierme', '0') ['M', 0.9310592412948608]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the first 50 names in the training dataset\n",
    "for i in names_train[:50]:\n",
    "    # Print the name and its predicted gender label and probability\n",
    "    # The predict function takes the name, the trained model, and the device ('cpu') as arguments\n",
    "    print(i, predict(i[0], model_rnn_inference, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map label indices to their corresponding gender labels\n",
    "# '1' corresponds to 'F' (Female) and '0' corresponds to 'M' (Male)\n",
    "label_mapping = {\"1\": \"F\", \"0\": \"M\"}\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "# The predict function takes the name, the trained model, and the device ('cpu') as arguments\n",
    "pred_valid = [predict(i[0], model_rnn_inference, vocab)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the names where the model's prediction is incorrect\n",
    "failed = []\n",
    "\n",
    "# Loop through the validation dataset\n",
    "for i in range(len(true_valid)):\n",
    "    # Compare the true gender label with the predicted gender label\n",
    "    if true_valid[i] != pred_valid[i]:\n",
    "        # If the labels do not match, add the name to the failed list\n",
    "        error_name = names_valid[i][0]\n",
    "        failed.append((error_name, true_valid[i], pred_valid[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carem', 'F', 'M'),\n",
       " ('marlui', 'F', 'M'),\n",
       " ('michico', 'F', 'M'),\n",
       " ('alderige', 'M', 'F'),\n",
       " ('caici', 'M', 'F'),\n",
       " ('clautenes', 'F', 'M'),\n",
       " ('cleacir', 'M', 'F'),\n",
       " ('kenad', 'M', 'F'),\n",
       " ('itagibe', 'M', 'F'),\n",
       " ('yaeco', 'F', 'M')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(failed, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9312468786415848\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.94      0.93      0.94      9929\n",
      "           M       0.92      0.93      0.92      8092\n",
      "\n",
      "    accuracy                           0.93     18021\n",
      "   macro avg       0.93      0.93      0.93     18021\n",
      "weighted avg       0.93      0.93      0.93     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9268  661]\n",
      " [ 578 7514]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(true_valid, pred_valid))\n",
    "print(f\"Classification Report:\\n {classification_report(true_valid, pred_valid)}\")\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory (LSTM) Networks: An In-Depth Overview\n",
    "\n",
    "\n",
    "### Challenges with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Traditional RNNs maintain a state across multiple time steps, which helps process sequential data. However, they often struggle with **long-term dependencies** because of two main issues:\n",
    "\n",
    "- **Vanishing Gradient Problem**  \n",
    "  During training, gradients (derivatives) are passed backward through layers. When these gradients are multiplied repeatedly, they may shrink exponentially, especially if the weight matrices have values smaller than 1. This results in near-zero gradients, making it hard for the network to capture dependencies that are far apart in the sequence.\n",
    "\n",
    "  > **Key Point:**  \n",
    "  In backpropagation through time, the repeated multiplication of small gradient values leads to a vanishing effect, reducing the network's ability to learn from long-distance connections.\n",
    "\n",
    "- **Exploding Gradient Problem**  \n",
    "  In contrast, if the weight matrices contain values larger than 1, the gradient values can grow exponentially, leading to very large values. This makes the training process unstable.\n",
    "\n",
    "  > **Important Note:**  \n",
    "  Both issues arise from the chain rule in gradient calculation, where repeated multiplication can either excessively diminish or excessively magnify the gradient.\n",
    "\n",
    "\n",
    "**Illustrative Example of Long-Term Dependencies:**  \n",
    "Consider the sentence:  \n",
    "\n",
    "> \"O cachorro passou o dia brincando .......... estava cansado.\"  \n",
    "\n",
    "Here, the word \"estava\" depends on \"cachorro\" even when they are many words apart. Standard RNNs often struggle to capture such distant relationships due to the vanishing and exploding gradient problems.\n",
    "\n",
    "\n",
    "### Introduction to LSTM Networks\n",
    "\n",
    "LSTM networks were designed to overcome these challenges in RNNs. They excel in capturing long-term dependencies by using a more sophisticated cell structure with dedicated mechanisms to regulate information flow. The concept was introduced in 1997 by [Hochreiter and Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "\n",
    "### LSTM Network Architecture\n",
    "\n",
    "An LSTM network is composed of a memory cell along with three critical gating mechanisms: the **input gate**, **output gate**, and **forget gate**. The unique design of these gates allows the network to effectively store and manage information over long sequences.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/lstm.jpeg\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### The Memory Cell\n",
    "\n",
    "- **Cell State:**  \n",
    "  Represented by a thick line in the cell diagram, the cell state serves as the internal memory. It carries information forward through time steps, with its value updated by various gates.\n",
    "\n",
    "- **Candidate Cell State Calculation:**  \n",
    "  The candidate information for updating the cell state is computed using:\n",
    "  $$\n",
    "  \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "  $$\n",
    "  - $ \\tilde{C}_t $: Candidate cell state at time $ t $\n",
    "  - $ \\tanh $: Activation function compressing values between -1 and 1\n",
    "  - $ W_C $: Weight matrix for the candidate information\n",
    "  - $ [h_{t-1}, x_t] $: Concatenation of the previous hidden state and current input\n",
    "  - $ b_C $: Bias vector\n",
    "\n",
    "- **Updating the Cell State:**  \n",
    "  The current cell state is updated using the equation:\n",
    "  $$\n",
    "  C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "  $$\n",
    "  - $ C_t $: New cell state at time $ t $\n",
    "  - $ f_t $: Forget gate output (determines what to discard from the previous cell state)\n",
    "  - $ i_t $: Input gate output (determines how much of the candidate state to add)\n",
    "\n",
    "#### The Hidden State\n",
    "\n",
    "The hidden state contains processed information passed to the next time step. It is calculated based on the updated cell state:\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "$$\n",
    "- $ h_t $: Hidden state at time $ t $\n",
    "- $ o_t $: Output gate output (controls which parts of the cell state are output)\n",
    "- $ \\tanh(C_t) $: Non-linear transformation of the cell state\n",
    "\n",
    "---\n",
    "\n",
    "### LSTM Gates in Detail\n",
    "\n",
    "Each gate in the LSTM has a distinct role, executed via a combination of a sigmoid activation and a pointwise multiplication operation. The sigmoid activation outputs values between 0 and 1, which act as filtering factors.\n",
    "\n",
    "#### The Input Gate\n",
    "\n",
    "- **Purpose:**  \n",
    "  Regulates the amount of new information to be added to the cell state.\n",
    "  \n",
    "- **Computation:**\n",
    "  $$\n",
    "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "  $$\n",
    "  - $ i_t $: Input gate output at time $ t $\n",
    "  - $ \\sigma $: Sigmoid function (outputs between 0 and 1)\n",
    "  - $ W_i $: Weight matrix for the input gate\n",
    "  - $ b_i $: Bias vector\n",
    "\n",
    "- **Function:**  \n",
    "  Values near 0 prevent new information from entering the cell, while values near 1 allow it.\n",
    "\n",
    "#### The Output Gate\n",
    "\n",
    "- **Purpose:**  \n",
    "  Controls what information from the cell state is used to compute the next hidden state.\n",
    "\n",
    "- **Computation:**\n",
    "  $$\n",
    "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "  $$\n",
    "  - $ o_t $: Output gate output at time $ t $\n",
    "  - $ W_o $: Weight matrix for the output gate\n",
    "  - $ b_o $: Bias vector\n",
    "\n",
    "- **Function:**  \n",
    "  Regulates the amount of cell state information that contributes to the hidden state $ h_t $.\n",
    "\n",
    "#### The Forget Gate\n",
    "\n",
    "- **Purpose:**  \n",
    "  Determines which parts of the previous cell state should be discarded.\n",
    "\n",
    "- **Computation:**\n",
    "  $$\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $$\n",
    "  - $ f_t $: Forget gate output at time $ t $\n",
    "  - $ W_f $: Weight matrix for the forget gate\n",
    "  - $ b_f $: Bias vector\n",
    "\n",
    "- **Function:**  \n",
    "  A value close to 0 eliminates information from the previous cell state, while a value near 1 retains it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Define an LSTM layer to encode our sequence of letters\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0.3,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),  # input size is the hidden size\n",
    "            self.dropout,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                hidden_size // 2, 2\n",
    "            ),  # output size is 2 because we're predicting binary gender\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Encode the embedded sequence using the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.encoder(embedded)\n",
    "\n",
    "        # Pass the final hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden[-1])\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,827 trainable parameters\n",
      "Epoch 1 - Training Loss: 0.3191 - Valid Loss: 0.2216 - New Best\n",
      "Epoch 2 - Training Loss: 0.2257 - Valid Loss: 0.1897 - New Best\n",
      "Epoch 3 - Training Loss: 0.2002 - Valid Loss: 0.1758 - New Best\n",
      "Epoch 4 - Training Loss: 0.1840 - Valid Loss: 0.1589 - New Best\n",
      "Epoch 5 - Training Loss: 0.1733 - Valid Loss: 0.1511 - New Best\n",
      "Epoch 6 - Training Loss: 0.1654 - Valid Loss: 0.1489 - New Best\n",
      "Epoch 7 - Training Loss: 0.1590 - Valid Loss: 0.1516\n",
      "Epoch 8 - Training Loss: 0.1543 - Valid Loss: 0.1353 - New Best\n",
      "Epoch 9 - Training Loss: 0.1465 - Valid Loss: 0.1420\n",
      "Epoch 10 - Training Loss: 0.1426 - Valid Loss: 0.1421\n",
      "Epoch 11 - Training Loss: 0.1384 - Valid Loss: 0.1303 - New Best\n",
      "Epoch 12 - Training Loss: 0.1357 - Valid Loss: 0.1280 - New Best\n",
      "Epoch 13 - Training Loss: 0.1325 - Valid Loss: 0.1183 - New Best\n",
      "Epoch 14 - Training Loss: 0.1286 - Valid Loss: 0.1180 - New Best\n",
      "Epoch 15 - Training Loss: 0.1261 - Valid Loss: 0.1186\n",
      "Epoch 16 - Training Loss: 0.1260 - Valid Loss: 0.1131 - New Best\n",
      "Epoch 17 - Training Loss: 0.1216 - Valid Loss: 0.1173\n",
      "Epoch 18 - Training Loss: 0.1173 - Valid Loss: 0.1192\n",
      "Epoch 19 - Training Loss: 0.1171 - Valid Loss: 0.1080 - New Best\n",
      "Epoch 20 - Training Loss: 0.1155 - Valid Loss: 0.1070 - New Best\n",
      "Epoch 21 - Training Loss: 0.1155 - Valid Loss: 0.1050 - New Best\n",
      "Epoch 22 - Training Loss: 0.1130 - Valid Loss: 0.1037 - New Best\n",
      "Epoch 23 - Training Loss: 0.1106 - Valid Loss: 0.1052\n",
      "Epoch 24 - Training Loss: 0.1089 - Valid Loss: 0.1024 - New Best\n",
      "Epoch 25 - Training Loss: 0.1063 - Valid Loss: 0.1005 - New Best\n",
      "Epoch 26 - Training Loss: 0.1049 - Valid Loss: 0.1006\n",
      "Epoch 27 - Training Loss: 0.1038 - Valid Loss: 0.0982 - New Best\n",
      "Epoch 28 - Training Loss: 0.1029 - Valid Loss: 0.0974 - New Best\n",
      "Epoch 29 - Training Loss: 0.1012 - Valid Loss: 0.1021\n",
      "Epoch 30 - Training Loss: 0.1008 - Valid Loss: 0.1035\n",
      "Time elapsed: 0:03:05.399267\n",
      "Mean time per epoch: 0:00:06.179976\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best LSTM model checkpoint\n",
    "checkpoint_fname = checkpoint_path / \"bestLSTM.pt\"\n",
    "\n",
    "# Create an instance of the NameLSTM model with specified hyperparameters\n",
    "model_lstm = NameLSTM(\n",
    "    hidden_size=50,  # Hidden state size of the LSTM\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_lstm)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_iterator: The iterator for the training dataset\n",
    "# - valid_iterator: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(\n",
    "    30,\n",
    "    model_lstm,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device,\n",
    "    checkpoint_fname,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the NameLSTM model with the same hyperparameters as the trained model\n",
    "model_lstm_inference = NameLSTM(\n",
    "    hidden_size=50,  # Hidden state size of the LSTM\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_lstm_inference.load_state_dict(torch.load(checkpoint_fname))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_lstm_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_lstm_inference = model_lstm_inference.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    \"1\": \"F\",\n",
    "    \"0\": \"M\",\n",
    "}  # Map label indices to gender labels ('1' -> 'F', '0' -> 'M')\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "pred_valid = [predict(i[0], model_lstm_inference, vocab)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.947949614338827\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.95      0.95      0.95      9929\n",
      "           M       0.94      0.94      0.94      8092\n",
      "\n",
      "    accuracy                           0.95     18021\n",
      "   macro avg       0.95      0.95      0.95     18021\n",
      "weighted avg       0.95      0.95      0.95     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9474  455]\n",
      " [ 483 7609]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(true_valid, pred_valid))\n",
    "print(f\"Classification Report:\\n {classification_report(true_valid, pred_valid)}\")\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Recurrent Networks\n",
    "\n",
    "In previous discussions, we explored the effectiveness of LSTM networks in capturing long-term dependencies within sequential data.  However, a fundamental characteristic of standard LSTM networks is their unidirectional nature. They process information in a single direction – typically from the beginning to the end of the sequence. While this approach is suitable for many applications, it presents limitations when the context from both past and future elements is crucial for understanding a specific point in the sequence.\n",
    "\n",
    "Consider the Portuguese sentence provided earlier:\n",
    "\n",
    "> \"O cachorro passou o dia brincando .......... estava cansado.\"\n",
    "\n",
    "As highlighted, determining the correct form of the verb \"estava\" (was) benefits significantly from understanding both preceding context like \"cachorro\" (dog) and succeeding context like \"cansado\" (tired). A conventional LSTM, processing the sentence from left to right, might struggle to fully capture the dependency between \"estava\" and \"cansado\" because it processes \"cansado\" *after* \"estava\".\n",
    "\n",
    "To address this limitation, **bidirectional recurrent networks**, particularly Bidirectional LSTMs (Bi-LSTMs), are introduced. These architectures overcome the unidirectional constraint by processing the input sequence in both forward and reverse directions.\n",
    "\n",
    "\n",
    "### Understanding Bidirectional LSTM Architecture\n",
    "\n",
    "A Bidirectional LSTM network is composed of two independent LSTM layers operating in parallel. One LSTM layer processes the input sequence in the **forward direction** (from the start to the end), just like a standard LSTM.  Simultaneously, a second LSTM layer processes the input sequence in the **backward direction** (from the end to the start).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/bidirectional_lstm.webp\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "It's crucial to note that while we focus on LSTMs, the concept of bidirectionality is applicable to all forms of recurrent neural networks, including simpler RNNs and GRUs. Any recurrent network can be configured to operate bidirectionally to gain access to both past and future context.\n",
    "\n",
    "### Deep Dive into the Dual Processing Mechanism\n",
    "\n",
    "Imagine a bidirectional network as having two \"reading heads\" scanning the input sequence simultaneously from opposite ends.\n",
    "\n",
    "- **Forward Layer:** This layer functions identically to a standard LSTM. It reads the input sequence $x = (x_1, x_2, ..., x_T)$ in its natural order, from $x_1$ to $x_T$. At each time step $t$, the forward layer LSTM computes a hidden state $\\overrightarrow{h}_t$ based on the current input $x_t$ and the previous forward hidden state $\\overrightarrow{h}_{t-1}$. This layer is effective at capturing dependencies based on preceding words or elements in the sequence. We can represent the forward pass computation as:\n",
    "\n",
    "    $$\n",
    "    \\overrightarrow{h}_t = \\text{LSTM}_{\\text{forward}}(x_t, \\overrightarrow{h}_{t-1})\n",
    "    $$\n",
    "\n",
    "- **Backward Layer:** This is the distinguishing feature of bidirectional networks.  This layer processes the sequence in reverse order, effectively reading from $x_T$ back to $x_1$.  At each time step $t$, the backward layer LSTM calculates a hidden state $\\overleftarrow{h}_t$ based on the input $x_t$ and the *next* backward hidden state $\\overleftarrow{h}_{t+1}$.  This allows the network to incorporate information from the *future* of the sequence relative to the current position. The backward pass computation can be formulated as:\n",
    "\n",
    "    $$\n",
    "    \\overleftarrow{h}_t = \\text{LSTM}_{\\text{backward}}(x_t, \\overleftarrow{h}_{t+1})\n",
    "    $$\n",
    "\n",
    "After both the forward and backward layers have processed the entire sequence, their outputs are combined to produce the final output for each time step. This combination, often achieved through **concatenation** or **addition**, merges the past and future context representations. For example, if we choose concatenation, the output $h_t$ at time step $t$ is given by:\n",
    "\n",
    "$$\n",
    "h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]\n",
    "$$\n",
    "\n",
    "where $ [;] $ denotes concatenation.\n",
    "\n",
    "This dual processing mechanism allows bidirectional networks to consider the entire context surrounding each element in the sequence. By having access to both past and future information, they are particularly powerful in tasks where understanding the full context is crucial for accurate predictions. Examples of such tasks, as mentioned earlier, include:\n",
    "\n",
    "- **Language Translation:**  Understanding the context on both sides of a word is vital for selecting the most appropriate translation.\n",
    "- **Text Generation:**  Generating coherent and contextually relevant text requires awareness of both preceding and subsequent parts of the sentence or paragraph.\n",
    "- **Speech Recognition:**  Recognizing spoken words accurately often depends on the surrounding phonetic context, both before and after the word in question.\n",
    "\n",
    "Fundamentally, bidirectional networks provide a more complete understanding of sequential data by looking in both directions, overcoming a key limitation of traditional unidirectional recurrent networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLSTMBidir(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Define a bidirectional LSTM layer to encode our sequence of letters\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                hidden_size * 2, hidden_size // 2\n",
    "            ),  # input size is the hidden size times 2 because of bidirectionality\n",
    "            self.dropout,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                hidden_size // 2, 2\n",
    "            ),  # output size is 2 because we're predicting binary gender\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Encode the embedded sequence using the bidirectional LSTM layer\n",
    "        packed_output, (hidden, cell) = self.encoder(embedded)\n",
    "\n",
    "        # Concatenate the final hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "\n",
    "        # Pass the concatenated hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 94,877 trainable parameters\n",
      "Epoch 1 - Training Loss: 0.3085 - Valid Loss: 0.2127 - New Best\n",
      "Epoch 2 - Training Loss: 0.2157 - Valid Loss: 0.1766 - New Best\n",
      "Epoch 3 - Training Loss: 0.1865 - Valid Loss: 0.1625 - New Best\n",
      "Epoch 4 - Training Loss: 0.1710 - Valid Loss: 0.1617 - New Best\n",
      "Epoch 5 - Training Loss: 0.1613 - Valid Loss: 0.1417 - New Best\n",
      "Epoch 6 - Training Loss: 0.1520 - Valid Loss: 0.1316 - New Best\n",
      "Epoch 7 - Training Loss: 0.1446 - Valid Loss: 0.1277 - New Best\n",
      "Epoch 8 - Training Loss: 0.1387 - Valid Loss: 0.1211 - New Best\n",
      "Epoch 9 - Training Loss: 0.1336 - Valid Loss: 0.1242\n",
      "Epoch 10 - Training Loss: 0.1277 - Valid Loss: 0.1146 - New Best\n",
      "Epoch 11 - Training Loss: 0.1247 - Valid Loss: 0.1256\n",
      "Epoch 12 - Training Loss: 0.1201 - Valid Loss: 0.1150\n",
      "Epoch 13 - Training Loss: 0.1154 - Valid Loss: 0.1135 - New Best\n",
      "Epoch 14 - Training Loss: 0.1117 - Valid Loss: 0.1072 - New Best\n",
      "Epoch 15 - Training Loss: 0.1101 - Valid Loss: 0.1062 - New Best\n",
      "Epoch 16 - Training Loss: 0.1060 - Valid Loss: 0.0995 - New Best\n",
      "Epoch 17 - Training Loss: 0.1042 - Valid Loss: 0.0982 - New Best\n",
      "Epoch 18 - Training Loss: 0.0995 - Valid Loss: 0.1034\n",
      "Epoch 19 - Training Loss: 0.0977 - Valid Loss: 0.0925 - New Best\n",
      "Epoch 20 - Training Loss: 0.0941 - Valid Loss: 0.0944\n",
      "Epoch 21 - Training Loss: 0.0927 - Valid Loss: 0.0899 - New Best\n",
      "Epoch 22 - Training Loss: 0.0901 - Valid Loss: 0.0935\n",
      "Epoch 23 - Training Loss: 0.0893 - Valid Loss: 0.0958\n",
      "Epoch 24 - Training Loss: 0.0873 - Valid Loss: 0.0905\n",
      "Epoch 25 - Training Loss: 0.0829 - Valid Loss: 0.0922\n",
      "Epoch 26 - Training Loss: 0.0818 - Valid Loss: 0.0861 - New Best\n",
      "Epoch 27 - Training Loss: 0.0786 - Valid Loss: 0.0884\n",
      "Epoch 28 - Training Loss: 0.0782 - Valid Loss: 0.0869\n",
      "Epoch 29 - Training Loss: 0.0767 - Valid Loss: 0.0858 - New Best\n",
      "Epoch 30 - Training Loss: 0.0760 - Valid Loss: 0.0817 - New Best\n",
      "Time elapsed: 0:05:00.037853\n",
      "Mean time per epoch: 0:00:10.001262\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best bidirectional LSTM model checkpoint\n",
    "checkpoint_fname = checkpoint_path / \"bestLSTMbidir.pt\"\n",
    "\n",
    "# Create an instance of the NameLSTMBidir model with specified hyperparameters\n",
    "model_lstm_bidir = NameLSTMBidir(\n",
    "    hidden_size=50,  # Hidden state size of the LSTM\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_lstm_bidir.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_lstm_bidir)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_loader: The iterator for the training dataset\n",
    "# - valid_loader: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(\n",
    "    30,\n",
    "    model_lstm_bidir,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device,\n",
    "    checkpoint_fname,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the NameLSTMBidir model with the same hyperparameters as the trained model\n",
    "model_lstm_bidir_inference = NameLSTMBidir(\n",
    "    hidden_size=50,  # Hidden state size of the LSTM\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_lstm_bidir_inference.load_state_dict(torch.load(checkpoint_fname))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_lstm_bidir_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_lstm_bidir_inference = model_lstm_bidir_inference.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    \"1\": \"F\",\n",
    "    \"0\": \"M\",\n",
    "}  # Map label indices to gender labels ('1' -> 'F', '0' -> 'M')\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "pred_valid = [predict(i[0], model_lstm_bidir_inference, vocab)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9589367959602686\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.95      0.97      0.96      9929\n",
      "           M       0.96      0.94      0.95      8092\n",
      "\n",
      "    accuracy                           0.96     18021\n",
      "   macro avg       0.96      0.96      0.96     18021\n",
      "weighted avg       0.96      0.96      0.96     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9651  278]\n",
      " [ 462 7630]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(true_valid, pred_valid))\n",
    "print(f\"Classification Report:\\n {classification_report(true_valid, pred_valid)}\")\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU) Networks\n",
    "\n",
    "GRU networks are one of the main alternatives to LSTM networks for managing long-term dependencies in sequential data. They simplify the recurrent neural network (RNN) structure by reducing the number of gates and combining the memory components, which can lead to computational efficiency and simpler training.\n",
    "\n",
    "- **Traditional RNNs:**  \n",
    "  Suffer from vanishing and exploding gradient issues, making it difficult to learn long-term dependencies.\n",
    "\n",
    "- **LSTM Networks:**  \n",
    "  Address these issues by introducing a cell state along with three gating mechanisms (input, output, and forget gates). This design improves the learning of longer sequences but introduces a higher computational cost.\n",
    "\n",
    "- **GRU Networks:**  \n",
    "  Introduced in 2014 by Kyunghyun Cho et al. in [*Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*](https://arxiv.org/abs/1406.1078), GRUs simplify the LSTM design by merging the cell state and hidden state and using only two gates.\n",
    "\n",
    "- **GRU networks** simplify the recurrent unit architecture by:\n",
    "  - Merging the cell and hidden states.\n",
    "  - Using only two gates (update and reset).\n",
    "- **Efficiency:** Their reduced complexity can lead to faster computation and less memory usage.\n",
    "- **Functionality:** The update gate manages the balance between retaining previous information and updating it with new data, while the reset gate moderates the influence of previous data on the current input.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/gru.png\" alt=\"\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "This streamlined approach makes GRU networks an attractive option when resources are limited or when tasks do not demand the full complexity offered by LSTM architectures.\n",
    "\n",
    "## GRU Architecture\n",
    "\n",
    "The GRU is built around a single hidden state that keeps the memory of the system. It employs two gating mechanisms:\n",
    "\n",
    "1. **Update Gate ($z_t$)**\n",
    "2. **Reset Gate ($r_t$)**\n",
    "\n",
    "Each gate’s output lies between 0 and 1, determining the degree to which information is retained or discarded in each time step.\n",
    "\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let:  \n",
    "- $ \\mathbf{x}_t $ be the input at time $ t $,  \n",
    "- $ \\mathbf{h}_{t-1} $ be the previous hidden state, and  \n",
    "- $ \\mathbf{h}_t $ be the new hidden state.\n",
    "\n",
    "The computations in a GRU cell are as follows:\n",
    "\n",
    "1. **Update Gate:**\n",
    "\n",
    "   $$\n",
    "   z_t = \\sigma(W_z \\mathbf{x}_t + U_z \\mathbf{h}_{t-1})\n",
    "   $$\n",
    "\n",
    "   - $W_z$ and $U_z$ are weight matrices.\n",
    "   - $\\sigma(\\cdot)$ is the sigmoid function.\n",
    "\n",
    "2. **Reset Gate:**\n",
    "\n",
    "   $$\n",
    "   r_t = \\sigma(W_r \\mathbf{x}_t + U_r \\mathbf{h}_{t-1})\n",
    "   $$\n",
    "\n",
    "   - $W_r$ and $U_r$ are weight matrices.\n",
    "\n",
    "3. **Candidate Hidden State:**\n",
    "\n",
    "   $$\n",
    "   \\tilde{\\mathbf{h}}_t = \\tanh(W \\mathbf{x}_t + U (r_t \\odot \\mathbf{h}_{t-1}))\n",
    "   $$\n",
    "\n",
    "   - $W$ and $U$ are weight matrices.\n",
    "   - $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "4. **Final Hidden State:**\n",
    "\n",
    "   $$\n",
    "   \\mathbf{h}_t = (1 - z_t) \\odot \\mathbf{h}_{t-1} + z_t \\odot \\tilde{\\mathbf{h}}_t\n",
    "   $$\n",
    "\n",
    "   The update gate $z_t$ controls the balance between retaining the past hidden state and updating with the new candidate hidden state, $\\tilde{\\mathbf{h}}_t$.\n",
    "\n",
    "\n",
    "\n",
    "## Explanation of the Key Components\n",
    "\n",
    "- **The Hidden State:**  \n",
    "  In GRUs, there is no separate cell state. The hidden state $ \\mathbf{h}_t $ combines both the memory and the output information. This unified approach simplifies the model architecture and reduces the number of parameters.\n",
    "\n",
    "- **The Update Gate:**  \n",
    "  - **Role:** Determines how much of the previous hidden state is carried forward into the new hidden state.\n",
    "  - **Effect:**  \n",
    "    - If $ z_t $ is close to 0, the network prefers to keep most of the previous information.\n",
    "    - If $ z_t $ is close to 1, the network gives preference to the candidate hidden state, meaning it updates the state significantly with new information.\n",
    "\n",
    "- **The Reset Gate:**  \n",
    "  - **Role:** Determines how much of the previous hidden state should be reset before computing the candidate hidden state.\n",
    "  - **Effect:**  \n",
    "    - If $ r_t $ is close to 0, it enables the unit to discard most past information, focusing on the new input.\n",
    "    - If $ r_t $ is close to 1, it allows the network to use the previous state information more fully when calculating $\\tilde{\\mathbf{h}}_t$.\n",
    "\n",
    "> **Note:** GRUs and LSTMs often perform similarly on tasks such as machine translation, text generation, and speech recognition. The decision between the two can depend on the complexity of the task and computational resources available. GRUs tend to train faster due to their simplified structure, while LSTMs might better capture longer dependencies in more complex tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGRUBidir(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define an embedding layer to convert our letters to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Define a bidirectional GRU layer to encode our sequence of letters\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Define a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a classifier layer to output our final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                hidden_size * 2, hidden_size // 2\n",
    "            ),  # input size is the hidden size times 2 because of bidirectionality\n",
    "            self.dropout,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                hidden_size // 2, 2\n",
    "            ),  # output size is 2 because we're predicting binary gender\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, name_len):\n",
    "        # Convert our sequence of letters to a sequence of vectors using the embedding layer\n",
    "        embedded = self.embedding(seq)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Encode the embedded sequence using the bidirectional GRU layer\n",
    "        packed_output, hidden = self.encoder(embedded)\n",
    "\n",
    "        # Concatenate the final hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "\n",
    "        # Pass the concatenated hidden state through the classifier layer to get our prediction\n",
    "        preds = self.classifier(hidden)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 71,977 trainable parameters\n",
      "Epoch 1 - Training Loss: 0.3147 - Valid Loss: 0.2220 - New Best\n",
      "Epoch 2 - Training Loss: 0.2094 - Valid Loss: 0.1782 - New Best\n",
      "Epoch 3 - Training Loss: 0.1801 - Valid Loss: 0.1590 - New Best\n",
      "Epoch 4 - Training Loss: 0.1675 - Valid Loss: 0.1432 - New Best\n",
      "Epoch 5 - Training Loss: 0.1565 - Valid Loss: 0.1394 - New Best\n",
      "Epoch 6 - Training Loss: 0.1499 - Valid Loss: 0.1271 - New Best\n",
      "Epoch 7 - Training Loss: 0.1426 - Valid Loss: 0.1242 - New Best\n",
      "Epoch 8 - Training Loss: 0.1355 - Valid Loss: 0.1166 - New Best\n",
      "Epoch 9 - Training Loss: 0.1296 - Valid Loss: 0.1126 - New Best\n",
      "Epoch 10 - Training Loss: 0.1250 - Valid Loss: 0.1135\n",
      "Epoch 11 - Training Loss: 0.1209 - Valid Loss: 0.1042 - New Best\n",
      "Epoch 12 - Training Loss: 0.1167 - Valid Loss: 0.1104\n",
      "Epoch 13 - Training Loss: 0.1120 - Valid Loss: 0.1016 - New Best\n",
      "Epoch 14 - Training Loss: 0.1095 - Valid Loss: 0.0990 - New Best\n",
      "Epoch 15 - Training Loss: 0.1053 - Valid Loss: 0.1013\n",
      "Epoch 16 - Training Loss: 0.1026 - Valid Loss: 0.0932 - New Best\n",
      "Epoch 17 - Training Loss: 0.0997 - Valid Loss: 0.0927 - New Best\n",
      "Epoch 18 - Training Loss: 0.0977 - Valid Loss: 0.0914 - New Best\n",
      "Epoch 19 - Training Loss: 0.0936 - Valid Loss: 0.0873 - New Best\n",
      "Epoch 20 - Training Loss: 0.0925 - Valid Loss: 0.0862 - New Best\n",
      "Epoch 21 - Training Loss: 0.0899 - Valid Loss: 0.0877\n",
      "Epoch 22 - Training Loss: 0.0891 - Valid Loss: 0.0892\n",
      "Epoch 23 - Training Loss: 0.0861 - Valid Loss: 0.0844 - New Best\n",
      "Epoch 24 - Training Loss: 0.0839 - Valid Loss: 0.0823 - New Best\n",
      "Epoch 25 - Training Loss: 0.0808 - Valid Loss: 0.0798 - New Best\n",
      "Epoch 26 - Training Loss: 0.0810 - Valid Loss: 0.0780 - New Best\n",
      "Epoch 27 - Training Loss: 0.0772 - Valid Loss: 0.0796\n",
      "Epoch 28 - Training Loss: 0.0781 - Valid Loss: 0.0791\n",
      "Epoch 29 - Training Loss: 0.0747 - Valid Loss: 0.0770 - New Best\n",
      "Epoch 30 - Training Loss: 0.0732 - Valid Loss: 0.0777\n",
      "Time elapsed: 0:04:48.733502\n",
      "Mean time per epoch: 0:00:09.624450\n"
     ]
    }
   ],
   "source": [
    "# Define the filename for saving the best bidirectional GRU model checkpoint\n",
    "checkpoint_fname = checkpoint_path / \"bestGRUBidir.pt\"\n",
    "\n",
    "# Create an instance of the NameGRUBidir model with specified hyperparameters\n",
    "model_gru_bidir = NameGRUBidir(\n",
    "    hidden_size=50,  # Hidden state size of the GRU\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Define the optimizer to use for training the model\n",
    "# Here, we use the Adam optimizer with a learning rate of 3e-4\n",
    "optimizer = optim.Adam(model_gru_bidir.parameters(), lr=3e-4)\n",
    "\n",
    "# Define the loss function to use for training the model\n",
    "# Here, we use the CrossEntropyLoss, which is suitable for classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for 30 epochs\n",
    "# Arguments:\n",
    "# - epochs: Number of epochs to train the model\n",
    "# - model: The model to train (model_gru_bidir)\n",
    "# - optimizer: The optimizer to use for training (Adam optimizer)\n",
    "# - criterion: The loss function to use for training (CrossEntropyLoss)\n",
    "# - train_loader: The iterator for the training dataset\n",
    "# - valid_loader: The iterator for the validation dataset\n",
    "# - device: The device to use for computation (e.g., 'cpu' or 'cuda')\n",
    "# - checkpoint_fname: The filename for saving the best model checkpoint\n",
    "train(\n",
    "    30,\n",
    "    model_gru_bidir,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    device,\n",
    "    checkpoint_fname,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the NameGRUBidir model with the same hyperparameters as the trained model\n",
    "# This ensures that the model architecture matches the one used during training\n",
    "model_gru_bidir_inference = NameGRUBidir(\n",
    "    hidden_size=50,  # Hidden state size of the GRU\n",
    "    embedding_dim=25,  # Dimension of the embedding vectors\n",
    "    vocab_size=len(vocab),  # Vocabulary size\n",
    "    pad_idx=pad_idx,  # Padding index for the embedding layer\n",
    ")\n",
    "\n",
    "# Load the trained model's state dictionary into the new model instance\n",
    "# This will set the model parameters to the best found during training\n",
    "model_gru_bidir_inference.load_state_dict(\n",
    "    torch.load(checkpoint_fname, weights_only=False)\n",
    ")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This disables dropout and other training-specific behaviors\n",
    "model_gru_bidir_inference.eval()\n",
    "\n",
    "# Send the model to the CPU for inference\n",
    "# This ensures that the model runs on the CPU, which is typically used for inference\n",
    "model_gru_bidir_inference = model_gru_bidir_inference.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    \"1\": \"F\",\n",
    "    \"0\": \"M\",\n",
    "}  # Map label indices to gender labels ('1' -> 'F', '0' -> 'M')\n",
    "\n",
    "# Create a list of true gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and map the label index to the gender label using label_mapping\n",
    "true_valid = [label_mapping[i[1]] for i in names_valid]\n",
    "\n",
    "# Create a list of predicted gender labels for the validation dataset\n",
    "# Iterate over the names in the validation dataset and use the predict function to get the predicted gender label\n",
    "pred_valid = [predict(i[0], model_gru_bidir_inference, vocab)[0] for i in names_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9607125020809056\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           F       0.96      0.97      0.96      9929\n",
      "           M       0.96      0.95      0.96      8092\n",
      "\n",
      "    accuracy                           0.96     18021\n",
      "   macro avg       0.96      0.96      0.96     18021\n",
      "weighted avg       0.96      0.96      0.96     18021\n",
      "\n",
      "Confusion Matrix:\n",
      " [[9620  309]\n",
      " [ 399 7693]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(true_valid, pred_valid))\n",
    "print(f\"Classification Report:\\n {classification_report(true_valid, pred_valid)}\")\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(true_valid, pred_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "| Architecture | Accuracy | Number of Parameters |\n",
    "|-------------------------|----------|----------------------|\n",
    "| Vanilla RNN | 93.12% | 10,977 |\n",
    "| LSTM | 94.79% | 37,827 |\n",
    "| Bidirectional LSTM | 95.89% | 94,877 |\n",
    "| Bidirectional GRU | 96.07% | 71,977 |\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- **Accuracy**:\n",
    "    - The **Bidirectional GRU** model achieves the highest accuracy at 96.07%, followed closely by the **Bidirectional LSTM** at 95.89%.\n",
    "    - The **LSTM** model outperforms the **Vanilla RNN**, with an accuracy of 94.79% compared to 93.12%.\n",
    "\n",
    "- **Number of Parameters**:\n",
    "    - The **Vanilla RNN** has the fewest parameters (10,977), making it the most lightweight model.\n",
    "    - The **Bidirectional LSTM** has the highest number of parameters (94,877), indicating a more complex and potentially more powerful model.\n",
    "    - The **Bidirectional GRU** has fewer parameters (71,977) than the **Bidirectional LSTM**, but more than the **LSTM** (37,827) and **Vanilla RNN**.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "- The **Bidirectional GRU** provides the best accuracy with fewer parameters than the Bidirectional LSTM, making it an efficient choice for this task.\n",
    "- The **LSTM** improves accuracy significantly over the **Vanilla RNN** without an excessive increase in parameters.\n",
    "- The **Vanilla RNN** is the simplest model with the fewest parameters but also the lowest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now covered an extensive journey, exploring the foundations and insights into the world of Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRU). We've examined the strengths and limitations of RNNs and learned how LSTMs and GRUs have significantly improved upon these foundations to better handle long-term dependencies.\n",
    "\n",
    "Understanding these three forms of neural networks is crucial for anyone interested in Natural Language Processing (NLP). In NLP tasks, we often deal with sequenced data where understanding temporal dependencies is key, be it understanding the sentiment behind a customer review or translating a passage from one language to another. RNNs, LSTMs, and GRUs offer us powerful tools to make sense of this complex, sequential data.\n",
    "\n",
    "However, remember that the choice of architecture should be dictated by the specifics of your task. While RNNs can suffice for simpler, short-term dependencies, LSTMs and GRUs are better suited for more complex tasks or longer sequences. But no model can serve as a silver bullet. Experimenting, iterating, and continuous learning are part of the process.\n",
    "\n",
    "Next class we will start to work with a new type of neural network: Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- Understanding RNNs, LSTMs, and GRUs is crucial for natural language processing tasks that involve sequential data and require capturing dependencies over time.\n",
    "\n",
    "- The choice of architecture depends on the specific task and its complexity. RNNs can suffice for simpler, short-term dependencies, while LSTMs and GRUs are better suited for more complex tasks or longer sequences.\n",
    "\n",
    "- Bidirectional networks offer additional context by processing sequences in both forward and backward directions, leading to improved performance in sequence processing tasks.\n",
    "\n",
    "- Experimenting with different architectures, hyperparameters, and iterative model evaluation is essential to find the optimal solution for a given task.\n",
    "\n",
    "- Comprehending the strengths and limitations of each architecture and considering the tradeoffs between model complexity, computational efficiency, and performance is crucial when selecting the appropriate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What are the main types of recurrent neural networks discussed in this class?\n",
    "\n",
    "2. What is the vanishing gradient problem and how do LSTMs address it?\n",
    "\n",
    "3. How does a bidirectional LSTM network process sequences differently from a standard LSTM?\n",
    "\n",
    "4. What are the key components of an LSTM cell and what are their functions?\n",
    "\n",
    "5. How does a GRU simplify the architecture of an LSTM?\n",
    "\n",
    "6. What was the case study used to demonstrate the application of these different network architectures?\n",
    "\n",
    "7. Which model achieved the highest accuracy on the name gender classification task?\n",
    "\n",
    "8. How many trainable parameters did the bidirectional LSTM model have compared to the vanilla RNN?\n",
    "\n",
    "9. What are some of the tradeoffs to consider when choosing between RNNs, LSTMs and GRUs?\n",
    "\n",
    "10. Why are recurrent networks particularly useful for natural language processing tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "\n",
    "<!-- 1. RNNs are the simplest, processing sequences with hidden states. LSTMs introduce a memory cell and gates to better handle long-term dependencies. GRUs simplify LSTMs by combining hidden and cell states and using only update and reset gates.\n",
    "\n",
    "2. RNNs process sequential data by maintaining a hidden state that captures information from previous time steps. They face challenges with vanishing and exploding gradients when dealing with long sequences.\n",
    "\n",
    "3. Vanishing gradients occur when gradients become extremely small during backpropagation, preventing effective learning. Exploding gradients happen when gradients become very large, leading to unstable training.\n",
    "\n",
    "4. LSTMs have a memory cell that maintains long-term information, and gates (input, output, forget) that regulate information flow, allowing them to capture long-term dependencies effectively.\n",
    "\n",
    "5. The input gate controls what new information is added to the cell state. The forget gate determines what information to discard from the cell state. The output gate decides what information from the cell state is used to compute the hidden state.\n",
    "\n",
    "6. Bidirectional LSTMs process sequences in both forward and backward directions, capturing both past and future contexts. This allows them to have a more complete understanding of the sequence.\n",
    "\n",
    "7. GRUs combine the hidden and cell states into a single hidden state and use only two gates (update and reset), making them computationally more efficient than LSTMs while still handling long-term dependencies effectively.\n",
    "\n",
    "8. The dataset used was from the 2010 Brazilian Census, containing 90,104 names (49,274 female and 40,830 male). Names that could be associated with both genders were excluded. The data was split into 80% training and 20% validation sets.\n",
    "\n",
    "9. Bidirectional networks process sequences in both forward and backward directions, allowing them to capture both past and future contexts. This additional context helps improve the performance of sequence processing tasks.\n",
    "\n",
    "10. The Bidirectional LSTM achieved the highest accuracy (97.37%), followed by the Bidirectional GRU (97.28%), LSTM (96.61%), and Vanilla RNN (95.57%). The Bidirectional LSTM had the most parameters, while the Vanilla RNN had the fewest. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imd1107-nlp-1oxVnwDa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
